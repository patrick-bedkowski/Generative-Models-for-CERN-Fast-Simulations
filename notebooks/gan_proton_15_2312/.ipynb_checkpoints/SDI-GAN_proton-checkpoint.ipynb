{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for running in collab, sagemaker etc.\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/studio-lab-user/Generative_Models_for_CERN_Fast_Simulations/utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 09:30:04.121379: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 09:30:04.315902: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-25 09:30:04.315962: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-25 09:30:05.164128: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-25 09:30:05.164217: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-25 09:30:05.164227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 09:30:06.251194: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-25 09:30:06.251238: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-25 09:30:06.251262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (default): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "re6ywGTYfdzE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, decomposition, manifold, preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import time\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel as sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_EXPERIMENT_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CtnTQjy0Q05",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9tDJ602Bolmd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (48714, 56, 30) max: 678.0\n",
      "Loaded cond:  (48714, 13) max: 7000.0 min: -7000.0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('../../data/data_proton_photonsum_15_2133.pkl')\n",
    "print('Loaded: ',  data.shape, \"max:\", data.max())\n",
    "\n",
    "# Data containing particle conditional data from particle having responses with proton photon sum in interval [70, 2312] without taking into consideration photon sums of neutron responses.\n",
    "data_cond = pd.read_pickle('../../data/data_cond_stddev_photonsum_15_2133_15_3273.pkl')\n",
    "print('Loaded cond: ',  data_cond.shape, \"max:\", data_cond.values.max(), \"min:\", data_cond.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate min max proton sum\n",
    "photon_sum_proton_min, photon_sum_proton_max = data_cond.proton_photon_sum.min(), data_cond.proton_photon_sum.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Energy', 'Vx', 'Vy', 'Vz', 'Px', 'Py', 'Pz', 'mass', 'charge', 'std'], dtype='object'),\n",
       " 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond.drop(columns=['proton_photon_sum', 'neutron_photon_sum', 'group_number'], inplace=True)\n",
    "data_cond.columns, len(data_cond.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>Pz</th>\n",
       "      <th>mass</th>\n",
       "      <th>charge</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3192.38</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.182957</td>\n",
       "      <td>-3192.38</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3961.55</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.076487</td>\n",
       "      <td>0.179845</td>\n",
       "      <td>3961.55</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2770.10</td>\n",
       "      <td>1.861170e-17</td>\n",
       "      <td>2.517190e-17</td>\n",
       "      <td>-1.689330e-13</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>0.412760</td>\n",
       "      <td>-2770.10</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3195.12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>-0.149980</td>\n",
       "      <td>3195.11</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1714.07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.457768</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>-1714.07</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Energy            Vx            Vy            Vz        Px        Py  \\\n",
       "0  3192.38  0.000000e+00  0.000000e+00  0.000000e+00  0.022422 -0.182957   \n",
       "1  3961.55  0.000000e+00  0.000000e+00  0.000000e+00 -0.076487  0.179845   \n",
       "2  2770.10  1.861170e-17  2.517190e-17 -1.689330e-13  0.305187  0.412760   \n",
       "3  3195.12  0.000000e+00  0.000000e+00  0.000000e+00  0.560528 -0.149980   \n",
       "4  1714.07  0.000000e+00  0.000000e+00  0.000000e+00  0.457768  0.145639   \n",
       "\n",
       "        Pz        mass  charge       std  \n",
       "0 -3192.38  939.565413     0.0  0.076311  \n",
       "1  3961.55  938.272081     1.0  0.479201  \n",
       "2 -2770.10  497.611000     0.0  0.090233  \n",
       "3  3195.11  938.272081     1.0  0.131331  \n",
       "4 -1714.07  939.565413     0.0  0.103628  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment DIR:  experiments/sdi-gan_15_2133_25_04_2023_09_30\n"
     ]
    }
   ],
   "source": [
    "STRENGTH = 0.1\n",
    "\n",
    "DATE_STR = datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "\n",
    "NAME = \"sdi-gan\"\n",
    "\n",
    "wandb_run_name = f\"{int(photon_sum_proton_min)}_{int(photon_sum_proton_max)}_{DATE_STR}\"\n",
    "\n",
    "EXPERIMENT_DIR_NAME = f\"experiments/{NAME}_{int(photon_sum_proton_min)}_{int(photon_sum_proton_max)}_{DATE_STR}\"\n",
    "\n",
    "print(\"Experiment DIR: \", EXPERIMENT_DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_scales(model_name, scaler_means, scaler_scales):\n",
    "    out_fnm = f\"{model_name}_scales.txt\"\n",
    "    res = \"#means\"\n",
    "    for mean_ in scaler_means:\n",
    "        res += \"\\n\" + str(mean_)\n",
    "    res += \"\\n\\n#scales\"\n",
    "    for scale_ in scaler_scales:\n",
    "        res += \"\\n\" + str(scale_)\n",
    "\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        filepath = f\"../../{EXPERIMENT_DIR_NAME}/scales/\"\n",
    "        create_dir(filepath)\n",
    "        with open(filepath+out_fnm, mode=\"w\") as f:\n",
    "            f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>Pz</th>\n",
       "      <th>mass</th>\n",
       "      <th>charge</th>\n",
       "      <th>std</th>\n",
       "      <th>cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3192.38</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.182957</td>\n",
       "      <td>-3192.38</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076311</td>\n",
       "      <td>3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3961.55</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.076487</td>\n",
       "      <td>0.179845</td>\n",
       "      <td>3961.55</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2770.10</td>\n",
       "      <td>1.861170e-17</td>\n",
       "      <td>2.517190e-17</td>\n",
       "      <td>-1.689330e-13</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>0.412760</td>\n",
       "      <td>-2770.10</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090233</td>\n",
       "      <td>2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3195.12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>-0.149980</td>\n",
       "      <td>3195.11</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131331</td>\n",
       "      <td>3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1714.07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.457768</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>-1714.07</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103628</td>\n",
       "      <td>1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Energy            Vx            Vy            Vz        Px        Py  \\\n",
       "0  3192.38  0.000000e+00  0.000000e+00  0.000000e+00  0.022422 -0.182957   \n",
       "1  3961.55  0.000000e+00  0.000000e+00  0.000000e+00 -0.076487  0.179845   \n",
       "2  2770.10  1.861170e-17  2.517190e-17 -1.689330e-13  0.305187  0.412760   \n",
       "3  3195.12  0.000000e+00  0.000000e+00  0.000000e+00  0.560528 -0.149980   \n",
       "4  1714.07  0.000000e+00  0.000000e+00  0.000000e+00  0.457768  0.145639   \n",
       "\n",
       "        Pz        mass  charge       std  \\\n",
       "0 -3192.38  939.565413     0.0  0.076311   \n",
       "1  3961.55  938.272081     1.0  0.479201   \n",
       "2 -2770.10  497.611000     0.0  0.090233   \n",
       "3  3195.11  938.272081     1.0  0.131331   \n",
       "4 -1714.07  939.565413     0.0  0.103628   \n",
       "\n",
       "                                                cond  \n",
       "0  3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...  \n",
       "1  3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...  \n",
       "2  2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...  \n",
       "3  3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...  \n",
       "4  1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond[\"cond\"] = data_cond[\"Energy\"].astype(str) +\"|\"+ data_cond[\"Vx\"].astype(str) +\"|\"+  data_cond[\"Vy\"].astype(str) +\"|\"+ data_cond[\"Vz\"].astype(str) +\"|\"+  data_cond[\"Px\"].astype(str) +\"|\"+  data_cond[\"Py\"].astype(str) +\"|\"+ data_cond[\"Pz\"].astype(str) +\"|\"+  data_cond[\"mass\"].astype(str) +\"|\"+  data_cond[\"charge\"].astype(str)\n",
    "data_cond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48709</th>\n",
       "      <td>48709</td>\n",
       "      <td>1456.66|7.79676e-18|2.26923e-18|-4.06504e-14|0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48710</th>\n",
       "      <td>48710</td>\n",
       "      <td>3812.18|0.0|0.0|0.0|0.0529534|0.402807|-3812.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48711</th>\n",
       "      <td>48711</td>\n",
       "      <td>1422.39|1.1825599999999999e-05|-1.3857e-06|0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48712</th>\n",
       "      <td>48712</td>\n",
       "      <td>4071.23|0.0|0.0|0.0|0.089305|0.0694104|-4071.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48713</th>\n",
       "      <td>48713</td>\n",
       "      <td>4003.6|0.0|0.0|0.0|0.769178|-0.276742999999999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48714 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                               cond\n",
       "0          0  3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...\n",
       "1          1  3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...\n",
       "2          2  2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...\n",
       "3          3  3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...\n",
       "4          4  1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...\n",
       "...      ...                                                ...\n",
       "48709  48709  1456.66|7.79676e-18|2.26923e-18|-4.06504e-14|0...\n",
       "48710  48710  3812.18|0.0|0.0|0.0|0.0529534|0.402807|-3812.1...\n",
       "48711  48711  1422.39|1.1825599999999999e-05|-1.3857e-06|0.0...\n",
       "48712  48712  4071.23|0.0|0.0|0.0|0.089305|0.0694104|-4071.2...\n",
       "48713  48713  4003.6|0.0|0.0|0.0|0.769178|-0.276742999999999...\n",
       "\n",
       "[48714 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond_id = data_cond[[\"cond\"]].reset_index()\n",
    "data_cond_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index_x\n",
       "0        23527\n",
       "1         2399\n",
       "2        29630\n",
       "3        28756\n",
       "4        22304\n",
       "         ...  \n",
       "48709    15384\n",
       "48710    19389\n",
       "48711    23186\n",
       "48712    24615\n",
       "48713    43374\n",
       "Name: index_y, Length: 48714, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = data_cond_id.merge(data_cond_id.sample(frac=1), on=[\"cond\"], how=\"inner\").groupby(\"index_x\").first()\n",
    "ids = ids[\"index_y\"]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>Pz</th>\n",
       "      <th>mass</th>\n",
       "      <th>charge</th>\n",
       "      <th>std</th>\n",
       "      <th>cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3192.38</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.182957</td>\n",
       "      <td>-3192.38</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076311</td>\n",
       "      <td>3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3961.55</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.076487</td>\n",
       "      <td>0.179845</td>\n",
       "      <td>3961.55</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2770.10</td>\n",
       "      <td>1.861170e-17</td>\n",
       "      <td>2.517190e-17</td>\n",
       "      <td>-1.689330e-13</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>0.412760</td>\n",
       "      <td>-2770.10</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090233</td>\n",
       "      <td>2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3195.12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>-0.149980</td>\n",
       "      <td>3195.11</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131331</td>\n",
       "      <td>3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1714.07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.457768</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>-1714.07</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103628</td>\n",
       "      <td>1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48709</th>\n",
       "      <td>1456.66</td>\n",
       "      <td>7.796760e-18</td>\n",
       "      <td>2.269230e-18</td>\n",
       "      <td>-4.065040e-14</td>\n",
       "      <td>0.279388</td>\n",
       "      <td>0.081315</td>\n",
       "      <td>-1456.66</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062655</td>\n",
       "      <td>1456.66|7.79676e-18|2.26923e-18|-4.06504e-14|0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48710</th>\n",
       "      <td>3812.18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.052953</td>\n",
       "      <td>0.402807</td>\n",
       "      <td>-3812.18</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.362445</td>\n",
       "      <td>3812.18|0.0|0.0|0.0|0.0529534|0.402807|-3812.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48711</th>\n",
       "      <td>1422.39</td>\n",
       "      <td>1.182560e-05</td>\n",
       "      <td>-1.385700e-06</td>\n",
       "      <td>3.655780e-02</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>-0.052227</td>\n",
       "      <td>1422.39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>1422.39|1.1825599999999999e-05|-1.3857e-06|0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48712</th>\n",
       "      <td>4071.23</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.089305</td>\n",
       "      <td>0.069410</td>\n",
       "      <td>-4071.23</td>\n",
       "      <td>1115.683000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.422491</td>\n",
       "      <td>4071.23|0.0|0.0|0.0|0.089305|0.0694104|-4071.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48713</th>\n",
       "      <td>4003.60</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.769178</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>4003.60</td>\n",
       "      <td>1115.683000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153674</td>\n",
       "      <td>4003.6|0.0|0.0|0.0|0.769178|-0.276742999999999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48714 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Energy            Vx            Vy            Vz        Px        Py  \\\n",
       "0      3192.38  0.000000e+00  0.000000e+00  0.000000e+00  0.022422 -0.182957   \n",
       "1      3961.55  0.000000e+00  0.000000e+00  0.000000e+00 -0.076487  0.179845   \n",
       "2      2770.10  1.861170e-17  2.517190e-17 -1.689330e-13  0.305187  0.412760   \n",
       "3      3195.12  0.000000e+00  0.000000e+00  0.000000e+00  0.560528 -0.149980   \n",
       "4      1714.07  0.000000e+00  0.000000e+00  0.000000e+00  0.457768  0.145639   \n",
       "...        ...           ...           ...           ...       ...       ...   \n",
       "48709  1456.66  7.796760e-18  2.269230e-18 -4.065040e-14  0.279388  0.081315   \n",
       "48710  3812.18  0.000000e+00  0.000000e+00  0.000000e+00  0.052953  0.402807   \n",
       "48711  1422.39  1.182560e-05 -1.385700e-06  3.655780e-02  0.511508 -0.052227   \n",
       "48712  4071.23  0.000000e+00  0.000000e+00  0.000000e+00  0.089305  0.069410   \n",
       "48713  4003.60  0.000000e+00  0.000000e+00  0.000000e+00  0.769178 -0.276743   \n",
       "\n",
       "            Pz         mass  charge       std  \\\n",
       "0     -3192.38   939.565413     0.0  0.076311   \n",
       "1      3961.55   938.272081     1.0  0.479201   \n",
       "2     -2770.10   497.611000     0.0  0.090233   \n",
       "3      3195.11   938.272081     1.0  0.131331   \n",
       "4     -1714.07   939.565413     0.0  0.103628   \n",
       "...        ...          ...     ...       ...   \n",
       "48709 -1456.66   497.611000     0.0  0.062655   \n",
       "48710 -3812.18   938.272081     1.0  0.362445   \n",
       "48711  1422.39     0.000000     0.0  0.048635   \n",
       "48712 -4071.23  1115.683000     0.0  0.422491   \n",
       "48713  4003.60  1115.683000     0.0  0.153674   \n",
       "\n",
       "                                                    cond  \n",
       "0      3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...  \n",
       "1      3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...  \n",
       "2      2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...  \n",
       "3      3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...  \n",
       "4      1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...  \n",
       "...                                                  ...  \n",
       "48709  1456.66|7.79676e-18|2.26923e-18|-4.06504e-14|0...  \n",
       "48710  3812.18|0.0|0.0|0.0|0.0529534|0.402807|-3812.1...  \n",
       "48711  1422.39|1.1825599999999999e-05|-1.3857e-06|0.0...  \n",
       "48712  4071.23|0.0|0.0|0.0|0.089305|0.0694104|-4071.2...  \n",
       "48713  4003.6|0.0|0.0|0.0|0.769178|-0.276742999999999...  \n",
       "\n",
       "[48714 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10522,
     "status": "ok",
     "timestamp": 1623609981138,
     "user": {
      "displayName": "Jan Dubiński",
      "photoUrl": "",
      "userId": "04866767089811362617"
     },
     "user_tz": -120
    },
    "id": "g5jISZN7WrvL",
    "outputId": "0cf63b56-f149-4ff0-d3a8-972fd3b7dca3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 6.5206213 min 0.0\n",
      "std max 1.0 min 0.0\n"
     ]
    }
   ],
   "source": [
    "data = np.log(data+1)\n",
    "data = np.float32(data)\n",
    "print(\"data max\", data.max(), \"min\", data.min())\n",
    "\n",
    "data_2 = data[ids]\n",
    "\n",
    "data_cond = data_cond.drop(columns=\"cond\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "std = data_cond[\"std\"].values.reshape(-1,1)\n",
    "std = np.float32(std)\n",
    "std = scaler.fit_transform(std)\n",
    "print(\"std max\", std.max(), \"min\", std.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond max 21.432093 min -35.632454\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "data_cond = np.float32(data_cond.drop(columns=[\"std\"]))\n",
    "data_cond = scaler.fit_transform(data_cond)\n",
    "print(\"cond max\", data_cond.max(), \"min\", data_cond.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1623609981462,
     "user": {
      "displayName": "Jan Dubiński",
      "photoUrl": "",
      "userId": "04866767089811362617"
     },
     "user_tz": -120
    },
    "id": "45N-b-FGn4CO",
    "outputId": "9110f272-5c82-4a0b-dd31-2234414dfe5a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38971, 56, 30) (9743, 56, 30) (38971, 9) (9743, 9)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, x_train_2, x_test_2, y_train, y_test, std_train, std_test = train_test_split(data, data_2, data_cond, std, test_size=0.2, shuffle=False)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save scales\n",
    "if SAVE_EXPERIMENT_DATA:\n",
    "    save_scales(\"Proton\", scaler.mean_, scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9FMxwgNpn-CU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 09:30:11.064775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size=128)\n",
    "dataset_2 = tf.data.Dataset.from_tensor_slices(x_train_2).batch(batch_size=128)\n",
    "dataset_cond = tf.data.Dataset.from_tensor_slices(y_train).batch(batch_size=128)\n",
    "dataset_std = tf.data.Dataset.from_tensor_slices(std_train).batch(batch_size=128)\n",
    "fake_cond =  tf.data.Dataset.from_tensor_slices(y_train).shuffle(12800).batch(batch_size=128)\n",
    "dataset_with_cond = tf.data.Dataset.zip((dataset, dataset_2, dataset_cond, dataset_std, fake_cond)).shuffle(12800)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size=128)\n",
    "val_dataset_2 = tf.data.Dataset.from_tensor_slices(x_test_2).batch(batch_size=128)\n",
    "val_dataset_cond = tf.data.Dataset.from_tensor_slices(y_test).batch(batch_size=128)\n",
    "val_dataset_std = tf.data.Dataset.from_tensor_slices(std_test).batch(batch_size=128)\n",
    "val_fake_cond =  tf.data.Dataset.from_tensor_slices(y_test).shuffle(12800).batch(batch_size=128)\n",
    "val_dataset_with_cond = tf.data.Dataset.zip((val_dataset, val_dataset_2, val_dataset_cond, val_dataset_std, val_fake_cond)).shuffle(12800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41Ri3GB8n7oI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iFhM4_mYfdzJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 19)           0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          5120        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256)         1024        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 256)          0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 25600)        6579200     ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 25600)       102400      ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 25600)        0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 25600)        0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 20, 10, 128)  0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 60, 20, 128)  0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 59, 19, 256)  131328      ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 59, 19, 256)  1024       ['conv2d[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 59, 19, 256)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 59, 19, 256)  0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 59, 38, 256)  0          ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 58, 37, 128)  131200      ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 58, 37, 128)  512        ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 58, 37, 128)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 58, 37, 128)  0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 57, 36, 64)   32832       ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 57, 36, 64)  256         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 57, 36, 64)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 57, 36, 64)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 56, 30, 1)    897         ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,985,793\n",
      "Trainable params: 6,933,185\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 56, 30, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 54, 28, 32)   320         ['input_img[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 54, 28, 32)  128         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 54, 28, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 54, 28, 32)   0           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 27, 14, 32)   0           ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 25, 12, 16)   4624        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 25, 12, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 25, 12, 16)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 25, 12, 16)   0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 16)  0           ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2304)         0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2313)         0           ['flatten[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          296192      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128)         512         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 128)          0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 128)          0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8256        ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64)          256         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64)           0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 64)           0           ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            65          ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,417\n",
      "Trainable params: 309,937\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 10\n",
    "cond_dim = 9\n",
    "poz_dim = 6\n",
    "\n",
    "############################ generator ############################\n",
    "\n",
    "x = Input(shape=(latent_dim,))\n",
    "cond = Input(shape=(cond_dim,))\n",
    "inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "layer_1 = Dense(128*2)(inputs)\n",
    "layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "layer_2 = Dense(128*20*10)(layer_1_a)\n",
    "layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "reshaped = Reshape((20,10,128))(layer_2_a)\n",
    "reshaped_s = UpSampling2D(size=(3,2))(reshaped)\n",
    "\n",
    "conv1 = Conv2D(256, kernel_size=(2, 2), padding='valid')(reshaped_s)\n",
    "conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "conv1_a_s = UpSampling2D(size=(1,2))(conv1_a)\n",
    "\n",
    "conv2 = Conv2D(128, kernel_size=2)(conv1_a_s)\n",
    "conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "conv3 = Conv2D(64, kernel_size=2)(conv2_a)\n",
    "conv3_bd = Dropout(0.2)(BatchNormalization()(conv3))\n",
    "conv3_a = LeakyReLU(alpha=0.1)(conv3_bd)\n",
    "\n",
    "outputs = Conv2D(1, kernel_size=(2, 7), activation='relu')(conv3_a)\n",
    "\n",
    "generator = Model([x, cond], outputs, name='generator')\n",
    "generator.summary()\n",
    "\n",
    "############################ discriminator ############################\n",
    "\n",
    "input_img = Input(shape=[56,30,1],name='input_img')\n",
    "conv1 = Conv2D(32, kernel_size=3)(input_img)\n",
    "conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "conv2 = Conv2D(16, kernel_size=3)(pool1)\n",
    "conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 1))(conv2_a)\n",
    "flat = Flatten()(pool2)\n",
    "cond = Input(shape=(cond_dim,))\n",
    "inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "layer_1 = Dense(128)(inputs2)\n",
    "layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "layer_2 = Dense(64)(layer_1_a)\n",
    "layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "outputs = Dense(1, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "discriminator = Model([input_img, cond], [outputs,layer_2_a], name='discriminator')\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1623609985345,
     "user": {
      "displayName": "Jan Dubiński",
      "photoUrl": "",
      "userId": "04866767089811362617"
     },
     "user_tz": -120
    },
    "id": "UVXZMLDUfdzJ",
    "outputId": "b6cd2a13-46f7-4c7b-d9b6-e6eb0dec7757",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# latent_dim = 10\n",
    "# cond_dim = 10\n",
    "# poz_dim = 6\n",
    "\n",
    "# ############################ generator ############################\n",
    "\n",
    "# x = Input(shape=(latent_dim,))\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "# inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "# layer_1 = Dense(128*2)(inputs)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(128*28*15)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# reshaped = Reshape((28, 15, 128))(layer_2_a)\n",
    "# # reshaped_s = UpSampling2D()(reshaped)\n",
    "\n",
    "# conv1 = Conv2D(128, kernel_size=1)(reshaped)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# conv1_a_s = UpSampling2D()(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(64, kernel_size=1)(conv1_a_s)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "# outputs = Conv2D(1, kernel_size=1,activation='relu')(conv2_a)\n",
    "\n",
    "# generator = Model([x, cond], outputs, name='generator')\n",
    "# generator.summary()\n",
    "\n",
    "# ############################ discriminator ############################\n",
    "\n",
    "# input_img = Input(shape=[56,30,1],name='input_img')\n",
    "# conv1 = Conv2D(32, kernel_size=3)(input_img)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "# conv2 = Conv2D(16, kernel_size=3)(pool1)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "# pool2 = MaxPooling2D(pool_size=(2, 2))(conv2_a)\n",
    "# flat = Flatten()(pool2)\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "\n",
    "# inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "# layer_1 = Dense(128)(inputs2)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "# layer_2 = Dense(64)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "# outputs = Dense(1, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "# discriminator = Model([input_img, cond], outputs, name='discriminator')\n",
    "# discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "87NnkVJwaCOo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    d_acc_r.update_state(tf.ones_like(real_output), real_output)\n",
    "    d_acc_f.update_state(tf.zeros_like(fake_output), fake_output)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZXwATQ9uaigO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HqTYRo-uki5k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "d_acc_r = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "d_acc_f = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "g_acc = keras.metrics.BinaryAccuracy(name=\"g_acc_g\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yjX97hnkkmlf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(step, fake_output,\n",
    "                   fake_latent, fake_latent_2, noise, noise_2, std):\n",
    "    g_acc.update_state(tf.ones_like(fake_output), fake_output)\n",
    "    crossentropy_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    div = tf.math.divide(tf.reduce_mean(tf.abs(fake_latent - fake_latent_2),(1)), tf.reduce_mean(tf.abs(noise-noise_2),(1)))\n",
    "    div_loss = std * STRENGTH / (div + 1e-5)\n",
    "\n",
    "    div_loss = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std,(1)), div_loss))\n",
    "\n",
    "    return crossentropy_loss + div_loss, div_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "I4HsHLgwkurp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "noise_dim = 10\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "START_GENERATING_IMG_FROM_IDX = 20\n",
    "# Seed to reuse for generating samples for comparison during training\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed_cond = y_test[START_GENERATING_IMG_FROM_IDX:START_GENERATING_IMG_FROM_IDX+num_examples_to_generate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbedkowski-patrick\u001b[0m (\u001b[33mnlp-wut-2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/studio-lab-user/Generative_Models_for_CERN_Fast_Simulations/notebooks/gan_proton_15_2312/wandb/run-20230425_093014-uoa18dm0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/uoa18dm0' target=\"_blank\">15_2133_25_04_2023_09_30</a></strong> to <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations' target=\"_blank\">https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/uoa18dm0' target=\"_blank\">https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/uoa18dm0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/uoa18dm0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2af8e0c820>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Generative Models for CERN Fast Simulations\",\n",
    "    name=wandb_run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"Model\": NAME,\n",
    "    \"dataset\": \"proton_data\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"Date\": DATE_STR,\n",
    "    \"Proton_min\": photon_sum_proton_min,\n",
    "    \"Proton_max\": photon_sum_proton_max,\n",
    "    \"Experiment_dir_name\": EXPERIMENT_DIR_NAME\n",
    "    },\n",
    "    tags=[f\"proton_min_{photon_sum_proton_min}\",\n",
    "          f\"proton_max_{photon_sum_proton_max}\",\n",
    "          f\"gan_strength_{STRENGTH}\", \"sdi-gan\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "rMxBrHhsTDXO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "org=np.exp(x_test)-1\n",
    "ch_org = np.array(org).reshape(-1,56,30)\n",
    "ch_org = pd.DataFrame(sum_channels_parallel(ch_org)).values\n",
    "del org\n",
    "\n",
    "\n",
    "def calculate_ws_ch(n_calc):\n",
    "    ws= [0,0,0,0,0]\n",
    "    for j in range(n_calc):\n",
    "        z = np.random.normal(0,1,(x_test.shape[0],10))\n",
    "        z_c = y_test\n",
    "        results = generator.predict([z,z_c])\n",
    "        results = np.exp(results)-1\n",
    "        try:\n",
    "            ch_gen = np.array(results).reshape(-1,56,30)\n",
    "            ch_gen = pd.DataFrame(sum_channels_parallel(ch_gen)).values\n",
    "            for i in range(5):\n",
    "                ws[i] = ws[i] + wasserstein_distance(ch_org[:,i], ch_gen[:,i])\n",
    "            ws =np.array(ws)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "    ws = ws/n_calc\n",
    "    ws_mean = ws.sum()/5\n",
    "    print(\"ws mean\",f'{ws_mean:.2f}', end=\" \")\n",
    "    for n, score in enumerate(ws):\n",
    "        print(\"ch\"+str(n+1),f'{score:.2f}',end=\" \")\n",
    "    return ws_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "GmlMSiqCku5_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch,step):\n",
    "    images, images_2, cond, std, noise_cond = batch\n",
    "    step=step\n",
    "    BATCH_SIZE = tf.shape(images)[0]\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    noise_2 = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator([noise,noise_cond], training=True)\n",
    "        generated_images_2 = generator([noise_2,noise_cond], training=True)\n",
    "\n",
    "        real_output,real_latent  = discriminator([images,cond], training=True)\n",
    "        # real_output_2,real_latent_2  = discriminator([images_2,cond], training=True)\n",
    "\n",
    "        fake_output,fake_latent = discriminator([generated_images, noise_cond], training=True)\n",
    "        fake_output_2,fake_latent_2 = discriminator([generated_images_2, noise_cond], training=True)\n",
    "\n",
    "\n",
    "        gen_loss, div_loss = generator_loss(step,fake_output,\n",
    "                                          fake_latent, fake_latent_2, noise, noise_2, std)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "\n",
    "    #         generated_images = generator([noise,noise_cond], training=True)\n",
    "\n",
    "    #         real_output = discriminator([images,cond], training=True)\n",
    "    #         fake_output = discriminator([generated_images, noise_cond], training=True)\n",
    "\n",
    "    #         gen_loss = generator_loss(step, fake_output)\n",
    "    #         real_loss, fake_loss = discriminator_loss(real_output, fake_output)\n",
    "    #         disc_loss = real_loss + fake_loss\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, div_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "D-wATS0PkvJo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_EXPERIMENT_DATA:\n",
    "    filepath_mod = f\"../../{EXPERIMENT_DIR_NAME}/models/\"\n",
    "    create_dir(filepath_mod)\n",
    "\n",
    "history = []\n",
    "def train(dataset, epochs):\n",
    "    experiment_start = time.time()\n",
    "    tf_step = tf.Variable(0, dtype=float)\n",
    "    step=0\n",
    "\n",
    "    # generate first image\n",
    "    generate_and_save_images(generator,\n",
    "                             epochs,\n",
    "                             [seed, seed_cond])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        gen_loss_epoch = []\n",
    "        div_loss_epoch = []\n",
    "        disc_loss_epoch = []\n",
    "        for batch in dataset:\n",
    "            gen_loss, disc_loss, div_loss =train_step(batch,tf_step)\n",
    "            \n",
    "            history.append([gen_loss,disc_loss,\n",
    "                100*d_acc_r.result().numpy(),\n",
    "                100*d_acc_f.result().numpy(),\n",
    "                100*g_acc.result().numpy(),\n",
    "                ])\n",
    "            tf_step.assign_add(1)\n",
    "            step = step+1\n",
    "            \n",
    "            gen_loss_epoch.append(gen_loss)\n",
    "            disc_loss_epoch.append(disc_loss)\n",
    "            div_loss_epoch.append(div_loss)\n",
    "            if step % 100 == 0:\n",
    "                print(\"%d [D real acc: %.2f%%] [D fake acc: %.2f%%] [G acc: %.2f%%] \"% (\n",
    "                    step,\n",
    "                    100*d_acc_r.result().numpy(),\n",
    "                    100*d_acc_f.result().numpy(),\n",
    "                    100*g_acc.result().numpy()))\n",
    "\n",
    "        plot = generate_and_save_images(generator,\n",
    "                                 epoch,\n",
    "                                 [seed, seed_cond])\n",
    "        \n",
    "        if SAVE_EXPERIMENT_DATA:\n",
    "            # Save the model every epoch\n",
    "            generator.compile()\n",
    "            discriminator.compile()\n",
    "            generator.save((os.path.join(filepath_mod, \"gen_\"+NAME + \"_\"+ str(epoch) +\".h5\")))\n",
    "            discriminator.save((os.path.join(filepath_mod, \"disc_\"+NAME + \"_\"+ str(epoch) +\".h5\")))\n",
    "            np.savez(os.path.join(filepath_mod, \"history_\"+NAME+\".npz\"),np.array(history))\n",
    "\n",
    "        ws_mean = calculate_ws_ch(min(epoch//5+1,5))\n",
    "        \n",
    "        wandb.log({\n",
    "            'ws_mean': ws_mean,\n",
    "            'gen_loss': np.mean(gen_loss_epoch),\n",
    "            'div_loss': np.mean(div_loss_epoch),\n",
    "            'disc_loss': np.mean(disc_loss_epoch),\n",
    "            'epoch': epoch,\n",
    "            'plot': wandb.Image(plot),\n",
    "            'experiment_time': time.time()-experiment_start\n",
    "        })\n",
    "\n",
    "        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "CxeGwn7ek8Q-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_EXPERIMENT_DATA:\n",
    "    filepath_img = f\"../../{EXPERIMENT_DIR_NAME}/images/\"\n",
    "    create_dir(filepath_img)\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    \n",
    "    SUPTITLE_TXT = f\"\\nModel: GAN proton data\" \\\n",
    "               f\"\\nPhotonsum interval: [{photon_sum_proton_min}, {photon_sum_proton_max}]\" \\\n",
    "               f\"\\nEPOCH: {epoch}\"\n",
    "    \n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)  # returns 16 responses\n",
    "\n",
    "    fig, axs = plt.subplots(2, 7, figsize=(15, 5))\n",
    "    fig.suptitle(SUPTITLE_TXT, x=0.1, horizontalalignment='left')\n",
    "\n",
    "    for i in range(0, 14):\n",
    "        if i < 7:\n",
    "            x = x_test[20 + i].reshape(56, 30)\n",
    "        else:\n",
    "            x = predictions[i - 7].numpy().reshape(56, 30)\n",
    "        #x[x<=0]=x.max()*-0.1\n",
    "        im = axs[i // 7, i % 7].imshow(x, cmap='gnuplot')\n",
    "        axs[i // 7, i % 7].axis('off')\n",
    "        fig.colorbar(im, ax=axs[i // 7, i % 7])\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.975])\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        plt.savefig(os.path.join(filepath_img, 'image_at_epoch_{:04d}.png'.format(epoch)))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmR61h2W0vxC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = train(dataset_with_cond, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "5.GAN.ipynb",
   "provenance": [
    {
     "file_id": "1NUVAcZshSKsDKHaeeNNI8ohcUJvqUKqO",
     "timestamp": 1623369938483
    },
    {
     "file_id": "1bthIWyh_69c0sa09VMrRj9mXKOPNu7sP",
     "timestamp": 1623365007022
    },
    {
     "file_id": "1L77q4mrDL6LbE2HURRWfH8vtKm8pUBzK",
     "timestamp": 1620124217177
    },
    {
     "file_id": "1640z4JoZlvsSRGao2KCW322_UwjcQaSp",
     "timestamp": 1620084259696
    },
    {
     "file_id": "1zwFy4NM6SiPYsXCIgVUOEhBXbJ6Rj1tV",
     "timestamp": 1620082260980
    },
    {
     "file_id": "1IbQMqLHIYF-vg6B4Abbobl4pkROWBwes",
     "timestamp": 1620082045291
    },
    {
     "file_id": "1fnbp6zHVBrdi1nCGkrokht8i-pAUfIyf",
     "timestamp": 1620056039573
    },
    {
     "file_id": "1WXvm9ORGBKSJCVApOV4gJIC0NC7OQFzX",
     "timestamp": 1619997366147
    },
    {
     "file_id": "1hYuZL48eIXUGFk2mPXxeUh610OkoOfmV",
     "timestamp": 1619991567406
    },
    {
     "file_id": "1SYhFD0Djg7etn9UB47zsjc948E5VxTFG",
     "timestamp": 1616868089323
    },
    {
     "file_id": "17CrRwBBNZfhpN3y5pk_dTt7PelHqfFSI",
     "timestamp": 1616535329540
    },
    {
     "file_id": "1eTiiJCqFmQvy8KLbd0FSI1WfweHUCz-w",
     "timestamp": 1616531331246
    },
    {
     "file_id": "1Tndd1egGbtLRTO0Hnz6QnI9XiV5nEZKq",
     "timestamp": 1616528772133
    },
    {
     "file_id": "1kTX59Ymn4DGGgjVVQTt7ocE_oy5Sanpx",
     "timestamp": 1616281970086
    },
    {
     "file_id": "1kPO5iwHQPVcMuTc_Tn5JcSIRJ4hNrWjH",
     "timestamp": 1616181812747
    },
    {
     "file_id": "1nkcov8gu5MJRqWxL1awc78jThGTXsTyZ",
     "timestamp": 1616013741366
    },
    {
     "file_id": "13iHBeXIFuSw6-EvChxlqiveFVFurjENx",
     "timestamp": 1615923191272
    },
    {
     "file_id": "1q1UXZxrBrZdwYFvuOtkaRAnqykzUqK1-",
     "timestamp": 1615843147388
    },
    {
     "file_id": "1NUjBm8LUmJ5_Cu3ithC2JLJ4aFAWFEm2",
     "timestamp": 1615585196418
    },
    {
     "file_id": "1Y8oTevpUwSdSV3oCdx_e37mkhaxPzrGf",
     "timestamp": 1615506244659
    }
   ]
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
