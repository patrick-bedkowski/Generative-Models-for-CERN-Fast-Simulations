{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended absolute path: /home/studio-lab-user/Generative_Models_for_CERN_Fast_Simulations/utils\n"
     ]
    }
   ],
   "source": [
    "# for running in collab, sagemaker etc.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def _append_sys_path_of_utils():\n",
    "    \"\"\"\n",
    "    Appends an utils directory to a system path so it is visible for the script.\n",
    "    \"\"\"\n",
    "    UTILS_RELATIVE_PATH = \"../../utils/\"\n",
    "\n",
    "    absolute_path_of_utils = os.path.abspath(UTILS_RELATIVE_PATH)\n",
    "    if absolute_path_of_utils:\n",
    "        if absolute_path_of_utils in sys.path:\n",
    "            print(f\"Absolute path already in sys.path: {absolute_path_of_utils}\")\n",
    "        else:\n",
    "            print(f\"Appended absolute path: {absolute_path_of_utils}\")\n",
    "            sys.path.insert(0, absolute_path_of_utils)\n",
    "    else:\n",
    "        print(\"Absolute path of utils module not found.\")\n",
    "\n",
    "\n",
    "_append_sys_path_of_utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbedkowski-patrick\u001b[0m (\u001b[33mnlp-wut-2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 21:18:30.252967: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-10 21:18:30.253844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-10 21:18:30.253852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "re6ywGTYfdzE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, decomposition, manifold, preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import time\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel as sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_EXPERIMENT_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CtnTQjy0Q05",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9tDJ602Bolmd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (48714, 56, 30) max: 678.0\n"
     ]
    }
   ],
   "source": [
    "data_proton = pd.read_pickle('../../data/data_proton_photonsum_n_15_2133_p_15_3273.pkl')\n",
    "print('Loaded: ',  data_proton.shape, \"max:\", data_proton.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (48714, 44, 44) max: 532.0\n"
     ]
    }
   ],
   "source": [
    "data_neutron = pd.read_pickle('../../data/data_neutron_photonsum_n_15_2133_p_15_3273.pkl')\n",
    "print('Loaded: ',  data_neutron.shape, \"max:\", data_neutron.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cond:  (48714, 15) max: 7000.0 min: -7000.0\n"
     ]
    }
   ],
   "source": [
    "data_cond = pd.read_pickle('../../data/data_cond_stddev_photonsum_p_15_2133_n_15_3273.pkl')\n",
    "print('Loaded cond: ',  data_cond.shape, \"max:\", data_cond.values.max(), \"min:\", data_cond.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate min max proton, neutron sum\n",
    "photon_sum_proton_min, photon_sum_proton_max = data_cond.proton_photon_sum.min(), data_cond.proton_photon_sum.max()\n",
    "photon_sum_neutron_min, photon_sum_neutron_max = data_cond.neutron_photon_sum.min(), data_cond.neutron_photon_sum.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Energy', 'Vx', 'Vy', 'Vz', 'Px', 'Py', 'Pz', 'mass', 'charge',\n",
       "        'std_proton', 'std_neutron'],\n",
       "       dtype='object'),\n",
       " 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond.drop(columns=['proton_photon_sum', 'neutron_photon_sum', 'group_number_proton', 'group_number_neutron'], inplace=True)\n",
    "data_cond.columns, len(data_cond.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>Pz</th>\n",
       "      <th>mass</th>\n",
       "      <th>charge</th>\n",
       "      <th>std_proton</th>\n",
       "      <th>std_neutron</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3192.38</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.182957</td>\n",
       "      <td>-3192.38</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076311</td>\n",
       "      <td>0.632134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3961.55</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.076487</td>\n",
       "      <td>0.179845</td>\n",
       "      <td>3961.55</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.127533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2770.10</td>\n",
       "      <td>1.861170e-17</td>\n",
       "      <td>2.517190e-17</td>\n",
       "      <td>-1.689330e-13</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>0.412760</td>\n",
       "      <td>-2770.10</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090233</td>\n",
       "      <td>0.779230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3195.12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>-0.149980</td>\n",
       "      <td>3195.11</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131331</td>\n",
       "      <td>0.259529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1714.07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.457768</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>-1714.07</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103628</td>\n",
       "      <td>0.355864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Energy            Vx            Vy            Vz        Px        Py  \\\n",
       "0  3192.38  0.000000e+00  0.000000e+00  0.000000e+00  0.022422 -0.182957   \n",
       "1  3961.55  0.000000e+00  0.000000e+00  0.000000e+00 -0.076487  0.179845   \n",
       "2  2770.10  1.861170e-17  2.517190e-17 -1.689330e-13  0.305187  0.412760   \n",
       "3  3195.12  0.000000e+00  0.000000e+00  0.000000e+00  0.560528 -0.149980   \n",
       "4  1714.07  0.000000e+00  0.000000e+00  0.000000e+00  0.457768  0.145639   \n",
       "\n",
       "        Pz        mass  charge  std_proton  std_neutron  \n",
       "0 -3192.38  939.565413     0.0    0.076311     0.632134  \n",
       "1  3961.55  938.272081     1.0    0.479201     0.127533  \n",
       "2 -2770.10  497.611000     0.0    0.090233     0.779230  \n",
       "3  3195.11  938.272081     1.0    0.131331     0.259529  \n",
       "4 -1714.07  939.565413     0.0    0.103628     0.355864  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment DIR:  experiments/sdi-gan-padded_15_2133_10_08_2023_21_18\n"
     ]
    }
   ],
   "source": [
    "STRENGTH = 0.1\n",
    "\n",
    "DATE_STR = datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "\n",
    "NAME = \"sdi-gan-padded\"\n",
    "\n",
    "wandb_run_name = f\"{int(photon_sum_proton_min)}_{int(photon_sum_proton_max)}_{DATE_STR}\"\n",
    "\n",
    "EXPERIMENT_DIR_NAME = f\"experiments/{NAME}_{int(photon_sum_proton_min)}_{int(photon_sum_proton_max)}_{DATE_STR}\"\n",
    "\n",
    "print(\"Experiment DIR: \", EXPERIMENT_DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_scales(model_name, scaler_means, scaler_scales):\n",
    "    out_fnm = f\"{model_name}_scales.txt\"\n",
    "    res = \"#means\"\n",
    "    for mean_ in scaler_means:\n",
    "        res += \"\\n\" + str(mean_)\n",
    "    res += \"\\n\\n#scales\"\n",
    "    for scale_ in scaler_scales:\n",
    "        res += \"\\n\" + str(scale_)\n",
    "\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        filepath = f\"../../{EXPERIMENT_DIR_NAME}/scales/\"\n",
    "        create_dir(filepath)\n",
    "        with open(filepath+out_fnm, mode=\"w\") as f:\n",
    "            f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>Pz</th>\n",
       "      <th>mass</th>\n",
       "      <th>charge</th>\n",
       "      <th>std_proton</th>\n",
       "      <th>std_neutron</th>\n",
       "      <th>cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3192.38</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.182957</td>\n",
       "      <td>-3192.38</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076311</td>\n",
       "      <td>0.632134</td>\n",
       "      <td>3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3961.55</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.076487</td>\n",
       "      <td>0.179845</td>\n",
       "      <td>3961.55</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.127533</td>\n",
       "      <td>3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2770.10</td>\n",
       "      <td>1.861170e-17</td>\n",
       "      <td>2.517190e-17</td>\n",
       "      <td>-1.689330e-13</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>0.412760</td>\n",
       "      <td>-2770.10</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090233</td>\n",
       "      <td>0.779230</td>\n",
       "      <td>2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3195.12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>-0.149980</td>\n",
       "      <td>3195.11</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131331</td>\n",
       "      <td>0.259529</td>\n",
       "      <td>3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1714.07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.457768</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>-1714.07</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103628</td>\n",
       "      <td>0.355864</td>\n",
       "      <td>1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Energy            Vx            Vy            Vz        Px        Py  \\\n",
       "0  3192.38  0.000000e+00  0.000000e+00  0.000000e+00  0.022422 -0.182957   \n",
       "1  3961.55  0.000000e+00  0.000000e+00  0.000000e+00 -0.076487  0.179845   \n",
       "2  2770.10  1.861170e-17  2.517190e-17 -1.689330e-13  0.305187  0.412760   \n",
       "3  3195.12  0.000000e+00  0.000000e+00  0.000000e+00  0.560528 -0.149980   \n",
       "4  1714.07  0.000000e+00  0.000000e+00  0.000000e+00  0.457768  0.145639   \n",
       "\n",
       "        Pz        mass  charge  std_proton  std_neutron  \\\n",
       "0 -3192.38  939.565413     0.0    0.076311     0.632134   \n",
       "1  3961.55  938.272081     1.0    0.479201     0.127533   \n",
       "2 -2770.10  497.611000     0.0    0.090233     0.779230   \n",
       "3  3195.11  938.272081     1.0    0.131331     0.259529   \n",
       "4 -1714.07  939.565413     0.0    0.103628     0.355864   \n",
       "\n",
       "                                                cond  \n",
       "0  3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...  \n",
       "1  3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...  \n",
       "2  2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...  \n",
       "3  3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...  \n",
       "4  1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond[\"cond\"] = data_cond[\"Energy\"].astype(str) +\"|\"+ data_cond[\"Vx\"].astype(str) +\"|\"+  data_cond[\"Vy\"].astype(str) +\"|\"+ data_cond[\"Vz\"].astype(str) +\"|\"+  data_cond[\"Px\"].astype(str) +\"|\"+  data_cond[\"Py\"].astype(str) +\"|\"+ data_cond[\"Pz\"].astype(str) +\"|\"+  data_cond[\"mass\"].astype(str) +\"|\"+  data_cond[\"charge\"].astype(str)\n",
    "data_cond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48709</th>\n",
       "      <td>48709</td>\n",
       "      <td>1456.66|7.79676e-18|2.26923e-18|-4.06504e-14|0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48710</th>\n",
       "      <td>48710</td>\n",
       "      <td>3812.18|0.0|0.0|0.0|0.0529534|0.402807|-3812.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48711</th>\n",
       "      <td>48711</td>\n",
       "      <td>1422.39|1.1825599999999999e-05|-1.3857e-06|0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48712</th>\n",
       "      <td>48712</td>\n",
       "      <td>4071.23|0.0|0.0|0.0|0.089305|0.0694104|-4071.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48713</th>\n",
       "      <td>48713</td>\n",
       "      <td>4003.6|0.0|0.0|0.0|0.769178|-0.276742999999999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48714 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                               cond\n",
       "0          0  3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...\n",
       "1          1  3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...\n",
       "2          2  2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...\n",
       "3          3  3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...\n",
       "4          4  1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...\n",
       "...      ...                                                ...\n",
       "48709  48709  1456.66|7.79676e-18|2.26923e-18|-4.06504e-14|0...\n",
       "48710  48710  3812.18|0.0|0.0|0.0|0.0529534|0.402807|-3812.1...\n",
       "48711  48711  1422.39|1.1825599999999999e-05|-1.3857e-06|0.0...\n",
       "48712  48712  4071.23|0.0|0.0|0.0|0.089305|0.0694104|-4071.2...\n",
       "48713  48713  4003.6|0.0|0.0|0.0|0.769178|-0.276742999999999...\n",
       "\n",
       "[48714 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond_id = data_cond[[\"cond\"]].reset_index()\n",
    "data_cond_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index_x\n",
       "0        12888\n",
       "1        42140\n",
       "2         9280\n",
       "3        22415\n",
       "4         6122\n",
       "         ...  \n",
       "48709    14586\n",
       "48710    12543\n",
       "48711    18948\n",
       "48712    32413\n",
       "48713    29295\n",
       "Name: index_y, Length: 48714, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a random index of the same conditional data\n",
    "# shuffle the data and merge it according to the conditional data. Pick the first index of the grouped conditional data\n",
    "# if some unique conditional data has only one index of sample, then pair it with the same index \n",
    "ids = data_cond_id.merge(data_cond_id.sample(frac=1), on=[\"cond\"], how=\"inner\").groupby(\"index_x\").first()\n",
    "ids = ids[\"index_y\"]\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea\n",
    "Eliminate samples that are unique in terms of conditional data leaving only these responses that have few samples grouped by condtional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have two conditional datasets and two response datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>Pz</th>\n",
       "      <th>mass</th>\n",
       "      <th>charge</th>\n",
       "      <th>std_proton</th>\n",
       "      <th>std_neutron</th>\n",
       "      <th>cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3192.38</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.182957</td>\n",
       "      <td>-3192.38</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076311</td>\n",
       "      <td>0.632134</td>\n",
       "      <td>3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3961.55</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.076487</td>\n",
       "      <td>0.179845</td>\n",
       "      <td>3961.55</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.127533</td>\n",
       "      <td>3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2770.10</td>\n",
       "      <td>1.861170e-17</td>\n",
       "      <td>2.517190e-17</td>\n",
       "      <td>-1.689330e-13</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>0.412760</td>\n",
       "      <td>-2770.10</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090233</td>\n",
       "      <td>0.779230</td>\n",
       "      <td>2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3195.12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>-0.149980</td>\n",
       "      <td>3195.11</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131331</td>\n",
       "      <td>0.259529</td>\n",
       "      <td>3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1714.07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.457768</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>-1714.07</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103628</td>\n",
       "      <td>0.355864</td>\n",
       "      <td>1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48709</th>\n",
       "      <td>1456.66</td>\n",
       "      <td>7.796760e-18</td>\n",
       "      <td>2.269230e-18</td>\n",
       "      <td>-4.065040e-14</td>\n",
       "      <td>0.279388</td>\n",
       "      <td>0.081315</td>\n",
       "      <td>-1456.66</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062655</td>\n",
       "      <td>0.654967</td>\n",
       "      <td>1456.66|7.79676e-18|2.26923e-18|-4.06504e-14|0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48710</th>\n",
       "      <td>3812.18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.052953</td>\n",
       "      <td>0.402807</td>\n",
       "      <td>-3812.18</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.362445</td>\n",
       "      <td>0.144962</td>\n",
       "      <td>3812.18|0.0|0.0|0.0|0.0529534|0.402807|-3812.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48711</th>\n",
       "      <td>1422.39</td>\n",
       "      <td>1.182560e-05</td>\n",
       "      <td>-1.385700e-06</td>\n",
       "      <td>3.655780e-02</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>-0.052227</td>\n",
       "      <td>1422.39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>0.105028</td>\n",
       "      <td>1422.39|1.1825599999999999e-05|-1.3857e-06|0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48712</th>\n",
       "      <td>4071.23</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.089305</td>\n",
       "      <td>0.069410</td>\n",
       "      <td>-4071.23</td>\n",
       "      <td>1115.683000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.422491</td>\n",
       "      <td>0.886010</td>\n",
       "      <td>4071.23|0.0|0.0|0.0|0.089305|0.0694104|-4071.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48713</th>\n",
       "      <td>4003.60</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.769178</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>4003.60</td>\n",
       "      <td>1115.683000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153674</td>\n",
       "      <td>0.803139</td>\n",
       "      <td>4003.6|0.0|0.0|0.0|0.769178|-0.276742999999999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48714 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Energy            Vx            Vy            Vz        Px        Py  \\\n",
       "0      3192.38  0.000000e+00  0.000000e+00  0.000000e+00  0.022422 -0.182957   \n",
       "1      3961.55  0.000000e+00  0.000000e+00  0.000000e+00 -0.076487  0.179845   \n",
       "2      2770.10  1.861170e-17  2.517190e-17 -1.689330e-13  0.305187  0.412760   \n",
       "3      3195.12  0.000000e+00  0.000000e+00  0.000000e+00  0.560528 -0.149980   \n",
       "4      1714.07  0.000000e+00  0.000000e+00  0.000000e+00  0.457768  0.145639   \n",
       "...        ...           ...           ...           ...       ...       ...   \n",
       "48709  1456.66  7.796760e-18  2.269230e-18 -4.065040e-14  0.279388  0.081315   \n",
       "48710  3812.18  0.000000e+00  0.000000e+00  0.000000e+00  0.052953  0.402807   \n",
       "48711  1422.39  1.182560e-05 -1.385700e-06  3.655780e-02  0.511508 -0.052227   \n",
       "48712  4071.23  0.000000e+00  0.000000e+00  0.000000e+00  0.089305  0.069410   \n",
       "48713  4003.60  0.000000e+00  0.000000e+00  0.000000e+00  0.769178 -0.276743   \n",
       "\n",
       "            Pz         mass  charge  std_proton  std_neutron  \\\n",
       "0     -3192.38   939.565413     0.0    0.076311     0.632134   \n",
       "1      3961.55   938.272081     1.0    0.479201     0.127533   \n",
       "2     -2770.10   497.611000     0.0    0.090233     0.779230   \n",
       "3      3195.11   938.272081     1.0    0.131331     0.259529   \n",
       "4     -1714.07   939.565413     0.0    0.103628     0.355864   \n",
       "...        ...          ...     ...         ...          ...   \n",
       "48709 -1456.66   497.611000     0.0    0.062655     0.654967   \n",
       "48710 -3812.18   938.272081     1.0    0.362445     0.144962   \n",
       "48711  1422.39     0.000000     0.0    0.048635     0.105028   \n",
       "48712 -4071.23  1115.683000     0.0    0.422491     0.886010   \n",
       "48713  4003.60  1115.683000     0.0    0.153674     0.803139   \n",
       "\n",
       "                                                    cond  \n",
       "0      3192.38|0.0|0.0|0.0|0.022422400000000002|-0.18...  \n",
       "1      3961.55|0.0|0.0|0.0|-0.0764874|0.179845|3961.5...  \n",
       "2      2770.1|1.86117e-17|2.5171900000000004e-17|-1.6...  \n",
       "3      3195.12|0.0|0.0|0.0|0.560528|-0.14998|3195.11|...  \n",
       "4      1714.07|0.0|0.0|0.0|0.457768|0.145639000000000...  \n",
       "...                                                  ...  \n",
       "48709  1456.66|7.79676e-18|2.26923e-18|-4.06504e-14|0...  \n",
       "48710  3812.18|0.0|0.0|0.0|0.0529534|0.402807|-3812.1...  \n",
       "48711  1422.39|1.1825599999999999e-05|-1.3857e-06|0.0...  \n",
       "48712  4071.23|0.0|0.0|0.0|0.089305|0.0694104|-4071.2...  \n",
       "48713  4003.6|0.0|0.0|0.0|0.769178|-0.276742999999999...  \n",
       "\n",
       "[48714 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10522,
     "status": "ok",
     "timestamp": 1623609981138,
     "user": {
      "displayName": "Jan Dubiński",
      "photoUrl": "",
      "userId": "04866767089811362617"
     },
     "user_tz": -120
    },
    "id": "g5jISZN7WrvL",
    "outputId": "0cf63b56-f149-4ff0-d3a8-972fd3b7dca3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 6.5206213 min 0.0\n",
      "data max 6.2785215 min 0.0\n",
      "std max 1.0 min 0.0\n",
      "std max 1.0 min 0.0\n"
     ]
    }
   ],
   "source": [
    "data_proton = np.log(data_proton+1)\n",
    "data_proton = np.float32(data_proton)\n",
    "print(\"data max\", data_proton.max(), \"min\", data_proton.min())\n",
    "\n",
    "data_proton_2 = data_proton[ids]\n",
    "\n",
    "data_neutron = np.log(data_neutron+1)\n",
    "data_neutron = np.float32(data_neutron)\n",
    "print(\"data max\", data_neutron.max(), \"min\", data_neutron.min())\n",
    "\n",
    "data_neutron_2 = data_neutron[ids]\n",
    "\n",
    "data_cond = data_cond.drop(columns=\"cond\")\n",
    "\n",
    "scaler_proton = MinMaxScaler()\n",
    "std_proton = data_cond[\"std_proton\"].values.reshape(-1,1)\n",
    "std_proton = np.float32(std_proton)\n",
    "std_proton = scaler_proton.fit_transform(std_proton)\n",
    "print(\"std max\", std_proton.max(), \"min\", std_proton.min())\n",
    "\n",
    "scaler_neutron = MinMaxScaler()\n",
    "std_neutron = data_cond[\"std_neutron\"].values.reshape(-1,1)\n",
    "std_neutron = np.float32(std_neutron)\n",
    "std_neutron = scaler_neutron.fit_transform(std_neutron)\n",
    "print(\"std max\", std_neutron.max(), \"min\", std_neutron.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond max 21.432093 min -35.632454\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "data_cond = np.float32(data_cond.drop(columns=[\"std_proton\", \"std_neutron\"]))\n",
    "data_cond = scaler.fit_transform(data_cond)\n",
    "print(\"cond max\", data_cond.max(), \"min\", data_cond.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1623609981462,
     "user": {
      "displayName": "Jan Dubiński",
      "photoUrl": "",
      "userId": "04866767089811362617"
     },
     "user_tz": -120
    },
    "id": "45N-b-FGn4CO",
    "outputId": "9110f272-5c82-4a0b-dd31-2234414dfe5a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38971, 56, 30) (9743, 56, 30) (38971, 56, 30) (9743, 56, 30) (38971, 44, 44) (9743, 44, 44) (38971, 44, 44) (9743, 44, 44)\n",
      "(38971, 9) (9743, 9) (38971, 1) (9743, 1) (38971, 1) (9743, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_p, x_test_p, x_train_p_2, x_test_p_2, x_train_n, x_test_n, x_train_n_2, x_test_n_2, y_train, y_test, std_proton_train, std_proton_test, std_neutron_train, std_neutron_test = train_test_split(\n",
    "data_proton, data_proton_2, data_neutron, data_neutron_2, data_cond, std_proton, std_neutron, test_size=0.2, shuffle=False)\n",
    "print(x_train_p.shape, x_test_p.shape, x_train_p_2.shape, x_test_p_2.shape, x_train_n.shape, x_test_n.shape, x_train_n_2.shape, x_test_n_2.shape)\n",
    "print(y_train.shape, y_test.shape, std_proton_train.shape, std_proton_test.shape, std_neutron_train.shape, std_neutron_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save scales\n",
    "if SAVE_EXPERIMENT_DATA:\n",
    "    save_scales(\"Proton\", scaler.mean_, scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x_train_p, x_test_p,\n",
    "- x_train_p_2, x_test_p_2,\n",
    "- x_train_n, x_test_n,\n",
    "- x_train_n_2, x_test_n_2,\n",
    "- y_train, y_test,\n",
    "- std_proton_train, std_proton_test,\n",
    "- std_neutron_train, std_neutron_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9FMxwgNpn-CU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# Training dataset\n",
    "\n",
    "# datasets that in each index contain two samples from the same conditional data\n",
    "dataset_p = tf.data.Dataset.from_tensor_slices(x_train_p).batch(batch_size=BATCH_SIZE)\n",
    "dataset_p_2 = tf.data.Dataset.from_tensor_slices(x_train_p_2).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "dataset_n = tf.data.Dataset.from_tensor_slices(x_train_n).batch(batch_size=BATCH_SIZE)\n",
    "dataset_n_2 = tf.data.Dataset.from_tensor_slices(x_train_n_2).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "# conditional data\n",
    "dataset_cond = tf.data.Dataset.from_tensor_slices(y_train).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "# standard deviation for each conditional data samples\n",
    "dataset_std_proton = tf.data.Dataset.from_tensor_slices(std_proton_train).batch(batch_size=BATCH_SIZE)\n",
    "dataset_std_neutron = tf.data.Dataset.from_tensor_slices(std_neutron_train).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "# shuffled conditional data\n",
    "fake_cond = tf.data.Dataset.from_tensor_slices(y_train).shuffle(12800).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "# zipped data\n",
    "dataset_with_cond = tf.data.Dataset.zip((dataset_p, dataset_p_2,\n",
    "                                         dataset_n, dataset_n_2,\n",
    "                                         dataset_cond, dataset_std_proton, dataset_std_neutron, fake_cond)).shuffle(12800)\n",
    "\n",
    "# Validation dataset\n",
    "\n",
    "val_dataset_p = tf.data.Dataset.from_tensor_slices(x_test_p).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_p_2 = tf.data.Dataset.from_tensor_slices(x_test_p_2).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset_n = tf.data.Dataset.from_tensor_slices(x_test_n).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_n_2 = tf.data.Dataset.from_tensor_slices(x_test_n_2).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset_cond = tf.data.Dataset.from_tensor_slices(y_test).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_std_proton = tf.data.Dataset.from_tensor_slices(std_proton_test).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_std_neutron = tf.data.Dataset.from_tensor_slices(std_neutron_test).batch(batch_size=BATCH_SIZE)\n",
    "val_fake_cond =  tf.data.Dataset.from_tensor_slices(y_test).shuffle(12800).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset_with_cond = tf.data.Dataset.zip((val_dataset_p, val_dataset_p_2,\n",
    "                                             val_dataset_n, val_dataset_n_2,\n",
    "                                             val_dataset_cond, val_dataset_std_proton, val_dataset_std_neutron, val_fake_cond)).shuffle(12800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41Ri3GB8n7oI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "iFhM4_mYfdzJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 17)           0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          4608        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256)         1024        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 256)          0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30720)        7895040     ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 30720)       122880      ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30720)        0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 30720)        0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 20, 12, 128)  0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 60, 24, 128)  0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 59, 19, 256)  393472      ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 55, 23, 256)  393472      ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 59, 19, 256)  1024       ['conv2d[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 55, 23, 256)  1024       ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 59, 19, 256)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 55, 23, 256)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 59, 19, 256)  0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 55, 23, 256)  0           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 59, 38, 256)  0          ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 55, 46, 256)  0          ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 58, 33, 128)  393344      ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 50, 45, 128)  393344      ['up_sampling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 58, 33, 128)  512        ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 50, 45, 128)  512        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 58, 33, 128)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 50, 45, 128)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 58, 33, 128)  0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 50, 45, 128)  0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 57, 30, 64)   65600       ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 45, 44, 64)   98368       ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 57, 30, 64)  256         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 45, 44, 64)  256         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 57, 30, 64)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 45, 44, 64)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 57, 30, 64)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 45, 44, 64)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 56, 30, 1)    129         ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 44, 44, 1)    129         ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,764,994\n",
      "Trainable params: 9,701,250\n",
      "Non-trainable params: 63,744\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img_proton (InputLayer)  [(None, 56, 30, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_img_neutron (InputLayer)  [(None, 44, 44, 1)]  0          []                               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 54, 28, 32)   320         ['input_img_proton[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 42, 42, 32)   320         ['input_img_neutron[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 54, 28, 32)  128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 42, 42, 32)  128         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 54, 28, 32)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 42, 42, 32)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 54, 28, 32)   0           ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 42, 42, 32)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 27, 14, 32)   0           ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 21, 21, 32)  0           ['leaky_re_lu_10[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 25, 12, 16)   4624        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 19, 19, 16)   4624        ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 25, 12, 16)  64          ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 19, 19, 16)  64          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 25, 12, 16)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 19, 19, 16)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 25, 12, 16)   0           ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, 19, 19, 16)   0           ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 16)  0           ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 9, 19, 16)   0           ['leaky_re_lu_11[0][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2304)         0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2736)         0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 5049)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          646400      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 128)         512         ['dense_2[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 128)          0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 128)          0           ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8256        ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 64)          256         ['dense_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 64)           0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 64)           0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            130         ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 665,826\n",
      "Trainable params: 665,250\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# OLD Architecture\n",
    "\n",
    "latent_dim = 8\n",
    "cond_dim = 9\n",
    "\n",
    "############################ generator ############################\n",
    "\n",
    "x = Input(shape=(latent_dim,))\n",
    "cond = Input(shape=(cond_dim,))\n",
    "inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "\n",
    "# PROTON HEAD\n",
    "layer_1 = Dense(128*2)(inputs)\n",
    "layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "layer_2 = Dense(128*20*12)(layer_1_a)\n",
    "layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "reshaped = Reshape((20,12,128))(layer_2_a)\n",
    "reshaped_s = UpSampling2D(size=(3,2))(reshaped)\n",
    "\n",
    "# PROTON HEAD\n",
    "conv1 = Conv2D(256, kernel_size=(2, 6), padding='valid')(reshaped_s)\n",
    "conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "conv1_a_s = UpSampling2D(size=(1,2))(conv1_a)\n",
    "\n",
    "conv2 = Conv2D(128, kernel_size=(2, 6))(conv1_a_s)\n",
    "conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "conv3 = Conv2D(64, kernel_size=(2, 4))(conv2_a)\n",
    "conv3_bd = Dropout(0.2)(BatchNormalization()(conv3))\n",
    "conv3_a = LeakyReLU(alpha=0.1)(conv3_bd)\n",
    "\n",
    "# NEUTRON HEAD\n",
    "conv4 = Conv2D(256, kernel_size=(6, 2), padding='valid')(reshaped_s)\n",
    "conv4_bd = Dropout(0.2)(BatchNormalization()(conv4))\n",
    "conv4_a = LeakyReLU(alpha=0.1)(conv4_bd)\n",
    "conv4_a_s = UpSampling2D(size=(1,2))(conv4_a)\n",
    "\n",
    "conv5 = Conv2D(128, kernel_size=(6, 2))(conv4_a_s)\n",
    "conv5_bd = Dropout(0.2)(BatchNormalization()(conv5))\n",
    "conv5_a = LeakyReLU(alpha=0.1)(conv5_bd)\n",
    "\n",
    "conv6 = Conv2D(64, kernel_size=(6, 2))(conv5_a)\n",
    "conv6_bd = Dropout(0.2)(BatchNormalization()(conv6))\n",
    "conv6_a = LeakyReLU(alpha=0.1)(conv6_bd)\n",
    "\n",
    "outputs_proton = Conv2D(1, kernel_size=(2, 1), activation='relu')(conv3_a)\n",
    "outputs_neutron = Conv2D(1, kernel_size=(2, 1), activation='relu')(conv6_a)\n",
    "\n",
    "generator = Model([x, cond], [outputs_proton, outputs_neutron], name='generator')\n",
    "generator.summary()\n",
    "\n",
    "############################ discriminator ############################\n",
    "\n",
    "# PROTON IMAGE\n",
    "input_img_proton = Input(shape=[56, 30, 1], name='input_img_proton')\n",
    "conv1 = Conv2D(32, kernel_size=3)(input_img_proton)\n",
    "conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "\n",
    "conv2 = Conv2D(16, kernel_size=3)(pool1)\n",
    "conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 1))(conv2_a)\n",
    "\n",
    "flat_proton = Flatten()(pool2)\n",
    "\n",
    "# NEUTRON IMAGE\n",
    "input_img_neutron = Input(shape=[44, 44, 1], name='input_img_neutron')\n",
    "conv3 = Conv2D(32, kernel_size=3)(input_img_neutron)\n",
    "conv3_bd = Dropout(0.2)(BatchNormalization()(conv3))\n",
    "conv3_a = LeakyReLU(alpha=0.1)(conv3_bd)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3_a)\n",
    "\n",
    "conv4 = Conv2D(16, kernel_size=3)(pool3)\n",
    "conv4_bd = Dropout(0.2)(BatchNormalization()(conv4))\n",
    "conv4_a = LeakyReLU(alpha=0.1)(conv4_bd)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 1))(conv4_a)\n",
    "\n",
    "flat_neutron = Flatten()(pool4)\n",
    "\n",
    "# CONDITIONAL\n",
    "cond = Input(shape=(cond_dim,))\n",
    "\n",
    "inputs2 = Concatenate(axis=1)([flat_proton, flat_neutron, cond])\n",
    "\n",
    "layer_1 = Dense(128)(inputs2)\n",
    "layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "layer_2 = Dense(64)(layer_1_a)\n",
    "layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "outputs = Dense(2, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "discriminator = Model([input_img_proton, input_img_neutron, cond], [outputs, layer_2_a], name='discriminator')\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Idea\n",
    "Increase number of latent dimensions with the old architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA\n",
    "Why do we only take into account a loss of only single generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discriminator_loss(real_output, fake_output, fake_output_2):\n",
    "#     real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "#     fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "#     fake_loss_2 = cross_entropy(tf.zeros_like(fake_output_2), fake_output_2)\n",
    "#     total_loss = real_loss + fake_loss + fake_loss_2\n",
    "    \n",
    "#     # update state of accuracy of real and false images\n",
    "#     d_acc_r.update_state(tf.ones_like(real_output), real_output)\n",
    "#     d_acc_f.update_state(tf.zeros_like(fake_output), fake_output)\n",
    "#     return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "87NnkVJwaCOo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    # update state of accuracy of real and false images\n",
    "    d_acc_r.update_state(tf.ones_like(real_output), real_output)\n",
    "    d_acc_f.update_state(tf.zeros_like(fake_output), fake_output)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ZXwATQ9uaigO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "HqTYRo-uki5k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "d_acc_r = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "d_acc_f = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "g_acc = keras.metrics.BinaryAccuracy(name=\"g_acc_g\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA\n",
    "Why do we only take into accoutn a single genrated image, not from two latent codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generator_loss(step, fake_output, fake_output_2,\n",
    "#                    fake_latent, fake_latent_2,\n",
    "#                    noise, noise_2,\n",
    "#                    std_proton, std_neutron):\n",
    "\n",
    "#     g_acc.update_state(tf.ones_like(fake_output), fake_output)\n",
    "#     g_acc.update_state(tf.ones_like(fake_output_2), fake_output_2)\n",
    "\n",
    "#     crossentropy_loss = cross_entropy(tf.ones_like(fake_output), fake_output) + cross_entropy(tf.ones_like(fake_output_2), fake_output_2)\n",
    "\n",
    "#     div = tf.math.divide(tf.reduce_mean(tf.abs(fake_latent - fake_latent_2),(1)), tf.reduce_mean(tf.abs(noise-noise_2),(1)))\n",
    "\n",
    "#     div_loss_proton = std_proton * STRENGTH / (div + 1e-5)\n",
    "#     div_loss_neutron = std_neutron * STRENGTH / (div + 1e-5)\n",
    "\n",
    "#     div_loss_proton = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_proton,(1)), div_loss_proton))\n",
    "#     div_loss_neutron = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_neutron,(1)), div_loss_neutron))\n",
    "#     div_loss = div_loss_proton + div_loss_neutron\n",
    "#     return crossentropy_loss + div_loss, div_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA\n",
    "Should we take mean of diversity loss or just sum the up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yjX97hnkkmlf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(step, fake_output,\n",
    "                   fake_latent, fake_latent_2,\n",
    "                   noise, noise_2,\n",
    "                   std_proton, std_neutron):\n",
    "\n",
    "    g_acc.update_state(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    crossentropy_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    div = tf.math.divide(tf.reduce_mean(tf.abs(fake_latent - fake_latent_2),(1)), tf.reduce_mean(tf.abs(noise-noise_2),(1)))\n",
    "\n",
    "    div_loss_proton = std_proton * STRENGTH / (div + 1e-5)\n",
    "    div_loss_neutron = std_neutron * STRENGTH / (div + 1e-5)\n",
    "\n",
    "    div_loss_proton = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_proton,(1)), div_loss_proton))\n",
    "    div_loss_neutron = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_neutron,(1)), div_loss_neutron))\n",
    "\n",
    "    # average diversity loss\n",
    "    div_loss = div_loss_proton + div_loss_neutron\n",
    "    return crossentropy_loss + div_loss, div_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "I4HsHLgwkurp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "noise_dim = latent_dim\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "START_GENERATING_IMG_FROM_IDX = 20\n",
    "# Seed to reuse for generating samples for comparison during training\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed_cond = y_test[START_GENERATING_IMG_FROM_IDX:START_GENERATING_IMG_FROM_IDX+num_examples_to_generate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbedkowski-patrick\u001b[0m (\u001b[33mnlp-wut-2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/studio-lab-user/Generative_Models_for_CERN_Fast_Simulations/notebooks/gan_proton_15_2312/wandb/run-20230810_211854-un2lfsny</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/un2lfsny' target=\"_blank\">15_2133_10_08_2023_21_18</a></strong> to <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations' target=\"_blank\">https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/un2lfsny' target=\"_blank\">https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/un2lfsny</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/un2lfsny?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f50e0570fd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Generative Models for CERN Fast Simulations\",\n",
    "    name=wandb_run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"Model\": NAME,\n",
    "    \"dataset\": \"proton_neutron_data\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"Date\": DATE_STR,\n",
    "    \"latent_dimension\": latent_dim,\n",
    "    \"Proton_min\": photon_sum_proton_min,\n",
    "    \"Proton_max\": photon_sum_proton_max,\n",
    "    \"Experiment_dir_name\": EXPERIMENT_DIR_NAME,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "    },\n",
    "    tags=[f\"proton_min_{photon_sum_proton_min}\",\n",
    "          f\"proton_max_{photon_sum_proton_max}\",\n",
    "          f\"neutron_min_{photon_sum_neutron_min}\",\n",
    "          f\"neutron_max_{photon_sum_neutron_max}\",\n",
    "          f\"gan_strength_{STRENGTH}\", \"sdi-gan\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rMxBrHhsTDXO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "org_p = np.exp(x_test_p)-1\n",
    "ch_org_p = np.array(org_p).reshape(-1,56,30)\n",
    "ch_org_p = pd.DataFrame(sum_channels_parallel(ch_org_p)).values\n",
    "del org_p\n",
    "\n",
    "org_n = np.exp(x_test_n)-1\n",
    "ch_org_n = np.array(org_n).reshape(-1,44,44)\n",
    "ch_org_n = pd.DataFrame(sum_channels_parallel(ch_org_n)).values\n",
    "del org_n\n",
    "\n",
    "\n",
    "def calculate_ws_ch(n_calc):\n",
    "    ws_p = [0,0,0,0,0]\n",
    "    ws_n = [0,0,0,0,0]\n",
    "    for j in range(n_calc):\n",
    "        z = np.random.normal(0,1,(x_test_p.shape[0], latent_dim))\n",
    "        z_c = y_test\n",
    "        results_p, results_n = generator.predict([z, z_c])\n",
    "\n",
    "        results_p = np.exp(results_p)-1\n",
    "        results_n = np.exp(results_n)-1\n",
    "        try:\n",
    "            ch_gen_p = np.array(results_p).reshape(-1,56,30)\n",
    "            ch_gen_p = pd.DataFrame(sum_channels_parallel(ch_gen_p)).values\n",
    "            ch_gen_n = np.array(results_n).reshape(-1,44,44)\n",
    "            ch_gen_n = pd.DataFrame(sum_channels_parallel(ch_gen_n)).values\n",
    "            for i in range(5):\n",
    "                ws_p[i] = ws_p[i] + wasserstein_distance(ch_org_p[:,i], ch_gen_p[:,i])\n",
    "                ws_n[i] = ws_n[i] + wasserstein_distance(ch_org_n[:,i], ch_gen_n[:,i])\n",
    "            ws_p = np.array(ws_p)\n",
    "            ws_n = np.array(ws_n)\n",
    "            ws = ws_p+ws_n\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "    ws = ws/n_calc\n",
    "    ws_mean = ws.sum()/5\n",
    "    print(\"ws mean\",f'{ws_mean:.2f}', end=\" \")\n",
    "    for n, score in enumerate(ws):\n",
    "        print(\"ch\"+str(n+1),f'{score:.2f}',end=\" \")\n",
    "    return ws_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "GmlMSiqCku5_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch, step):\n",
    "    # dataset proton, dataset 2 proton, dataset neutron, dataset 2 neutron, dataset_cond, dataset_std_proton, dataset_std_neutron, fake_cond\n",
    "    images_p, images_p_2, images_n, images_n_2, cond, std_proton, std_neutron, noise_cond = batch\n",
    "    step=step\n",
    "    BATCH_SIZE = tf.shape(images_p)[0]\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    noise_2 = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # for the same conditional data generate two images from different noises\n",
    "        generated_images_p, generated_images_n = generator([noise, noise_cond], training=True)\n",
    "        generated_images_p_2, generated_images_n_2 = generator([noise_2, noise_cond], training=True)\n",
    "        \n",
    "        # produce if real image is real or fake\n",
    "        real_output, real_latent = discriminator([images_p, images_n, cond], training=True)\n",
    "        # real_output_2,real_latent_2  = discriminator([images_2,cond], training=True)\n",
    "        \n",
    "        # produce if generated images from two different latent codes are real or fake\n",
    "        fake_output, fake_latent = discriminator([generated_images_p, generated_images_n, noise_cond], training=True)\n",
    "        fake_output_2, fake_latent_2 = discriminator([generated_images_p_2, generated_images_n_2, noise_cond], training=True)\n",
    "\n",
    "        gen_loss, div_loss = generator_loss(step, fake_output,\n",
    "                                            fake_latent, fake_latent_2,\n",
    "                                            noise, noise_2,\n",
    "                                            std_proton, std_neutron)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    #         generated_images = generator([noise,noise_cond], training=True)\n",
    "\n",
    "    #         real_output = discriminator([images,cond], training=True)\n",
    "    #         fake_output = discriminator([generated_images, noise_cond], training=True)\n",
    "\n",
    "    #         gen_loss = generator_loss(step, fake_output)\n",
    "    #         real_loss, fake_loss = discriminator_loss(real_output, fake_output)\n",
    "    #         disc_loss = real_loss + fake_loss\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, div_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "D-wATS0PkvJo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If model achieves WS metric less or equal to this number, its weights will be saved\n",
    "WS_MEAN_SAVE_THRESHOLD = 20\n",
    "\n",
    "\n",
    "if SAVE_EXPERIMENT_DATA:\n",
    "    filepath_mod = f\"../../{EXPERIMENT_DIR_NAME}/models/\"\n",
    "    create_dir(filepath_mod)\n",
    "\n",
    "history = []\n",
    "def train(dataset, epochs):\n",
    "    experiment_start = time.time()\n",
    "    tf_step = tf.Variable(0, dtype=float)\n",
    "    step=0\n",
    "\n",
    "    # generate first image\n",
    "    generate_and_save_images(generator,\n",
    "                             epochs,\n",
    "                             [seed, seed_cond])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        gen_loss_epoch = []\n",
    "        div_loss_epoch = []\n",
    "        disc_loss_epoch = []\n",
    "        for batch in dataset:\n",
    "            gen_loss, disc_loss, div_loss = train_step(batch,tf_step)\n",
    "\n",
    "            history.append([gen_loss,disc_loss,\n",
    "                100*d_acc_r.result().numpy(),\n",
    "                100*d_acc_f.result().numpy(),\n",
    "                100*g_acc.result().numpy(),\n",
    "                ])\n",
    "            tf_step.assign_add(1)\n",
    "            step = step+1\n",
    "\n",
    "            gen_loss_epoch.append(gen_loss)\n",
    "            disc_loss_epoch.append(disc_loss)\n",
    "            div_loss_epoch.append(div_loss)\n",
    "            if step % 100 == 0:\n",
    "                print(\"%d [D real acc: %.2f%%] [D fake acc: %.2f%%] [G acc: %.2f%%] \"% (\n",
    "                    step,\n",
    "                    100*d_acc_r.result().numpy(),\n",
    "                    100*d_acc_f.result().numpy(),\n",
    "                    100*g_acc.result().numpy()))\n",
    "\n",
    "        plot = generate_and_save_images(generator,\n",
    "                                 epoch,\n",
    "                                 [seed, seed_cond])\n",
    "\n",
    "        ws_mean = calculate_ws_ch(min(epoch//5+1,5))\n",
    "\n",
    "        if SAVE_EXPERIMENT_DATA:\n",
    "            if ws_mean <= WS_MEAN_SAVE_THRESHOLD:\n",
    "                # Save the model every epoch\n",
    "                generator.compile()\n",
    "                # discriminator.compile()\n",
    "                generator.save((os.path.join(filepath_mod, \"gen_\"+NAME + \"_\"+ str(epoch) +\".h5\")))\n",
    "                # discriminator.save((os.path.join(filepath_mod, \"disc_\"+NAME + \"_\"+ str(epoch) +\".h5\")))\n",
    "                # np.savez(os.path.join(filepath_mod, \"history_\"+NAME+\".npz\"),np.array(history))\n",
    "\n",
    "        wandb.log({\n",
    "            'ws_mean': ws_mean,\n",
    "            'gen_loss': np.mean(gen_loss_epoch),\n",
    "            'div_loss': np.mean(div_loss_epoch),\n",
    "            'disc_loss': np.mean(disc_loss_epoch),\n",
    "            'epoch': epoch,\n",
    "            'plot': wandb.Image(plot),\n",
    "            'experiment_time': time.time()-experiment_start\n",
    "        })\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "CxeGwn7ek8Q-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_EXPERIMENT_DATA:\n",
    "    filepath_img = f\"../../{EXPERIMENT_DIR_NAME}/images/\"\n",
    "    create_dir(filepath_img)\n",
    "\n",
    "    \n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    START_INDEX = 6\n",
    "    SUPTITLE_TXT = f\"\\nModel: GAN proton data\" \\\n",
    "               f\"\\nPhotonsum interval: [{photon_sum_proton_min}, {photon_sum_proton_max}]\" \\\n",
    "               f\"\\nEPOCH: {epoch}\"\n",
    "\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions_p, predictions_n = model(test_input, training=False)\n",
    "    \"\"\"\n",
    "    predictions has shape (n_samples, 56, 44, 2). First channel has proton data, second has neutrons\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "    plt.title(f\"EPOCH {epoch}\")\n",
    "\n",
    "    subfigs = fig.subfigures(1, 4)\n",
    "\n",
    "    for particle_num, subfig in enumerate(subfigs.flat):  # iterate over 4 particles\n",
    "        subfig.suptitle(f'Particle {particle_num} response')\n",
    "        axs = subfig.subplots(2, 2)\n",
    "        \n",
    "        for i, ax in enumerate(axs.flat):  # iterate over 4 images of single particle\n",
    "            m_2 = i % 2  # 0 if proton, 1 if neutron\n",
    "            if i < 2:\n",
    "                # Real response\n",
    "                if m_2 == 0:  # proton\n",
    "                    x = x_test_p[START_INDEX+particle_num].reshape(56, 30)\n",
    "                else:  # neutron\n",
    "                    x = x_test_n[START_INDEX+particle_num].reshape(44, 44)\n",
    "                axs[i//2, m_2].set_title(\"neutron\" if m_2 else \"proton\")\n",
    "            else:\n",
    "                # Generated response\n",
    "                if m_2 == 0:  # proton\n",
    "                    x = predictions_p[START_INDEX+particle_num].numpy().reshape(56, 30)\n",
    "                else:  # neutron\n",
    "                    x = predictions_n[START_INDEX+particle_num].numpy().reshape(44, 44)\n",
    "            axs[i//2, m_2].set_axis_off()\n",
    "            im = axs[i//2, m_2].imshow(x, interpolation='none', cmap='gnuplot')\n",
    "            fig.colorbar(im, ax=axs[i//2, m_2])\n",
    "\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        plt.savefig(os.path.join(filepath_img, 'image_at_epoch_{:04d}.png'.format(epoch)))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmR61h2W0vxC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D real acc: 51.01%] [D fake acc: 56.15%] [G acc: 43.85%] \n",
      "200 [D real acc: 56.60%] [D fake acc: 55.91%] [G acc: 44.09%] \n",
      "300 [D real acc: 59.99%] [D fake acc: 55.51%] [G acc: 44.49%] \n",
      "305/305 [==============================] - 15s 46ms/step\n",
      "ws mean 6757615662069491.00 ch1 28962905271.56 ch2 10874979.64 ch3 22500987538845308.00 ch4 159579117619304.03 ch5 11127482680102590.00 Time for epoch 1 is 449.2795214653015 sec\n",
      "400 [D real acc: 60.86%] [D fake acc: 55.07%] [G acc: 44.93%] \n",
      "500 [D real acc: 61.04%] [D fake acc: 55.08%] [G acc: 44.92%] \n",
      "600 [D real acc: 61.36%] [D fake acc: 55.51%] [G acc: 44.49%] \n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 3604.82 ch1 346.39 ch2 528.19 ch3 50.29 ch4 5083.40 ch5 12015.81 Time for epoch 2 is 410.713271856308 sec\n",
      "700 [D real acc: 61.54%] [D fake acc: 55.92%] [G acc: 44.08%] \n",
      "800 [D real acc: 61.73%] [D fake acc: 55.94%] [G acc: 44.06%] \n",
      "900 [D real acc: 61.71%] [D fake acc: 56.12%] [G acc: 43.88%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "ws mean 55.70 ch1 35.79 ch2 51.91 ch3 30.55 ch4 33.85 ch5 126.39 Time for epoch 3 is 424.6681079864502 sec\n",
      "1000 [D real acc: 61.52%] [D fake acc: 56.35%] [G acc: 43.65%] \n",
      "1100 [D real acc: 61.46%] [D fake acc: 56.54%] [G acc: 43.46%] \n",
      "1200 [D real acc: 61.37%] [D fake acc: 56.79%] [G acc: 43.21%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "ws mean 99.56 ch1 89.24 ch2 42.41 ch3 26.42 ch4 99.44 ch5 240.26 Time for epoch 4 is 427.0182902812958 sec\n",
      "1300 [D real acc: 61.37%] [D fake acc: 57.02%] [G acc: 42.98%] \n",
      "1400 [D real acc: 61.38%] [D fake acc: 57.28%] [G acc: 42.72%] \n",
      "1500 [D real acc: 61.47%] [D fake acc: 57.62%] [G acc: 42.38%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "ws mean 38.31 ch1 23.51 ch2 11.90 ch3 30.42 ch4 44.83 ch5 80.91 Time for epoch 5 is 425.7621338367462 sec\n",
      "1600 [D real acc: 61.60%] [D fake acc: 57.90%] [G acc: 42.10%] \n",
      "1700 [D real acc: 61.78%] [D fake acc: 58.20%] [G acc: 41.80%] \n",
      "1800 [D real acc: 61.90%] [D fake acc: 58.49%] [G acc: 41.51%] \n",
      "305/305 [==============================] - 13s 42ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "ws mean 85.99 ch1 47.26 ch2 37.32 ch3 78.01 ch4 58.17 ch5 209.17 Time for epoch 6 is 426.88560795783997 sec\n",
      "1900 [D real acc: 62.05%] [D fake acc: 58.78%] [G acc: 41.22%] \n",
      "2000 [D real acc: 62.23%] [D fake acc: 59.05%] [G acc: 40.95%] \n",
      "2100 [D real acc: 62.50%] [D fake acc: 59.31%] [G acc: 40.69%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "ws mean 93.91 ch1 78.00 ch2 53.56 ch3 32.76 ch4 79.47 ch5 225.79 Time for epoch 7 is 439.79847979545593 sec\n",
      "2200 [D real acc: 62.76%] [D fake acc: 59.64%] [G acc: 40.36%] \n",
      "2300 [D real acc: 62.94%] [D fake acc: 59.92%] [G acc: 40.08%] \n",
      "2400 [D real acc: 63.21%] [D fake acc: 60.18%] [G acc: 39.82%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 80.93 ch1 85.13 ch2 44.59 ch3 16.25 ch4 66.12 ch5 192.58 Time for epoch 8 is 440.8479700088501 sec\n",
      "2500 [D real acc: 63.45%] [D fake acc: 60.45%] [G acc: 39.55%] \n",
      "2600 [D real acc: 63.60%] [D fake acc: 60.67%] [G acc: 39.33%] \n",
      "2700 [D real acc: 63.75%] [D fake acc: 60.89%] [G acc: 39.11%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 41.72 ch1 28.22 ch2 25.12 ch3 19.82 ch4 42.90 ch5 92.53 Time for epoch 9 is 441.07111501693726 sec\n",
      "2800 [D real acc: 63.91%] [D fake acc: 61.13%] [G acc: 38.87%] \n",
      "2900 [D real acc: 64.08%] [D fake acc: 61.39%] [G acc: 38.61%] \n",
      "3000 [D real acc: 64.32%] [D fake acc: 61.62%] [G acc: 38.38%] \n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 13s 41ms/step\n",
      "ws mean 1345.12 ch1 111.66 ch2 2873.64 ch3 39.34 ch4 25.77 ch5 3675.21 Time for epoch 10 is 425.7271611690521 sec\n",
      "3100 [D real acc: 64.54%] [D fake acc: 61.91%] [G acc: 38.09%] \n",
      "3200 [D real acc: 64.78%] [D fake acc: 62.17%] [G acc: 37.83%] \n",
      "3300 [D real acc: 65.03%] [D fake acc: 62.44%] [G acc: 37.56%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "ws mean 188.29 ch1 118.18 ch2 57.31 ch3 18.83 ch4 270.53 ch5 476.63 Time for epoch 11 is 437.3432092666626 sec\n",
      "3400 [D real acc: 65.30%] [D fake acc: 62.70%] [G acc: 37.30%] \n",
      "3500 [D real acc: 65.55%] [D fake acc: 62.92%] [G acc: 37.08%] \n",
      "3600 [D real acc: 65.77%] [D fake acc: 63.13%] [G acc: 36.87%] \n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 14s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 84.77 ch1 71.75 ch2 61.83 ch3 19.50 ch4 49.85 ch5 220.91 Time for epoch 12 is 455.30934023857117 sec\n",
      "3700 [D real acc: 65.96%] [D fake acc: 63.37%] [G acc: 36.63%] \n",
      "3800 [D real acc: 66.13%] [D fake acc: 63.52%] [G acc: 36.48%] \n",
      "3900 [D real acc: 66.34%] [D fake acc: 63.59%] [G acc: 36.41%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 157.58 ch1 209.18 ch2 100.60 ch3 40.79 ch4 44.54 ch5 392.78 Time for epoch 13 is 453.312148809433 sec\n",
      "4000 [D real acc: 66.47%] [D fake acc: 63.77%] [G acc: 36.23%] \n",
      "4100 [D real acc: 66.56%] [D fake acc: 63.90%] [G acc: 36.10%] \n",
      "4200 [D real acc: 66.67%] [D fake acc: 64.01%] [G acc: 35.99%] \n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "ws mean 32.87 ch1 14.77 ch2 9.82 ch3 15.11 ch4 41.95 ch5 82.73 Time for epoch 14 is 430.50786209106445 sec\n",
      "4300 [D real acc: 66.76%] [D fake acc: 64.12%] [G acc: 35.88%] \n",
      "4400 [D real acc: 66.84%] [D fake acc: 64.24%] [G acc: 35.76%] \n",
      "4500 [D real acc: 66.94%] [D fake acc: 64.37%] [G acc: 35.63%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 834.55 ch1 25.97 ch2 364.03 ch3 141.74 ch4 1528.39 ch5 2112.62 Time for epoch 15 is 451.8715500831604 sec\n",
      "4600 [D real acc: 67.06%] [D fake acc: 64.53%] [G acc: 35.47%] \n",
      "4700 [D real acc: 67.21%] [D fake acc: 64.68%] [G acc: 35.32%] \n",
      "4800 [D real acc: 67.40%] [D fake acc: 64.82%] [G acc: 35.18%] \n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 84.10 ch1 53.65 ch2 29.63 ch3 12.26 ch4 112.17 ch5 212.80 Time for epoch 16 is 468.9659740924835 sec\n",
      "4900 [D real acc: 67.54%] [D fake acc: 65.03%] [G acc: 34.97%] \n",
      "5000 [D real acc: 67.65%] [D fake acc: 65.18%] [G acc: 34.82%] \n",
      "5100 [D real acc: 67.81%] [D fake acc: 65.33%] [G acc: 34.67%] \n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 14s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "ws mean 62.65 ch1 21.28 ch2 39.97 ch3 20.41 ch4 62.08 ch5 169.52 Time for epoch 17 is 469.6155104637146 sec\n",
      "5200 [D real acc: 67.96%] [D fake acc: 65.48%] [G acc: 34.52%] \n",
      "5300 [D real acc: 68.07%] [D fake acc: 65.59%] [G acc: 34.41%] \n",
      "5400 [D real acc: 68.19%] [D fake acc: 65.74%] [G acc: 34.26%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 48.14 ch1 8.45 ch2 8.42 ch3 20.74 ch4 86.66 ch5 116.41 Time for epoch 18 is 468.33906269073486 sec\n",
      "5500 [D real acc: 68.32%] [D fake acc: 65.86%] [G acc: 34.14%] \n",
      "5600 [D real acc: 68.47%] [D fake acc: 66.02%] [G acc: 33.98%] \n",
      "5700 [D real acc: 68.59%] [D fake acc: 66.16%] [G acc: 33.84%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "ws mean 73.33 ch1 42.10 ch2 69.44 ch3 26.27 ch4 46.44 ch5 182.42 Time for epoch 19 is 476.41728615760803 sec\n",
      "5800 [D real acc: 68.69%] [D fake acc: 66.26%] [G acc: 33.74%] \n",
      "5900 [D real acc: 68.80%] [D fake acc: 66.36%] [G acc: 33.64%] \n",
      "6000 [D real acc: 68.94%] [D fake acc: 66.51%] [G acc: 33.49%] \n",
      "6100 [D real acc: 69.02%] [D fake acc: 66.64%] [G acc: 33.36%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 96.30 ch1 98.23 ch2 67.76 ch3 18.85 ch4 49.45 ch5 247.22 Time for epoch 20 is 468.1609354019165 sec\n",
      "6200 [D real acc: 69.11%] [D fake acc: 66.75%] [G acc: 33.25%] \n",
      "6300 [D real acc: 69.21%] [D fake acc: 66.85%] [G acc: 33.15%] \n",
      "6400 [D real acc: 69.31%] [D fake acc: 66.96%] [G acc: 33.04%] \n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "ws mean 75.60 ch1 29.01 ch2 56.62 ch3 36.56 ch4 55.36 ch5 200.47 Time for epoch 21 is 462.73500299453735 sec\n",
      "6500 [D real acc: 69.41%] [D fake acc: 67.07%] [G acc: 32.93%] \n",
      "6600 [D real acc: 69.52%] [D fake acc: 67.16%] [G acc: 32.84%] \n",
      "6700 [D real acc: 69.60%] [D fake acc: 67.28%] [G acc: 32.72%] \n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 42.01 ch1 26.16 ch2 33.14 ch3 12.85 ch4 32.77 ch5 105.15 Time for epoch 22 is 472.9505455493927 sec\n",
      "6800 [D real acc: 69.68%] [D fake acc: 67.39%] [G acc: 32.61%] \n",
      "6900 [D real acc: 69.79%] [D fake acc: 67.50%] [G acc: 32.50%] \n",
      "7000 [D real acc: 69.90%] [D fake acc: 67.60%] [G acc: 32.40%] \n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 13s 41ms/step\n",
      "ws mean 98.19 ch1 105.49 ch2 56.35 ch3 43.48 ch4 37.43 ch5 248.21 Time for epoch 23 is 462.39432668685913 sec\n",
      "7100 [D real acc: 70.03%] [D fake acc: 67.74%] [G acc: 32.26%] \n",
      "7200 [D real acc: 70.11%] [D fake acc: 67.82%] [G acc: 32.18%] \n",
      "7300 [D real acc: 70.21%] [D fake acc: 67.94%] [G acc: 32.06%] \n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "ws mean 73.72 ch1 8.68 ch2 51.25 ch3 15.15 ch4 107.53 ch5 185.98 Time for epoch 24 is 451.40515327453613 sec\n",
      "7400 [D real acc: 70.35%] [D fake acc: 68.08%] [G acc: 31.92%] \n",
      "7500 [D real acc: 70.47%] [D fake acc: 68.22%] [G acc: 31.78%] \n",
      "7600 [D real acc: 70.61%] [D fake acc: 68.35%] [G acc: 31.65%] \n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "ws mean 38.90 ch1 21.64 ch2 13.18 ch3 19.80 ch4 42.24 ch5 97.63 Time for epoch 25 is 458.6247375011444 sec\n",
      "7700 [D real acc: 70.70%] [D fake acc: 68.45%] [G acc: 31.55%] \n",
      "7800 [D real acc: 70.78%] [D fake acc: 68.54%] [G acc: 31.46%] \n",
      "7900 [D real acc: 70.86%] [D fake acc: 68.61%] [G acc: 31.39%] \n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "ws mean 39.29 ch1 27.01 ch2 32.88 ch3 13.17 ch4 26.76 ch5 96.64 Time for epoch 26 is 465.0613453388214 sec\n",
      "8000 [D real acc: 70.95%] [D fake acc: 68.70%] [G acc: 31.30%] \n",
      "8100 [D real acc: 71.02%] [D fake acc: 68.80%] [G acc: 31.20%] \n",
      "8200 [D real acc: 71.11%] [D fake acc: 68.90%] [G acc: 31.10%] \n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "ws mean 137.83 ch1 75.97 ch2 55.58 ch3 15.15 ch4 197.36 ch5 345.09 Time for epoch 27 is 453.61319756507874 sec\n",
      "8300 [D real acc: 71.22%] [D fake acc: 69.02%] [G acc: 30.98%] \n",
      "8400 [D real acc: 71.30%] [D fake acc: 69.12%] [G acc: 30.88%] \n",
      "8500 [D real acc: 71.42%] [D fake acc: 69.24%] [G acc: 30.76%] \n",
      "305/305 [==============================] - 12s 41ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "305/305 [==============================] - 12s 39ms/step\n",
      "ws mean 71.21 ch1 72.65 ch2 38.38 ch3 34.30 ch4 36.53 ch5 174.17 Time for epoch 28 is 478.10283732414246 sec\n",
      "8600 [D real acc: 71.54%] [D fake acc: 69.37%] [G acc: 30.63%] \n",
      "8700 [D real acc: 71.61%] [D fake acc: 69.48%] [G acc: 30.52%] \n",
      "8800 [D real acc: 71.71%] [D fake acc: 69.58%] [G acc: 30.42%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 90.93 ch1 78.33 ch2 60.92 ch3 29.63 ch4 59.21 ch5 226.55 Time for epoch 29 is 470.56635785102844 sec\n",
      "8900 [D real acc: 71.79%] [D fake acc: 69.68%] [G acc: 30.32%] \n",
      "9000 [D real acc: 71.91%] [D fake acc: 69.79%] [G acc: 30.21%] \n",
      "9100 [D real acc: 72.03%] [D fake acc: 69.86%] [G acc: 30.14%] \n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "305/305 [==============================] - 12s 40ms/step\n",
      "ws mean 40.99 ch1 26.00 ch2 23.00 ch3 11.55 ch4 40.73 ch5 103.67 Time for epoch 30 is 476.4583168029785 sec\n",
      "9200 [D real acc: 72.10%] [D fake acc: 69.92%] [G acc: 30.08%] \n",
      "9300 [D real acc: 72.13%] [D fake acc: 69.97%] [G acc: 30.03%] \n",
      "9400 [D real acc: 72.19%] [D fake acc: 70.03%] [G acc: 29.97%] \n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 43ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "305/305 [==============================] - 14s 44ms/step\n",
      "305/305 [==============================] - 13s 44ms/step\n",
      "ws mean 58.92 ch1 35.61 ch2 52.57 ch3 24.97 ch4 39.78 ch5 141.68 Time for epoch 31 is 472.22329807281494 sec\n",
      "9500 [D real acc: 72.26%] [D fake acc: 70.11%] [G acc: 29.89%] \n",
      "9600 [D real acc: 72.31%] [D fake acc: 70.19%] [G acc: 29.81%] \n"
     ]
    }
   ],
   "source": [
    "history = train(dataset_with_cond, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('notebook', font_scale = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# total_loss, reconstruction_loss, kl_loss\n",
    "history_losses = np.array([[float(loss) for loss in losses] for losses in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator_loss = history_losses[:,0]\n",
    "disc_loss = history_losses[:,1]\n",
    "discriminator_r_acc = history_losses[:,2]\n",
    "discriminator_f_acc = history_losses[:,3]\n",
    "generator_acc = history_losses[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a4_dims = (14, 5)\n",
    "def print_loss(loss_values, loss_str):\n",
    "    fig, ax = plt.subplots(figsize=a4_dims)\n",
    "    sns.lineplot(loss_values)\n",
    "    plt.xscale('log')\n",
    "    ax.set_title(f\"{loss_str} loss in each epoch\")\n",
    "    ax.set_xlabel(\"Epoch number\")\n",
    "    ax.set_ylabel(f\"{loss_str} loss value\")\n",
    "    # fig.save(f\"../images/{EXPERIMENT_NAME}/{loss_str}_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_loss(generator_loss, \"Generator\")\n",
    "print_loss(disc_loss, \"Discriminator\")\n",
    "print_loss(discriminator_r_acc, \"Discriminator r accuracy\")\n",
    "print_loss(discriminator_f_acc, \"Discriminator f accuracy\")\n",
    "print_loss(generator_acc, \"Generator accuracy\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "5.GAN.ipynb",
   "provenance": [
    {
     "file_id": "1NUVAcZshSKsDKHaeeNNI8ohcUJvqUKqO",
     "timestamp": 1623369938483
    },
    {
     "file_id": "1bthIWyh_69c0sa09VMrRj9mXKOPNu7sP",
     "timestamp": 1623365007022
    },
    {
     "file_id": "1L77q4mrDL6LbE2HURRWfH8vtKm8pUBzK",
     "timestamp": 1620124217177
    },
    {
     "file_id": "1640z4JoZlvsSRGao2KCW322_UwjcQaSp",
     "timestamp": 1620084259696
    },
    {
     "file_id": "1zwFy4NM6SiPYsXCIgVUOEhBXbJ6Rj1tV",
     "timestamp": 1620082260980
    },
    {
     "file_id": "1IbQMqLHIYF-vg6B4Abbobl4pkROWBwes",
     "timestamp": 1620082045291
    },
    {
     "file_id": "1fnbp6zHVBrdi1nCGkrokht8i-pAUfIyf",
     "timestamp": 1620056039573
    },
    {
     "file_id": "1WXvm9ORGBKSJCVApOV4gJIC0NC7OQFzX",
     "timestamp": 1619997366147
    },
    {
     "file_id": "1hYuZL48eIXUGFk2mPXxeUh610OkoOfmV",
     "timestamp": 1619991567406
    },
    {
     "file_id": "1SYhFD0Djg7etn9UB47zsjc948E5VxTFG",
     "timestamp": 1616868089323
    },
    {
     "file_id": "17CrRwBBNZfhpN3y5pk_dTt7PelHqfFSI",
     "timestamp": 1616535329540
    },
    {
     "file_id": "1eTiiJCqFmQvy8KLbd0FSI1WfweHUCz-w",
     "timestamp": 1616531331246
    },
    {
     "file_id": "1Tndd1egGbtLRTO0Hnz6QnI9XiV5nEZKq",
     "timestamp": 1616528772133
    },
    {
     "file_id": "1kTX59Ymn4DGGgjVVQTt7ocE_oy5Sanpx",
     "timestamp": 1616281970086
    },
    {
     "file_id": "1kPO5iwHQPVcMuTc_Tn5JcSIRJ4hNrWjH",
     "timestamp": 1616181812747
    },
    {
     "file_id": "1nkcov8gu5MJRqWxL1awc78jThGTXsTyZ",
     "timestamp": 1616013741366
    },
    {
     "file_id": "13iHBeXIFuSw6-EvChxlqiveFVFurjENx",
     "timestamp": 1615923191272
    },
    {
     "file_id": "1q1UXZxrBrZdwYFvuOtkaRAnqykzUqK1-",
     "timestamp": 1615843147388
    },
    {
     "file_id": "1NUjBm8LUmJ5_Cu3ithC2JLJ4aFAWFEm2",
     "timestamp": 1615585196418
    },
    {
     "file_id": "1Y8oTevpUwSdSV3oCdx_e37mkhaxPzrGf",
     "timestamp": 1615506244659
    }
   ]
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
