{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for running in collab, sagemaker etc.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def _append_sys_path_of_utils():\n",
    "    \"\"\"\n",
    "    Appends an utils directory to a system path so it is visible for the script.\n",
    "    \"\"\"\n",
    "    UTILS_RELATIVE_PATH = \"../../utils/\"\n",
    "\n",
    "    absolute_path_of_utils = os.path.abspath(UTILS_RELATIVE_PATH)\n",
    "    if absolute_path_of_utils:\n",
    "        if absolute_path_of_utils in sys.path:\n",
    "            print(f\"Absolute path already in sys.path: {absolute_path_of_utils}\")\n",
    "        else:\n",
    "            print(f\"Appended absolute path: {absolute_path_of_utils}\")\n",
    "            sys.path.insert(0, absolute_path_of_utils)\n",
    "    else:\n",
    "        print(\"Absolute path of utils module not found.\")\n",
    "\n",
    "\n",
    "_append_sys_path_of_utils()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wandb login"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, decomposition, manifold, preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import time\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel as sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U tf2onnx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pwd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "model = keras.models.load_model(\"../../experiments/sdi-gan_15_2133_08_05_2023_21_20/models/best_model/gen_sdi_gan_62.h5\")\n",
    "tf.saved_model.save(model, \"gen_sdi_gan_62\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python3 -m tf2onnx.convert --saved-model gen_sdi_gan_62 --output \"gen_sdi_gan_62_epoch.onnx\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVE_EXPERIMENT_DATA = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and process data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../../data/data_proton_20_2312_neutron_20_3360_padded.pkl')\n",
    "print('Loaded: ',  data.shape, \"max:\", data.max())\n",
    "\n",
    "data_cond = pd.read_pickle('../../data/data_cond_stddev_photonsum_20_2312_20_3360_padded.pkl')\n",
    "print('Loaded cond: ',  data_cond.shape, \"max:\", data_cond.values.max(), \"min:\", data_cond.values.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate min max proton, neutron sum\n",
    "photon_sum_proton_min, photon_sum_proton_max = data_cond.proton_photon_sum.min(), data_cond.proton_photon_sum.max()\n",
    "photon_sum_neutron_min, photon_sum_neutron_max = data_cond.neutron_photon_sum.min(), data_cond.neutron_photon_sum.max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_cond.drop(columns=['proton_photon_sum', 'neutron_photon_sum', 'group_number_proton', 'group_number_neutron'], inplace=True)\n",
    "data_cond.columns, len(data_cond.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_cond.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "STRENGTH = 0.1\n",
    "\n",
    "DATE_STR = datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "\n",
    "NAME = \"sdi-gan-padded\"\n",
    "\n",
    "wandb_run_name = f\"{int(photon_sum_proton_min)}_{int(photon_sum_proton_max)}_{DATE_STR}\"\n",
    "\n",
    "EXPERIMENT_DIR_NAME = f\"experiments/{NAME}_{int(photon_sum_proton_min)}_{int(photon_sum_proton_max)}_{DATE_STR}\"\n",
    "\n",
    "print(\"Experiment DIR: \", EXPERIMENT_DIR_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_scales(model_name, scaler_means, scaler_scales):\n",
    "    out_fnm = f\"{model_name}_scales.txt\"\n",
    "    res = \"#means\"\n",
    "    for mean_ in scaler_means:\n",
    "        res += \"\\n\" + str(mean_)\n",
    "    res += \"\\n\\n#scales\"\n",
    "    for scale_ in scaler_scales:\n",
    "        res += \"\\n\" + str(scale_)\n",
    "\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        filepath = f\"../../{EXPERIMENT_DIR_NAME}/scales/\"\n",
    "        create_dir(filepath)\n",
    "        with open(filepath+out_fnm, mode=\"w\") as f:\n",
    "            f.write(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_cond[\"cond\"] = data_cond[\"Energy\"].astype(str) +\"|\"+ data_cond[\"Vx\"].astype(str) +\"|\"+  data_cond[\"Vy\"].astype(str) +\"|\"+ data_cond[\"Vz\"].astype(str) +\"|\"+  data_cond[\"Px\"].astype(str) +\"|\"+  data_cond[\"Py\"].astype(str) +\"|\"+ data_cond[\"Pz\"].astype(str) +\"|\"+  data_cond[\"mass\"].astype(str) +\"|\"+  data_cond[\"charge\"].astype(str)\n",
    "data_cond.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_cond_id = data_cond[[\"cond\"]].reset_index()\n",
    "data_cond_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select a random index of the same conditional data\n",
    "# shuffle the data and merge it according to the conditional data. Pick the first index of the grouped conditional data\n",
    "# if some unique conditional data has only one index of sample, then pair it with the same index \n",
    "ids = data_cond_id.merge(data_cond_id.sample(frac=1), on=[\"cond\"], how=\"inner\").groupby(\"index_x\").first()\n",
    "ids = ids[\"index_y\"]\n",
    "ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Idea\n",
    "Eliminate samples that are unique in terms of conditional data leaving only these responses that have few samples grouped by condtional data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will have two conditional datasets and two response datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_cond"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = np.log(data+1)\n",
    "data = np.float32(data)\n",
    "print(\"data max\", data.max(), \"min\", data.min())\n",
    "\n",
    "data_2 = data[ids]\n",
    "\n",
    "data_cond = data_cond.drop(columns=\"cond\")\n",
    "\n",
    "scaler_proton = MinMaxScaler()\n",
    "std_proton = data_cond[\"std_proton\"].values.reshape(-1,1)\n",
    "std_proton = np.float32(std_proton)\n",
    "std_proton = scaler_proton.fit_transform(std_proton)\n",
    "print(\"std max\", std_proton.max(), \"min\", std_proton.min())\n",
    "\n",
    "scaler_neutron = MinMaxScaler()\n",
    "std_neutron = data_cond[\"std_neutron\"].values.reshape(-1,1)\n",
    "std_neutron = np.float32(std_neutron)\n",
    "std_neutron = scaler_neutron.fit_transform(std_neutron)\n",
    "print(\"std max\", std_neutron.max(), \"min\", std_neutron.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_cond = np.float32(data_cond.drop(columns=[\"std_proton\", \"std_neutron\"]))\n",
    "data_cond = scaler.fit_transform(data_cond)\n",
    "print(\"cond max\", data_cond.max(), \"min\", data_cond.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, x_train_2, x_test_2, y_train, y_test, std_proton_train, std_proton_test, std_neutron_train, std_neutron_test = train_test_split(data, data_2, data_cond, std_proton, std_neutron, test_size=0.2, shuffle=False)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, std_proton_train.shape, std_proton_test.shape, std_neutron_train.shape, std_neutron_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save scales\n",
    "if SAVE_EXPERIMENT_DATA:\n",
    "    save_scales(\"Proton\", scaler.mean_, scaler.scale_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# Training dataset\n",
    "\n",
    "# datasets that in each index contain two samples from the same conditional data\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size=BATCH_SIZE)\n",
    "dataset_2 = tf.data.Dataset.from_tensor_slices(x_train_2).batch(batch_size=BATCH_SIZE)\n",
    "# conditional data\n",
    "dataset_cond = tf.data.Dataset.from_tensor_slices(y_train).batch(batch_size=BATCH_SIZE)\n",
    "# standard deviation for each conditional data samples\n",
    "dataset_std_proton = tf.data.Dataset.from_tensor_slices(std_proton_train).batch(batch_size=BATCH_SIZE)\n",
    "dataset_std_neutron = tf.data.Dataset.from_tensor_slices(std_neutron_train).batch(batch_size=BATCH_SIZE)\n",
    "# shuffled conditional data\n",
    "fake_cond =  tf.data.Dataset.from_tensor_slices(y_train).shuffle(12800).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "# zipped data\n",
    "dataset_with_cond = tf.data.Dataset.zip((dataset, dataset_2, dataset_cond, dataset_std_proton, dataset_std_neutron, fake_cond)).shuffle(12800)\n",
    "\n",
    "# Validation dataset\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_2 = tf.data.Dataset.from_tensor_slices(x_test_2).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_cond = tf.data.Dataset.from_tensor_slices(y_test).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_std_proton = tf.data.Dataset.from_tensor_slices(std_proton_test).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_std_neutron = tf.data.Dataset.from_tensor_slices(std_neutron_test).batch(batch_size=BATCH_SIZE)\n",
    "val_fake_cond =  tf.data.Dataset.from_tensor_slices(y_test).shuffle(12800).batch(batch_size=BATCH_SIZE)\n",
    "val_dataset_with_cond = tf.data.Dataset.zip((val_dataset, val_dataset_2, val_dataset_cond, val_dataset_std_proton, val_dataset_std_neutron, val_fake_cond)).shuffle(12800)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow import keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OLD Architecture with latent dimension 16 failed, achieved mode collapse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OLD Architecture\n",
    "\n",
    "latent_dim = 8\n",
    "cond_dim = 9\n",
    "\n",
    "############################ generator ############################\n",
    "\n",
    "x = Input(shape=(latent_dim,))\n",
    "cond = Input(shape=(cond_dim,))\n",
    "inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "layer_1 = Dense(128*2)(inputs)\n",
    "layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "layer_2 = Dense(128*20*12)(layer_1_a)\n",
    "layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "reshaped = Reshape((20,12,128))(layer_2_a)\n",
    "reshaped_s = UpSampling2D(size=(3,2))(reshaped)\n",
    "\n",
    "conv1 = Conv2D(256, kernel_size=(2, 2), padding='valid')(reshaped_s)\n",
    "conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "conv1_a_s = UpSampling2D(size=(1,2))(conv1_a)\n",
    "\n",
    "conv2 = Conv2D(128, kernel_size=2)(conv1_a_s)\n",
    "conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "conv3 = Conv2D(64, kernel_size=2)(conv2_a)\n",
    "conv3_bd = Dropout(0.2)(BatchNormalization()(conv3))\n",
    "conv3_a = LeakyReLU(alpha=0.1)(conv3_bd)\n",
    "\n",
    "outputs = Conv2D(2, kernel_size=(2, 1), activation='relu')(conv3_a)\n",
    "\n",
    "generator = Model([x, cond], outputs, name='generator')\n",
    "generator.summary()\n",
    "\n",
    "############################ discriminator ############################\n",
    "\n",
    "input_img = Input(shape=[56,44,2],name='input_img')\n",
    "conv1 = Conv2D(32, kernel_size=3)(input_img)\n",
    "conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "\n",
    "conv2 = Conv2D(16, kernel_size=3)(pool1)\n",
    "conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 1))(conv2_a)\n",
    "\n",
    "flat = Flatten()(pool2)\n",
    "cond = Input(shape=(cond_dim,))\n",
    "inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "layer_1 = Dense(128)(inputs2)\n",
    "layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "layer_2 = Dense(64)(layer_1_a)\n",
    "layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "outputs = Dense(1, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "discriminator = Model([input_img, cond], [outputs, layer_2_a], name='discriminator')\n",
    "discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Idea\n",
    "Increase number of latent dimensions with the old architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # architecture 4\n",
    "\n",
    "# latent_dim = 20\n",
    "# cond_dim = 9\n",
    "\n",
    "# ############################ generator ############################\n",
    "\n",
    "# x = Input(shape=(latent_dim,))\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "# inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "# layer_1 = Dense(256*2)(inputs)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(256*16*12)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# reshaped = Reshape((16,12,256))(layer_2_a)\n",
    "# reshaped_s = UpSampling2D(size=(4, 2))(reshaped)\n",
    "\n",
    "# conv1 = Conv2D(256, kernel_size=(3, 2), padding='valid')(reshaped_s)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# conv1_a_s = UpSampling2D(size=(1, 2))(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(128, kernel_size=(4, 2))(conv1_a_s)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "# conv3 = Conv2D(64, kernel_size=(1, 2))(conv2_a)\n",
    "# conv3_bd = Dropout(0.2)(BatchNormalization()(conv3))\n",
    "# conv3_a = LeakyReLU(alpha=0.1)(conv3_bd)\n",
    "\n",
    "# outputs = Conv2D(2, kernel_size=(4, 1), activation='relu')(conv3_a)\n",
    "\n",
    "# generator = Model([x, cond], outputs, name='generator')\n",
    "# generator.summary()\n",
    "\n",
    "# ############################ discriminator ############################\n",
    "\n",
    "# input_img = Input(shape=[56, 44, 2], name='input_img')\n",
    "# conv1 = Conv2D(64, kernel_size=3)(input_img)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(32, kernel_size=3)(pool1)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "# pool2 = MaxPooling2D(pool_size=(2, 1))(conv2_a)\n",
    "\n",
    "# flat = Flatten()(pool2)\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "\n",
    "# inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "# layer_1 = Dense(128)(inputs2)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(64)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# outputs = Dense(2, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "# discriminator = Model([input_img, cond], [outputs, layer_2_a], name='discriminator')\n",
    "# discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # architecture 3\n",
    "\n",
    "# latent_dim = 16\n",
    "# cond_dim = 9\n",
    "\n",
    "# ############################ generator ############################\n",
    "\n",
    "# x = Input(shape=(latent_dim,))\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "# inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "# layer_1 = Dense(128*2)(inputs)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(128*16*12)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# reshaped = Reshape((16,12,128))(layer_2_a)\n",
    "# reshaped_s = UpSampling2D(size=(4, 2))(reshaped)\n",
    "\n",
    "# conv1 = Conv2D(256, kernel_size=(3, 2), padding='valid')(reshaped_s)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# conv1_a_s = UpSampling2D(size=(1, 2))(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(128, kernel_size=(4, 2))(conv1_a_s)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "# conv3 = Conv2D(64, kernel_size=(1, 2))(conv2_a)\n",
    "# conv3_bd = Dropout(0.2)(BatchNormalization()(conv3))\n",
    "# conv3_a = LeakyReLU(alpha=0.1)(conv3_bd)\n",
    "\n",
    "# outputs = Conv2D(2, kernel_size=(4, 1), activation='relu')(conv3_a)\n",
    "\n",
    "# generator = Model([x, cond], outputs, name='generator')\n",
    "# generator.summary()\n",
    "\n",
    "# ############################ discriminator ############################\n",
    "\n",
    "# input_img = Input(shape=[56, 44, 2], name='input_img')\n",
    "# conv1 = Conv2D(64, kernel_size=3)(input_img)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(32, kernel_size=3)(pool1)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "# pool2 = MaxPooling2D(pool_size=(2, 1))(conv2_a)\n",
    "\n",
    "# flat = Flatten()(pool2)\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "\n",
    "# inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "# layer_1 = Dense(128)(inputs2)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(64)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# outputs = Dense(2, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "# discriminator = Model([input_img, cond], [outputs, layer_2_a], name='discriminator')\n",
    "# discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Second bigger architecture\n",
    "# latent_dim = 20\n",
    "# cond_dim = 9\n",
    "\n",
    "# ############################ generator ############################\n",
    "\n",
    "# x = Input(shape=(latent_dim,))\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "# inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "# layer_1 = Dense(256*2)(inputs)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(256*16*12)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# reshaped = Reshape((16,12,256))(layer_2_a)\n",
    "# reshaped_s = UpSampling2D(size=(4, 2))(reshaped)\n",
    "\n",
    "# conv1 = Conv2D(512, kernel_size=(3, 2), padding='valid')(reshaped_s)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# conv1_a_s = UpSampling2D(size=(1, 2))(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(256, kernel_size=(4, 2))(conv1_a_s)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "# conv3 = Conv2D(128, kernel_size=(1, 2))(conv2_a)\n",
    "# conv3_bd = Dropout(0.2)(BatchNormalization()(conv3))\n",
    "# conv3_a = LeakyReLU(alpha=0.1)(conv3_bd)\n",
    "\n",
    "# outputs = Conv2D(2, kernel_size=(4, 1), activation='relu')(conv3_a)\n",
    "\n",
    "# generator = Model([x, cond], outputs, name='generator')\n",
    "# generator.summary()\n",
    "\n",
    "# ############################ discriminator ############################\n",
    "\n",
    "# input_img = Input(shape=[56, 44, 2],name='input_img')\n",
    "# conv1 = Conv2D(64, kernel_size=3)(input_img)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(128, kernel_size=3)(pool1)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "# pool2 = MaxPooling2D(pool_size=(2, 1))(conv2_a)\n",
    "\n",
    "# flat = Flatten()(pool2)\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "\n",
    "# inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "# layer_1 = Dense(64)(inputs2)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(32)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# outputs = Dense(2, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "# discriminator = Model([input_img, cond], [outputs, layer_2_a], name='discriminator')\n",
    "# discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # OLD architecture\n",
    "# latent_dim = 20\n",
    "# cond_dim = 10\n",
    "\n",
    "# ############################ generator ############################\n",
    "\n",
    "# x = Input(shape=(latent_dim,))\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "# inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "# layer_1 = Dense(128*2)(inputs)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(128*28*22)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# reshaped = Reshape((28, 22, 128))(layer_2_a)\n",
    "# # reshaped_s = UpSampling2D()(reshaped)\n",
    "\n",
    "# conv1 = Conv2D(128, kernel_size=1)(reshaped)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# conv1_a_s = UpSampling2D()(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(64, kernel_size=1)(conv1_a_s)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "# outputs = Conv2D(1, kernel_size=1,activation='relu')(conv2_a)\n",
    "\n",
    "# generator = Model([x, cond], outputs, name='generator')\n",
    "# generator.summary()\n",
    "\n",
    "# ############################ discriminator ############################\n",
    "\n",
    "# input_img = Input(shape=[56,44,2],name='input_img')\n",
    "# conv1 = Conv2D(32, kernel_size=3)(input_img)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "# conv2 = Conv2D(16, kernel_size=3)(pool1)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "# pool2 = MaxPooling2D(pool_size=(2, 2))(conv2_a)\n",
    "# flat = Flatten()(pool2)\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "\n",
    "# inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "# layer_1 = Dense(128)(inputs2)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "# layer_2 = Dense(64)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "# outputs = Dense(1, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "# discriminator = Model([input_img, cond], outputs, name='discriminator')\n",
    "# discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IDEA\n",
    "Why do we only take into account a loss of only single generated image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def discriminator_loss(real_output, fake_output, fake_output_2):\n",
    "#     real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "#     fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "#     fake_loss_2 = cross_entropy(tf.zeros_like(fake_output_2), fake_output_2)\n",
    "#     total_loss = real_loss + fake_loss + fake_loss_2\n",
    "    \n",
    "#     # update state of accuracy of real and false images\n",
    "#     d_acc_r.update_state(tf.ones_like(real_output), real_output)\n",
    "#     d_acc_f.update_state(tf.zeros_like(fake_output), fake_output)\n",
    "#     return total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    # update state of accuracy of real and false images\n",
    "    d_acc_r.update_state(tf.ones_like(real_output), real_output)\n",
    "    d_acc_f.update_state(tf.zeros_like(fake_output), fake_output)\n",
    "    return total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "d_acc_r = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "d_acc_f = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "g_acc = keras.metrics.BinaryAccuracy(name=\"g_acc_g\", threshold=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IDEA\n",
    "Why do we only take into accoutn a single genrated image, not from two latent codes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def generator_loss(step, fake_output, fake_output_2,\n",
    "#                    fake_latent, fake_latent_2,\n",
    "#                    noise, noise_2,\n",
    "#                    std_proton, std_neutron):\n",
    "\n",
    "#     g_acc.update_state(tf.ones_like(fake_output), fake_output)\n",
    "#     g_acc.update_state(tf.ones_like(fake_output_2), fake_output_2)\n",
    "\n",
    "#     crossentropy_loss = cross_entropy(tf.ones_like(fake_output), fake_output) + cross_entropy(tf.ones_like(fake_output_2), fake_output_2)\n",
    "\n",
    "#     div = tf.math.divide(tf.reduce_mean(tf.abs(fake_latent - fake_latent_2),(1)), tf.reduce_mean(tf.abs(noise-noise_2),(1)))\n",
    "\n",
    "#     div_loss_proton = std_proton * STRENGTH / (div + 1e-5)\n",
    "#     div_loss_neutron = std_neutron * STRENGTH / (div + 1e-5)\n",
    "\n",
    "#     div_loss_proton = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_proton,(1)), div_loss_proton))\n",
    "#     div_loss_neutron = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_neutron,(1)), div_loss_neutron))\n",
    "#     div_loss = div_loss_proton + div_loss_neutron\n",
    "#     return crossentropy_loss + div_loss, div_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IDEA\n",
    "Should we take mean of diversity loss or just sum the up?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generator_loss(step, fake_output,\n",
    "                   fake_latent, fake_latent_2,\n",
    "                   noise, noise_2,\n",
    "                   std_proton, std_neutron):\n",
    "\n",
    "    g_acc.update_state(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    crossentropy_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    div = tf.math.divide(tf.reduce_mean(tf.abs(fake_latent - fake_latent_2),(1)), tf.reduce_mean(tf.abs(noise-noise_2),(1)))\n",
    "\n",
    "    div_loss_proton = std_proton * STRENGTH / (div + 1e-5)\n",
    "    div_loss_neutron = std_neutron * STRENGTH / (div + 1e-5)\n",
    "\n",
    "    div_loss_proton = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_proton,(1)), div_loss_proton))\n",
    "    div_loss_neutron = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_neutron,(1)), div_loss_neutron))\n",
    "\n",
    "    # average diversity loss\n",
    "    div_loss = div_loss_proton + div_loss_neutron\n",
    "    return crossentropy_loss + div_loss, div_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "noise_dim = latent_dim\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "START_GENERATING_IMG_FROM_IDX = 20\n",
    "# Seed to reuse for generating samples for comparison during training\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed_cond = y_test[START_GENERATING_IMG_FROM_IDX:START_GENERATING_IMG_FROM_IDX+num_examples_to_generate]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Generative Models for CERN Fast Simulations\",\n",
    "    name=wandb_run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"Model\": NAME,\n",
    "    \"dataset\": \"proton_neutron_data\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"Date\": DATE_STR,\n",
    "    \"latent_dimension\": latent_dim,\n",
    "    \"Proton_min\": photon_sum_proton_min,\n",
    "    \"Proton_max\": photon_sum_proton_max,\n",
    "    \"Experiment_dir_name\": EXPERIMENT_DIR_NAME,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "    },\n",
    "    tags=[f\"proton_min_{photon_sum_proton_min}\",\n",
    "          f\"proton_max_{photon_sum_proton_max}\",\n",
    "          f\"neutron_min_{photon_sum_neutron_min}\",\n",
    "          f\"neutron_max_{photon_sum_neutron_max}\",\n",
    "          f\"gan_strength_{STRENGTH}\", \"sdi-gan\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "org=np.exp(x_test)-1\n",
    "ch_org = np.array(org).reshape(-1,56,44)\n",
    "ch_org = pd.DataFrame(sum_channels_parallel(ch_org)).values\n",
    "del org\n",
    "\n",
    "\n",
    "def calculate_ws_ch(n_calc):\n",
    "    ws= [0,0,0,0,0]\n",
    "    for j in range(n_calc):\n",
    "        z = np.random.normal(0,1,(x_test.shape[0],latent_dim))\n",
    "        z_c = y_test\n",
    "        results = generator.predict([z,z_c])\n",
    "        results = np.exp(results)-1\n",
    "        try:\n",
    "            ch_gen = np.array(results).reshape(-1,56,44)\n",
    "            ch_gen = pd.DataFrame(sum_channels_parallel(ch_gen)).values\n",
    "            for i in range(5):\n",
    "                ws[i] = ws[i] + wasserstein_distance(ch_org[:,i], ch_gen[:,i])\n",
    "            ws =np.array(ws)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "    ws = ws/n_calc\n",
    "    ws_mean = ws.sum()/5\n",
    "    print(\"ws mean\",f'{ws_mean:.2f}', end=\" \")\n",
    "    for n, score in enumerate(ws):\n",
    "        print(\"ch\"+str(n+1),f'{score:.2f}',end=\" \")\n",
    "    return ws_mean\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def generator_loss(step, fake_output, fake_output_2,\n",
    "#                    fake_latent, fake_latent_2,\n",
    "#                    noise, noise_2,\n",
    "#                    std_proton, std_neutron):\n",
    "\n",
    "#     g_acc.update_state(tf.ones_like(fake_output), fake_output)\n",
    "#     g_acc.update_state(tf.ones_like(fake_output_2), fake_output_2)\n",
    "\n",
    "#     crossentropy_loss = cross_entropy(tf.ones_like(fake_output), fake_output) + cross_entropy(tf.ones_like(fake_output_2), fake_output_2)\n",
    "\n",
    "#     div = tf.math.divide(tf.reduce_mean(tf.abs(fake_latent - fake_latent_2),(1)), tf.reduce_mean(tf.abs(noise-noise_2),(1)))\n",
    "\n",
    "#     div_loss_proton = std_proton * STRENGTH / (div + 1e-5)\n",
    "#     div_loss_neutron = std_neutron * STRENGTH / (div + 1e-5)\n",
    "\n",
    "#     div_loss_proton = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_proton,(1)), div_loss_proton))\n",
    "#     div_loss_neutron = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_neutron,(1)), div_loss_neutron))\n",
    "#     div_loss = div_loss_proton + div_loss_neutron\n",
    "#     return crossentropy_loss + div_loss, div_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### IDEA\n",
    "Should we take mean of diversity loss or just sum the up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yjX97hnkkmlf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(step, fake_output,\n",
    "                   fake_latent, fake_latent_2,\n",
    "                   noise, noise_2,\n",
    "                   std_proton, std_neutron):\n",
    "\n",
    "    g_acc.update_state(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    crossentropy_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    div = tf.math.divide(tf.reduce_mean(tf.abs(fake_latent - fake_latent_2),(1)), tf.reduce_mean(tf.abs(noise-noise_2),(1)))\n",
    "\n",
    "    div_loss_proton = std_proton * STRENGTH / (div + 1e-5)\n",
    "    div_loss_neutron = std_neutron * STRENGTH / (div + 1e-5)\n",
    "\n",
    "    div_loss_proton = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_proton,(1)), div_loss_proton))\n",
    "    div_loss_neutron = tf.reduce_mean(tf.math.multiply(tf.reduce_mean(std_neutron,(1)), div_loss_neutron))\n",
    "\n",
    "    # average diversity loss\n",
    "    div_loss = div_loss_proton + div_loss_neutron\n",
    "    return crossentropy_loss + div_loss, div_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "I4HsHLgwkurp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "noise_dim = latent_dim\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "START_GENERATING_IMG_FROM_IDX = 20\n",
    "# Seed to reuse for generating samples for comparison during training\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed_cond = y_test[START_GENERATING_IMG_FROM_IDX:START_GENERATING_IMG_FROM_IDX+num_examples_to_generate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbedkowski-patrick\u001B[0m (\u001B[33mnlp-wut-2023\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/studio-lab-user/Generative_Models_for_CERN_Fast_Simulations/notebooks/gan_proton_15_2312/wandb/run-20230709_150456-miyita2q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/miyita2q' target=\"_blank\">20_1970_09_07_2023_15_04</a></strong> to <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations' target=\"_blank\">https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/miyita2q' target=\"_blank\">https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/miyita2q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-wut-2023/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/miyita2q?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f40902abfa0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Generative Models for CERN Fast Simulations\",\n",
    "    name=wandb_run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"Model\": NAME,\n",
    "    \"dataset\": \"proton_neutron_data\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"Date\": DATE_STR,\n",
    "    \"latent_dimension\": latent_dim,\n",
    "    \"Proton_min\": photon_sum_proton_min,\n",
    "    \"Proton_max\": photon_sum_proton_max,\n",
    "    \"Experiment_dir_name\": EXPERIMENT_DIR_NAME,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "    },\n",
    "    tags=[f\"proton_min_{photon_sum_proton_min}\",\n",
    "          f\"proton_max_{photon_sum_proton_max}\",\n",
    "          f\"neutron_min_{photon_sum_neutron_min}\",\n",
    "          f\"neutron_max_{photon_sum_neutron_max}\",\n",
    "          f\"gan_strength_{STRENGTH}\", \"sdi-gan\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rMxBrHhsTDXO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "org=np.exp(x_test)-1\n",
    "ch_org = np.array(org).reshape(-1,56,44)\n",
    "ch_org = pd.DataFrame(sum_channels_parallel(ch_org)).values\n",
    "del org\n",
    "\n",
    "\n",
    "def calculate_ws_ch(n_calc):\n",
    "    ws= [0,0,0,0,0]\n",
    "    for j in range(n_calc):\n",
    "        z = np.random.normal(0,1,(x_test.shape[0],latent_dim))\n",
    "        z_c = y_test\n",
    "        results = generator.predict([z,z_c])\n",
    "        results = np.exp(results)-1\n",
    "        try:\n",
    "            ch_gen = np.array(results).reshape(-1,56,44)\n",
    "            ch_gen = pd.DataFrame(sum_channels_parallel(ch_gen)).values\n",
    "            for i in range(5):\n",
    "                ws[i] = ws[i] + wasserstein_distance(ch_org[:,i], ch_gen[:,i])\n",
    "            ws =np.array(ws)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "    ws = ws/n_calc\n",
    "    ws_mean = ws.sum()/5\n",
    "    print(\"ws mean\",f'{ws_mean:.2f}', end=\" \")\n",
    "    for n, score in enumerate(ws):\n",
    "        print(\"ch\"+str(n+1),f'{score:.2f}',end=\" \")\n",
    "    return ws_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GmlMSiqCku5_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch, step):\n",
    "    # dataset, dataset_2, dataset_cond, dataset_std_proton, dataset_std_neutron, fake_cond\n",
    "    images, images_2, cond, std_proton, std_neutron, noise_cond = batch\n",
    "    step=step\n",
    "    BATCH_SIZE = tf.shape(images)[0]\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    noise_2 = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # for the same conditional data generate two images from different noises\n",
    "        generated_images = generator([noise, noise_cond], training=True)\n",
    "        generated_images_2 = generator([noise_2, noise_cond], training=True)\n",
    "        \n",
    "        # produce if real image is real or fake\n",
    "        real_output, real_latent = discriminator([images, cond], training=True)\n",
    "        # real_output_2,real_latent_2  = discriminator([images_2,cond], training=True)\n",
    "        \n",
    "        # produce if generated images from two different latent codes are real or fake\n",
    "        fake_output, fake_latent = discriminator([generated_images, noise_cond], training=True)\n",
    "        fake_output_2, fake_latent_2 = discriminator([generated_images_2, noise_cond], training=True)\n",
    "\n",
    "        gen_loss, div_loss = generator_loss(step, fake_output,\n",
    "                                            fake_latent, fake_latent_2,\n",
    "                                            noise, noise_2,\n",
    "                                            std_proton, std_neutron)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "\n",
    "    #         generated_images = generator([noise,noise_cond], training=True)\n",
    "\n",
    "    #         real_output = discriminator([images,cond], training=True)\n",
    "    #         fake_output = discriminator([generated_images, noise_cond], training=True)\n",
    "\n",
    "    #         gen_loss = generator_loss(step, fake_output)\n",
    "    #         real_loss, fake_loss = discriminator_loss(real_output, fake_output)\n",
    "    #         disc_loss = real_loss + fake_loss\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, div_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "D-wATS0PkvJo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If model achieves WS metric less or equal to this number, its weights will be saved\n",
    "WS_MEAN_SAVE_THRESHOLD = 20\n",
    "\n",
    "\n",
    "if SAVE_EXPERIMENT_DATA:\n",
    "    filepath_mod = f\"../../{EXPERIMENT_DIR_NAME}/models/\"\n",
    "    create_dir(filepath_mod)\n",
    "\n",
    "history = []\n",
    "def train(dataset, epochs):\n",
    "    experiment_start = time.time()\n",
    "    tf_step = tf.Variable(0, dtype=float)\n",
    "    step=0\n",
    "\n",
    "    # generate first image\n",
    "    generate_and_save_images(generator,\n",
    "                             epochs,\n",
    "                             [seed, seed_cond])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        gen_loss_epoch = []\n",
    "        div_loss_epoch = []\n",
    "        disc_loss_epoch = []\n",
    "        for batch in dataset:\n",
    "            gen_loss, disc_loss, div_loss =train_step(batch,tf_step)\n",
    "\n",
    "            history.append([gen_loss,disc_loss,\n",
    "                100*d_acc_r.result().numpy(),\n",
    "                100*d_acc_f.result().numpy(),\n",
    "                100*g_acc.result().numpy(),\n",
    "                ])\n",
    "            tf_step.assign_add(1)\n",
    "            step = step+1\n",
    "\n",
    "            gen_loss_epoch.append(gen_loss)\n",
    "            disc_loss_epoch.append(disc_loss)\n",
    "            div_loss_epoch.append(div_loss)\n",
    "            if step % 100 == 0:\n",
    "                print(\"%d [D real acc: %.2f%%] [D fake acc: %.2f%%] [G acc: %.2f%%] \"% (\n",
    "                    step,\n",
    "                    100*d_acc_r.result().numpy(),\n",
    "                    100*d_acc_f.result().numpy(),\n",
    "                    100*g_acc.result().numpy()))\n",
    "\n",
    "        plot = generate_and_save_images(generator,\n",
    "                                 epoch,\n",
    "                                 [seed, seed_cond])\n",
    "\n",
    "        ws_mean = calculate_ws_ch(min(epoch//5+1,5))\n",
    "\n",
    "        if SAVE_EXPERIMENT_DATA:\n",
    "            if ws_mean <= WS_MEAN_SAVE_THRESHOLD:\n",
    "                # Save the model every epoch\n",
    "                generator.compile()\n",
    "                # discriminator.compile()\n",
    "                generator.save((os.path.join(filepath_mod, \"gen_\"+NAME + \"_\"+ str(epoch) +\".h5\")))\n",
    "                # discriminator.save((os.path.join(filepath_mod, \"disc_\"+NAME + \"_\"+ str(epoch) +\".h5\")))\n",
    "                # np.savez(os.path.join(filepath_mod, \"history_\"+NAME+\".npz\"),np.array(history))\n",
    "\n",
    "        wandb.log({\n",
    "            'ws_mean': ws_mean,\n",
    "            'gen_loss': np.mean(gen_loss_epoch),\n",
    "            'div_loss': np.mean(div_loss_epoch),\n",
    "            'disc_loss': np.mean(disc_loss_epoch),\n",
    "            'epoch': epoch,\n",
    "            'plot': wandb.Image(plot),\n",
    "            'experiment_time': time.time()-experiment_start\n",
    "        })\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "CxeGwn7ek8Q-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_EXPERIMENT_DATA:\n",
    "    filepath_img = f\"../../{EXPERIMENT_DIR_NAME}/images/\"\n",
    "    create_dir(filepath_img)\n",
    "\n",
    "    \n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    START_INDEX = 6\n",
    "    SUPTITLE_TXT = f\"\\nModel: GAN proton data\" \\\n",
    "               f\"\\nPhotonsum interval: [{photon_sum_proton_min}, {photon_sum_proton_max}]\" \\\n",
    "               f\"\\nEPOCH: {epoch}\"\n",
    "\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "    \"\"\"\n",
    "    predictions has shape (n_samples, 56, 44, 2). First channel has proton data, second has neutrons\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "    plt.title(f\"EPOCH {epoch}\")\n",
    "\n",
    "    subfigs = fig.subfigures(1, 4)\n",
    "\n",
    "    for particle_num, subfig in enumerate(subfigs.flat):  # iterate over 4 particles\n",
    "        subfig.suptitle(f'Particle {particle_num} response')\n",
    "        axs = subfig.subplots(2, 2)\n",
    "        for i, ax in enumerate(axs.flat):  # iterate over 4 images of single particle\n",
    "            m_2 = i % 2  # 0 if proton, 1 if neutron\n",
    "            if i < 2:\n",
    "                # Real response\n",
    "                x = x_test[START_INDEX+particle_num][:,:,m_2].reshape(56,44)\n",
    "                axs[i//2, m_2].set_title(\"neutron\" if m_2 else \"proton\")\n",
    "            else:\n",
    "                # Generated response\n",
    "                x = predictions[START_INDEX+particle_num].numpy()[:,:,m_2].reshape(56,44)\n",
    "            axs[i//2, m_2].set_axis_off()\n",
    "            im = axs[i//2, m_2].imshow(x, interpolation='none', cmap='gnuplot')\n",
    "            fig.colorbar(im, ax=axs[i//2, m_2])\n",
    "\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        plt.savefig(os.path.join(filepath_img, 'image_at_epoch_{:04d}.png'.format(epoch)))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmR61h2W0vxC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D real acc: 28.84%] [D fake acc: 79.39%] [G acc: 20.61%] \n",
      "200 [D real acc: 41.35%] [D fake acc: 75.96%] [G acc: 24.04%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "ws mean 85345.42 ch1 208.35 ch2 213.12 ch3 218.16 ch4 424678.04 ch5 1409.44 Time for epoch 1 is 147.5119824409485 sec\n",
      "300 [D real acc: 50.50%] [D fake acc: 71.60%] [G acc: 28.40%] \n",
      "400 [D real acc: 55.58%] [D fake acc: 69.57%] [G acc: 30.43%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 759654153374984593997824.00 ch1 15862.25 ch2 3781326286550367037554688.00 ch3 170691.98 ch4 16944480324555928240128.00 ch5 72158681.55 Time for epoch 2 is 131.0457751750946 sec\n",
      "500 [D real acc: 58.91%] [D fake acc: 68.29%] [G acc: 31.71%] \n",
      "600 [D real acc: 61.11%] [D fake acc: 68.38%] [G acc: 31.62%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 1384009587.86 ch1 5380.35 ch2 1973242082.92 ch3 444736.87 ch4 4760731.37 ch5 4941595007.80 Time for epoch 3 is 132.08433890342712 sec\n",
      "700 [D real acc: 63.05%] [D fake acc: 68.45%] [G acc: 31.55%] \n",
      "800 [D real acc: 64.38%] [D fake acc: 68.08%] [G acc: 31.92%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 29731079172.38 ch1 2629.57 ch2 1606777373.12 ch3 1705.78 ch4 3127196.37 ch5 147045486957.06 Time for epoch 4 is 132.93305730819702 sec\n",
      "900 [D real acc: 65.17%] [D fake acc: 68.11%] [G acc: 31.89%] \n",
      "1000 [D real acc: 65.82%] [D fake acc: 68.16%] [G acc: 31.84%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 18935.51 ch1 781.17 ch2 3723.15 ch3 65480.61 ch4 3939.77 ch5 20752.83 Time for epoch 5 is 132.75829887390137 sec\n",
      "1100 [D real acc: 66.48%] [D fake acc: 68.32%] [G acc: 31.68%] \n",
      "1200 [D real acc: 67.18%] [D fake acc: 68.72%] [G acc: 31.28%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 222.42 ch1 48.08 ch2 141.29 ch3 158.63 ch4 148.09 ch5 615.98 Time for epoch 6 is 136.77618312835693 sec\n",
      "1300 [D real acc: 67.63%] [D fake acc: 69.15%] [G acc: 30.85%] \n",
      "1400 [D real acc: 68.02%] [D fake acc: 69.32%] [G acc: 30.68%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 465.57 ch1 10.72 ch2 15.00 ch3 49.16 ch4 33.69 ch5 2219.31 Time for epoch 7 is 137.10431933403015 sec\n",
      "1500 [D real acc: 68.42%] [D fake acc: 69.45%] [G acc: 30.55%] \n",
      "1600 [D real acc: 68.72%] [D fake acc: 69.72%] [G acc: 30.28%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 66.86 ch1 15.69 ch2 32.88 ch3 72.69 ch4 69.14 ch5 143.90 Time for epoch 8 is 136.59992241859436 sec\n",
      "1700 [D real acc: 69.13%] [D fake acc: 69.91%] [G acc: 30.09%] \n",
      "1800 [D real acc: 69.38%] [D fake acc: 70.16%] [G acc: 29.84%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 29.19 ch1 9.64 ch2 12.29 ch3 57.41 ch4 33.60 ch5 33.02 Time for epoch 9 is 136.7799334526062 sec\n",
      "1900 [D real acc: 69.72%] [D fake acc: 70.40%] [G acc: 29.60%] \n",
      "2000 [D real acc: 70.09%] [D fake acc: 70.63%] [G acc: 29.37%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 38.74 ch1 14.09 ch2 14.13 ch3 64.06 ch4 38.70 ch5 62.76 Time for epoch 10 is 137.11405277252197 sec\n",
      "2100 [D real acc: 70.45%] [D fake acc: 70.89%] [G acc: 29.11%] \n",
      "2200 [D real acc: 70.76%] [D fake acc: 71.01%] [G acc: 28.99%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 40.79 ch1 14.84 ch2 24.92 ch3 36.67 ch4 38.86 ch5 88.66 Time for epoch 11 is 140.62762427330017 sec\n",
      "2300 [D real acc: 70.96%] [D fake acc: 71.18%] [G acc: 28.82%] \n",
      "2400 [D real acc: 71.18%] [D fake acc: 71.33%] [G acc: 28.67%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 20.41 ch1 7.92 ch2 10.73 ch3 31.37 ch4 26.03 ch5 26.02 Time for epoch 12 is 140.64264273643494 sec\n",
      "2500 [D real acc: 71.38%] [D fake acc: 71.48%] [G acc: 28.52%] \n",
      "2600 [D real acc: 71.60%] [D fake acc: 71.64%] [G acc: 28.36%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 30.27 ch1 8.42 ch2 12.09 ch3 31.35 ch4 32.12 ch5 67.36 Time for epoch 13 is 140.6182737350464 sec\n",
      "2700 [D real acc: 71.85%] [D fake acc: 71.85%] [G acc: 28.15%] \n",
      "2800 [D real acc: 72.05%] [D fake acc: 72.01%] [G acc: 27.99%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 29.80 ch1 9.94 ch2 19.84 ch3 48.17 ch4 22.05 ch5 48.98 Time for epoch 14 is 141.21516752243042 sec\n",
      "2900 [D real acc: 72.28%] [D fake acc: 72.22%] [G acc: 27.78%] \n",
      "3000 [D real acc: 72.53%] [D fake acc: 72.44%] [G acc: 27.56%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 41.24 ch1 8.83 ch2 21.16 ch3 86.94 ch4 32.42 ch5 56.83 Time for epoch 15 is 140.60038352012634 sec\n",
      "3100 [D real acc: 72.79%] [D fake acc: 72.71%] [G acc: 27.29%] \n",
      "3200 [D real acc: 73.07%] [D fake acc: 72.97%] [G acc: 27.03%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 59.11 ch1 9.41 ch2 10.75 ch3 57.93 ch4 40.99 ch5 176.49 Time for epoch 16 is 144.54106187820435 sec\n",
      "3300 [D real acc: 73.33%] [D fake acc: 73.13%] [G acc: 26.87%] \n",
      "3400 [D real acc: 73.55%] [D fake acc: 73.33%] [G acc: 26.67%] \n",
      "3500 [D real acc: 73.70%] [D fake acc: 73.44%] [G acc: 26.56%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 56.84 ch1 19.03 ch2 59.81 ch3 28.88 ch4 137.79 ch5 38.70 Time for epoch 17 is 144.52710127830505 sec\n",
      "3600 [D real acc: 73.79%] [D fake acc: 73.56%] [G acc: 26.44%] \n",
      "3700 [D real acc: 73.83%] [D fake acc: 73.64%] [G acc: 26.36%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 17.44 ch1 8.71 ch2 15.02 ch3 22.79 ch4 25.04 ch5 15.62 Time for epoch 18 is 144.7081503868103 sec\n",
      "3800 [D real acc: 73.89%] [D fake acc: 73.72%] [G acc: 26.28%] \n",
      "3900 [D real acc: 73.97%] [D fake acc: 73.77%] [G acc: 26.23%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 88.12 ch1 5.22 ch2 63.79 ch3 33.95 ch4 119.05 ch5 218.57 Time for epoch 19 is 145.82728505134583 sec\n",
      "4000 [D real acc: 74.01%] [D fake acc: 73.75%] [G acc: 26.25%] \n",
      "4100 [D real acc: 74.05%] [D fake acc: 73.76%] [G acc: 26.24%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 133.86 ch1 81.95 ch2 95.03 ch3 94.09 ch4 160.36 ch5 237.84 Time for epoch 20 is 145.0569713115692 sec\n",
      "4200 [D real acc: 74.05%] [D fake acc: 73.79%] [G acc: 26.21%] \n",
      "4300 [D real acc: 74.06%] [D fake acc: 73.85%] [G acc: 26.15%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 18.49 ch1 11.46 ch2 9.58 ch3 18.73 ch4 24.85 ch5 27.83 Time for epoch 21 is 149.16402578353882 sec\n",
      "4400 [D real acc: 74.10%] [D fake acc: 73.88%] [G acc: 26.12%] \n",
      "4500 [D real acc: 74.13%] [D fake acc: 73.94%] [G acc: 26.06%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 17.59 ch1 9.57 ch2 5.92 ch3 19.17 ch4 37.29 ch5 16.00 Time for epoch 22 is 149.36649370193481 sec\n",
      "4600 [D real acc: 74.17%] [D fake acc: 73.95%] [G acc: 26.05%] \n",
      "4700 [D real acc: 74.21%] [D fake acc: 74.03%] [G acc: 25.97%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 30.82 ch1 6.30 ch2 11.76 ch3 58.07 ch4 33.03 ch5 44.97 Time for epoch 23 is 149.1711790561676 sec\n",
      "4800 [D real acc: 74.25%] [D fake acc: 74.09%] [G acc: 25.91%] \n",
      "4900 [D real acc: 74.28%] [D fake acc: 74.11%] [G acc: 25.89%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 24.77 ch1 9.70 ch2 15.04 ch3 41.43 ch4 31.97 ch5 25.69 Time for epoch 24 is 148.9593381881714 sec\n",
      "5000 [D real acc: 74.31%] [D fake acc: 74.16%] [G acc: 25.84%] \n",
      "5100 [D real acc: 74.36%] [D fake acc: 74.18%] [G acc: 25.82%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 21.18 ch1 6.17 ch2 28.89 ch3 28.66 ch4 14.82 ch5 27.36 Time for epoch 25 is 149.0506043434143 sec\n",
      "5200 [D real acc: 74.38%] [D fake acc: 74.21%] [G acc: 25.79%] \n",
      "5300 [D real acc: 74.40%] [D fake acc: 74.25%] [G acc: 25.75%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 19.10 ch1 8.22 ch2 8.18 ch3 14.73 ch4 18.88 ch5 45.47 Time for epoch 26 is 150.05217671394348 sec\n",
      "5400 [D real acc: 74.44%] [D fake acc: 74.28%] [G acc: 25.72%] \n",
      "5500 [D real acc: 74.48%] [D fake acc: 74.30%] [G acc: 25.70%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 19.51 ch1 3.62 ch2 6.39 ch3 27.07 ch4 37.89 ch5 22.59 Time for epoch 27 is 149.28665399551392 sec\n",
      "5600 [D real acc: 74.52%] [D fake acc: 74.34%] [G acc: 25.66%] \n",
      "5700 [D real acc: 74.55%] [D fake acc: 74.35%] [G acc: 25.65%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 15.90 ch1 4.06 ch2 8.81 ch3 36.42 ch4 15.95 ch5 14.27 Time for epoch 28 is 149.23780870437622 sec\n",
      "5800 [D real acc: 74.60%] [D fake acc: 74.38%] [G acc: 25.62%] \n",
      "5900 [D real acc: 74.63%] [D fake acc: 74.36%] [G acc: 25.64%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 23.65 ch1 7.20 ch2 15.97 ch3 25.67 ch4 47.44 ch5 21.98 Time for epoch 29 is 149.17538261413574 sec\n",
      "6000 [D real acc: 74.61%] [D fake acc: 74.36%] [G acc: 25.64%] \n",
      "6100 [D real acc: 74.63%] [D fake acc: 74.37%] [G acc: 25.63%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 13.28 ch1 4.06 ch2 7.95 ch3 10.38 ch4 20.83 ch5 23.19 Time for epoch 30 is 149.1058702468872 sec\n",
      "6200 [D real acc: 74.66%] [D fake acc: 74.37%] [G acc: 25.63%] \n",
      "6300 [D real acc: 74.70%] [D fake acc: 74.43%] [G acc: 25.57%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 13.14 ch1 4.21 ch2 7.88 ch3 18.21 ch4 17.37 ch5 18.03 Time for epoch 31 is 149.35109615325928 sec\n",
      "6400 [D real acc: 74.73%] [D fake acc: 74.47%] [G acc: 25.53%] \n",
      "6500 [D real acc: 74.77%] [D fake acc: 74.51%] [G acc: 25.49%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 18.86 ch1 4.64 ch2 8.96 ch3 30.39 ch4 27.32 ch5 23.00 Time for epoch 32 is 149.36485815048218 sec\n",
      "6600 [D real acc: 74.81%] [D fake acc: 74.54%] [G acc: 25.46%] \n",
      "6700 [D real acc: 74.86%] [D fake acc: 74.59%] [G acc: 25.41%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 23.23 ch1 7.75 ch2 8.72 ch3 19.05 ch4 59.16 ch5 21.45 Time for epoch 33 is 149.23117876052856 sec\n",
      "6800 [D real acc: 74.87%] [D fake acc: 74.61%] [G acc: 25.39%] \n",
      "6900 [D real acc: 74.90%] [D fake acc: 74.63%] [G acc: 25.37%] \n",
      "7000 [D real acc: 74.94%] [D fake acc: 74.65%] [G acc: 25.35%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 27.62 ch1 3.93 ch2 14.52 ch3 26.41 ch4 54.49 ch5 38.74 Time for epoch 34 is 150.13516926765442 sec\n",
      "7100 [D real acc: 74.98%] [D fake acc: 74.69%] [G acc: 25.31%] \n",
      "7200 [D real acc: 75.02%] [D fake acc: 74.74%] [G acc: 25.26%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 14.60 ch1 3.05 ch2 7.78 ch3 21.42 ch4 19.55 ch5 21.20 Time for epoch 35 is 149.13132977485657 sec\n",
      "7300 [D real acc: 75.08%] [D fake acc: 74.77%] [G acc: 25.23%] \n",
      "7400 [D real acc: 75.12%] [D fake acc: 74.81%] [G acc: 25.19%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 49.97 ch1 8.78 ch2 20.54 ch3 74.66 ch4 66.00 ch5 79.85 Time for epoch 36 is 149.13396430015564 sec\n",
      "7500 [D real acc: 75.17%] [D fake acc: 74.84%] [G acc: 25.16%] \n",
      "7600 [D real acc: 75.22%] [D fake acc: 74.89%] [G acc: 25.11%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 30.29 ch1 3.63 ch2 10.30 ch3 57.36 ch4 30.92 ch5 49.25 Time for epoch 37 is 145.42401790618896 sec\n",
      "7700 [D real acc: 75.26%] [D fake acc: 74.94%] [G acc: 25.06%] \n",
      "7800 [D real acc: 75.30%] [D fake acc: 74.97%] [G acc: 25.03%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 17.66 ch1 2.27 ch2 7.36 ch3 36.61 ch4 23.14 ch5 18.94 Time for epoch 38 is 142.80615615844727 sec\n",
      "7900 [D real acc: 75.34%] [D fake acc: 75.01%] [G acc: 24.99%] \n",
      "8000 [D real acc: 75.39%] [D fake acc: 75.04%] [G acc: 24.96%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 80.40 ch1 6.06 ch2 19.47 ch3 31.94 ch4 47.62 ch5 296.93 Time for epoch 39 is 148.11543011665344 sec\n",
      "8100 [D real acc: 75.43%] [D fake acc: 75.08%] [G acc: 24.92%] \n",
      "8200 [D real acc: 75.48%] [D fake acc: 75.14%] [G acc: 24.86%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 23.81 ch1 8.75 ch2 19.38 ch3 14.39 ch4 8.63 ch5 67.89 Time for epoch 40 is 149.4792604446411 sec\n",
      "8300 [D real acc: 75.52%] [D fake acc: 75.18%] [G acc: 24.82%] \n",
      "8400 [D real acc: 75.58%] [D fake acc: 75.24%] [G acc: 24.76%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 33.41 ch1 5.61 ch2 14.54 ch3 35.52 ch4 21.20 ch5 90.19 Time for epoch 41 is 149.51433396339417 sec\n",
      "8500 [D real acc: 75.65%] [D fake acc: 75.29%] [G acc: 24.71%] \n",
      "8600 [D real acc: 75.70%] [D fake acc: 75.34%] [G acc: 24.66%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 14.28 ch1 5.46 ch2 12.15 ch3 13.70 ch4 12.91 ch5 27.20 Time for epoch 42 is 149.5881049633026 sec\n",
      "8700 [D real acc: 75.75%] [D fake acc: 75.38%] [G acc: 24.62%] \n",
      "8800 [D real acc: 75.80%] [D fake acc: 75.42%] [G acc: 24.58%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 13.50 ch1 6.85 ch2 8.67 ch3 19.51 ch4 8.21 ch5 24.27 Time for epoch 43 is 151.00100207328796 sec\n",
      "8900 [D real acc: 75.84%] [D fake acc: 75.47%] [G acc: 24.53%] \n",
      "9000 [D real acc: 75.88%] [D fake acc: 75.49%] [G acc: 24.51%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 23.28 ch1 2.67 ch2 17.55 ch3 29.61 ch4 28.07 ch5 38.50 Time for epoch 44 is 149.36360216140747 sec\n",
      "9100 [D real acc: 75.93%] [D fake acc: 75.54%] [G acc: 24.46%] \n",
      "9200 [D real acc: 75.98%] [D fake acc: 75.58%] [G acc: 24.42%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 27.25 ch1 11.59 ch2 8.64 ch3 21.19 ch4 46.43 ch5 48.40 Time for epoch 45 is 149.522522687912 sec\n",
      "9300 [D real acc: 76.02%] [D fake acc: 75.61%] [G acc: 24.39%] \n",
      "9400 [D real acc: 76.08%] [D fake acc: 75.67%] [G acc: 24.33%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 32.56 ch1 8.37 ch2 15.57 ch3 16.51 ch4 45.07 ch5 77.27 Time for epoch 46 is 149.5009651184082 sec\n",
      "9500 [D real acc: 76.12%] [D fake acc: 75.72%] [G acc: 24.28%] \n",
      "9600 [D real acc: 76.18%] [D fake acc: 75.77%] [G acc: 24.23%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 13.94 ch1 5.37 ch2 10.66 ch3 13.77 ch4 13.27 ch5 26.65 Time for epoch 47 is 149.57907009124756 sec\n",
      "9700 [D real acc: 76.23%] [D fake acc: 75.81%] [G acc: 24.19%] \n",
      "9800 [D real acc: 76.28%] [D fake acc: 75.86%] [G acc: 24.14%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 20.94 ch1 3.07 ch2 7.16 ch3 38.86 ch4 19.85 ch5 35.78 Time for epoch 48 is 149.65529346466064 sec\n",
      "9900 [D real acc: 76.34%] [D fake acc: 75.92%] [G acc: 24.08%] \n",
      "10000 [D real acc: 76.39%] [D fake acc: 75.96%] [G acc: 24.04%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 44.88 ch1 12.91 ch2 29.08 ch3 13.81 ch4 59.43 ch5 109.15 Time for epoch 49 is 149.44844961166382 sec\n",
      "10100 [D real acc: 76.45%] [D fake acc: 76.00%] [G acc: 24.00%] \n",
      "10200 [D real acc: 76.51%] [D fake acc: 76.06%] [G acc: 23.94%] \n",
      "10300 [D real acc: 76.56%] [D fake acc: 76.11%] [G acc: 23.89%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 17.61 ch1 4.29 ch2 9.18 ch3 44.29 ch4 7.02 ch5 23.27 Time for epoch 50 is 149.57740330696106 sec\n",
      "10400 [D real acc: 76.62%] [D fake acc: 76.15%] [G acc: 23.85%] \n",
      "10500 [D real acc: 76.68%] [D fake acc: 76.20%] [G acc: 23.80%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 23.96 ch1 7.32 ch2 8.90 ch3 34.33 ch4 29.21 ch5 40.03 Time for epoch 51 is 149.69212913513184 sec\n",
      "10600 [D real acc: 76.74%] [D fake acc: 76.26%] [G acc: 23.74%] \n",
      "10700 [D real acc: 76.79%] [D fake acc: 76.32%] [G acc: 23.68%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 16.14 ch1 2.98 ch2 16.69 ch3 9.13 ch4 25.87 ch5 26.02 Time for epoch 52 is 149.60838055610657 sec\n",
      "10800 [D real acc: 76.84%] [D fake acc: 76.38%] [G acc: 23.62%] \n",
      "10900 [D real acc: 76.90%] [D fake acc: 76.44%] [G acc: 23.56%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 20.10 ch1 4.70 ch2 9.20 ch3 38.93 ch4 30.96 ch5 16.71 Time for epoch 53 is 149.64760303497314 sec\n",
      "11000 [D real acc: 76.96%] [D fake acc: 76.49%] [G acc: 23.51%] \n",
      "11100 [D real acc: 77.02%] [D fake acc: 76.56%] [G acc: 23.44%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 58.14 ch1 12.81 ch2 43.18 ch3 12.83 ch4 80.03 ch5 141.87 Time for epoch 54 is 149.5417242050171 sec\n",
      "11200 [D real acc: 77.08%] [D fake acc: 76.61%] [G acc: 23.39%] \n",
      "11300 [D real acc: 77.13%] [D fake acc: 76.66%] [G acc: 23.34%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 20.55 ch1 4.96 ch2 8.18 ch3 31.54 ch4 30.18 ch5 27.88 Time for epoch 55 is 149.52256512641907 sec\n",
      "11400 [D real acc: 77.19%] [D fake acc: 76.72%] [G acc: 23.28%] \n",
      "11500 [D real acc: 77.25%] [D fake acc: 76.77%] [G acc: 23.23%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 23.26 ch1 10.99 ch2 12.64 ch3 18.21 ch4 33.45 ch5 41.02 Time for epoch 56 is 151.0490324497223 sec\n",
      "11600 [D real acc: 77.30%] [D fake acc: 76.81%] [G acc: 23.19%] \n",
      "11700 [D real acc: 77.37%] [D fake acc: 76.87%] [G acc: 23.13%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 25.10 ch1 7.35 ch2 14.37 ch3 11.45 ch4 30.56 ch5 61.77 Time for epoch 57 is 149.48392343521118 sec\n",
      "11800 [D real acc: 77.42%] [D fake acc: 76.92%] [G acc: 23.08%] \n",
      "11900 [D real acc: 77.48%] [D fake acc: 76.98%] [G acc: 23.02%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 14.15 ch1 5.22 ch2 7.50 ch3 24.55 ch4 8.96 ch5 24.54 Time for epoch 58 is 151.25755214691162 sec\n",
      "12000 [D real acc: 77.54%] [D fake acc: 77.04%] [G acc: 22.96%] \n",
      "12100 [D real acc: 77.61%] [D fake acc: 77.10%] [G acc: 22.90%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 69.89 ch1 11.73 ch2 33.13 ch3 88.87 ch4 86.38 ch5 129.33 Time for epoch 59 is 149.66790533065796 sec\n",
      "12200 [D real acc: 77.67%] [D fake acc: 77.17%] [G acc: 22.83%] \n",
      "12300 [D real acc: 77.73%] [D fake acc: 77.23%] [G acc: 22.77%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 24.96 ch1 12.68 ch2 12.85 ch3 12.28 ch4 44.08 ch5 42.93 Time for epoch 60 is 149.55176901817322 sec\n",
      "12400 [D real acc: 77.79%] [D fake acc: 77.28%] [G acc: 22.72%] \n",
      "12500 [D real acc: 77.85%] [D fake acc: 77.34%] [G acc: 22.66%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 13.60 ch1 5.90 ch2 10.54 ch3 21.96 ch4 10.93 ch5 18.68 Time for epoch 61 is 149.65177130699158 sec\n",
      "12600 [D real acc: 77.93%] [D fake acc: 77.40%] [G acc: 22.60%] \n",
      "12700 [D real acc: 77.98%] [D fake acc: 77.45%] [G acc: 22.55%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 35.94 ch1 5.52 ch2 13.51 ch3 61.99 ch4 60.61 ch5 38.08 Time for epoch 62 is 149.66296482086182 sec\n",
      "12800 [D real acc: 78.03%] [D fake acc: 77.51%] [G acc: 22.49%] \n",
      "12900 [D real acc: 78.09%] [D fake acc: 77.56%] [G acc: 22.44%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 31.93 ch1 5.88 ch2 11.84 ch3 8.93 ch4 13.76 ch5 119.25 Time for epoch 63 is 149.49444556236267 sec\n",
      "13000 [D real acc: 78.15%] [D fake acc: 77.62%] [G acc: 22.38%] \n",
      "13100 [D real acc: 78.22%] [D fake acc: 77.68%] [G acc: 22.32%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 370.57 ch1 7.93 ch2 20.12 ch3 33.13 ch4 39.68 ch5 1752.01 Time for epoch 64 is 149.48038363456726 sec\n",
      "13200 [D real acc: 78.27%] [D fake acc: 77.74%] [G acc: 22.26%] \n",
      "13300 [D real acc: 78.32%] [D fake acc: 77.79%] [G acc: 22.21%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 27.73 ch1 3.93 ch2 13.95 ch3 27.39 ch4 36.38 ch5 57.00 Time for epoch 65 is 149.46471738815308 sec\n",
      "13400 [D real acc: 78.37%] [D fake acc: 77.83%] [G acc: 22.17%] \n",
      "13500 [D real acc: 78.42%] [D fake acc: 77.88%] [G acc: 22.12%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 16.31 ch1 4.35 ch2 9.94 ch3 11.57 ch4 22.38 ch5 33.30 Time for epoch 66 is 149.53062534332275 sec\n",
      "13600 [D real acc: 78.47%] [D fake acc: 77.93%] [G acc: 22.07%] \n",
      "13700 [D real acc: 78.53%] [D fake acc: 77.98%] [G acc: 22.02%] \n",
      "13800 [D real acc: 78.59%] [D fake acc: 78.04%] [G acc: 21.96%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 13.21 ch1 4.93 ch2 8.29 ch3 21.56 ch4 7.76 ch5 23.49 Time for epoch 67 is 149.70378994941711 sec\n",
      "13900 [D real acc: 78.65%] [D fake acc: 78.10%] [G acc: 21.90%] \n",
      "14000 [D real acc: 78.70%] [D fake acc: 78.15%] [G acc: 21.85%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 23.30 ch1 7.87 ch2 25.34 ch3 10.37 ch4 30.50 ch5 42.42 Time for epoch 68 is 149.62510108947754 sec\n",
      "14100 [D real acc: 78.74%] [D fake acc: 78.20%] [G acc: 21.80%] \n",
      "14200 [D real acc: 78.80%] [D fake acc: 78.26%] [G acc: 21.74%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 12.12 ch1 3.58 ch2 7.66 ch3 15.13 ch4 14.48 ch5 19.75 Time for epoch 69 is 151.1913721561432 sec\n",
      "14300 [D real acc: 78.86%] [D fake acc: 78.32%] [G acc: 21.68%] \n",
      "14400 [D real acc: 78.92%] [D fake acc: 78.38%] [G acc: 21.62%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 21.48 ch1 6.58 ch2 16.89 ch3 25.03 ch4 32.10 ch5 26.81 Time for epoch 70 is 149.5461049079895 sec\n",
      "14500 [D real acc: 78.98%] [D fake acc: 78.43%] [G acc: 21.57%] \n",
      "14600 [D real acc: 79.03%] [D fake acc: 78.49%] [G acc: 21.51%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 18.80 ch1 4.94 ch2 18.31 ch3 21.95 ch4 14.63 ch5 34.17 Time for epoch 71 is 151.4685022830963 sec\n",
      "14700 [D real acc: 79.09%] [D fake acc: 78.54%] [G acc: 21.46%] \n",
      "14800 [D real acc: 79.15%] [D fake acc: 78.59%] [G acc: 21.41%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 15.75 ch1 3.81 ch2 9.95 ch3 10.15 ch4 31.30 ch5 23.52 Time for epoch 72 is 149.66768670082092 sec\n",
      "14900 [D real acc: 79.21%] [D fake acc: 78.65%] [G acc: 21.35%] \n",
      "15000 [D real acc: 79.26%] [D fake acc: 78.70%] [G acc: 21.30%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 26.49 ch1 6.16 ch2 16.74 ch3 24.59 ch4 31.04 ch5 53.92 Time for epoch 73 is 149.7244222164154 sec\n",
      "15100 [D real acc: 79.32%] [D fake acc: 78.75%] [G acc: 21.25%] \n",
      "15200 [D real acc: 79.36%] [D fake acc: 78.80%] [G acc: 21.20%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 27.07 ch1 1.94 ch2 24.77 ch3 23.69 ch4 43.97 ch5 40.98 Time for epoch 74 is 149.5944471359253 sec\n",
      "15300 [D real acc: 79.41%] [D fake acc: 78.85%] [G acc: 21.15%] \n",
      "15400 [D real acc: 79.46%] [D fake acc: 78.90%] [G acc: 21.10%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 12.34 ch1 4.62 ch2 5.59 ch3 22.48 ch4 7.42 ch5 21.60 Time for epoch 75 is 149.62169289588928 sec\n",
      "15500 [D real acc: 79.51%] [D fake acc: 78.93%] [G acc: 21.07%] \n",
      "15600 [D real acc: 79.55%] [D fake acc: 78.98%] [G acc: 21.02%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 28.98 ch1 14.26 ch2 21.15 ch3 19.30 ch4 31.24 ch5 58.93 Time for epoch 76 is 149.77887535095215 sec\n",
      "15700 [D real acc: 79.59%] [D fake acc: 79.02%] [G acc: 20.98%] \n",
      "15800 [D real acc: 79.64%] [D fake acc: 79.07%] [G acc: 20.93%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 14.38 ch1 7.30 ch2 12.52 ch3 13.60 ch4 17.64 ch5 20.84 Time for epoch 77 is 149.5558545589447 sec\n",
      "15900 [D real acc: 79.69%] [D fake acc: 79.12%] [G acc: 20.88%] \n",
      "16000 [D real acc: 79.73%] [D fake acc: 79.16%] [G acc: 20.84%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 25.14 ch1 5.55 ch2 15.06 ch3 15.85 ch4 50.65 ch5 38.60 Time for epoch 78 is 149.66856336593628 sec\n",
      "16100 [D real acc: 79.77%] [D fake acc: 79.21%] [G acc: 20.79%] \n",
      "16200 [D real acc: 79.82%] [D fake acc: 79.25%] [G acc: 20.75%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 24.21 ch1 5.25 ch2 20.11 ch3 17.36 ch4 44.65 ch5 33.71 Time for epoch 79 is 149.5117371082306 sec\n",
      "16300 [D real acc: 79.86%] [D fake acc: 79.29%] [G acc: 20.71%] \n",
      "16400 [D real acc: 79.91%] [D fake acc: 79.33%] [G acc: 20.67%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 13.50 ch1 4.59 ch2 8.37 ch3 9.39 ch4 12.23 ch5 32.91 Time for epoch 80 is 149.59581995010376 sec\n",
      "16500 [D real acc: 79.95%] [D fake acc: 79.38%] [G acc: 20.62%] \n",
      "16600 [D real acc: 79.99%] [D fake acc: 79.42%] [G acc: 20.58%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 25.34 ch1 8.52 ch2 14.43 ch3 16.77 ch4 46.70 ch5 40.29 Time for epoch 81 is 149.68302583694458 sec\n",
      "16700 [D real acc: 80.03%] [D fake acc: 79.46%] [G acc: 20.54%] \n",
      "16800 [D real acc: 80.07%] [D fake acc: 79.51%] [G acc: 20.49%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 12.38 ch1 3.58 ch2 5.90 ch3 26.31 ch4 5.70 ch5 20.43 Time for epoch 82 is 149.6475806236267 sec\n",
      "16900 [D real acc: 80.12%] [D fake acc: 79.56%] [G acc: 20.44%] \n",
      "17000 [D real acc: 80.16%] [D fake acc: 79.61%] [G acc: 20.39%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 15.86 ch1 4.28 ch2 12.35 ch3 30.50 ch4 15.77 ch5 16.42 Time for epoch 83 is 149.76299381256104 sec\n",
      "17100 [D real acc: 80.21%] [D fake acc: 79.66%] [G acc: 20.34%] \n",
      "17200 [D real acc: 80.26%] [D fake acc: 79.70%] [G acc: 20.30%] \n",
      "17300 [D real acc: 80.30%] [D fake acc: 79.74%] [G acc: 20.26%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 16.02 ch1 8.03 ch2 10.98 ch3 14.80 ch4 13.02 ch5 33.29 Time for epoch 84 is 150.00839591026306 sec\n",
      "17400 [D real acc: 80.34%] [D fake acc: 79.78%] [G acc: 20.22%] \n",
      "17500 [D real acc: 80.38%] [D fake acc: 79.82%] [G acc: 20.18%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 21.80 ch1 11.97 ch2 15.05 ch3 17.72 ch4 39.28 ch5 25.00 Time for epoch 85 is 149.88163781166077 sec\n",
      "17600 [D real acc: 80.42%] [D fake acc: 79.86%] [G acc: 20.14%] \n",
      "17700 [D real acc: 80.45%] [D fake acc: 79.90%] [G acc: 20.10%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 116.57 ch1 4.37 ch2 11.96 ch3 18.97 ch4 44.93 ch5 502.60 Time for epoch 86 is 149.53744912147522 sec\n",
      "17800 [D real acc: 80.50%] [D fake acc: 79.94%] [G acc: 20.06%] \n",
      "17900 [D real acc: 80.53%] [D fake acc: 79.98%] [G acc: 20.02%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 18.27 ch1 4.46 ch2 15.23 ch3 15.49 ch4 25.93 ch5 30.26 Time for epoch 87 is 149.6076922416687 sec\n",
      "18000 [D real acc: 80.57%] [D fake acc: 80.02%] [G acc: 19.98%] \n",
      "18100 [D real acc: 80.61%] [D fake acc: 80.06%] [G acc: 19.94%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 26.34 ch1 7.37 ch2 15.61 ch3 23.12 ch4 42.10 ch5 43.50 Time for epoch 88 is 149.58524250984192 sec\n",
      "18200 [D real acc: 80.65%] [D fake acc: 80.10%] [G acc: 19.90%] \n",
      "18300 [D real acc: 80.68%] [D fake acc: 80.13%] [G acc: 19.87%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 17.31 ch1 3.89 ch2 13.12 ch3 18.05 ch4 20.61 ch5 30.89 Time for epoch 89 is 151.7886037826538 sec\n",
      "18400 [D real acc: 80.72%] [D fake acc: 80.17%] [G acc: 19.83%] \n",
      "18500 [D real acc: 80.76%] [D fake acc: 80.22%] [G acc: 19.78%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 24.30 ch1 11.02 ch2 20.22 ch3 9.52 ch4 27.02 ch5 53.71 Time for epoch 90 is 149.44581079483032 sec\n",
      "18600 [D real acc: 80.80%] [D fake acc: 80.26%] [G acc: 19.74%] \n",
      "18700 [D real acc: 80.85%] [D fake acc: 80.30%] [G acc: 19.70%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 19.65 ch1 7.07 ch2 13.63 ch3 22.49 ch4 25.93 ch5 29.13 Time for epoch 91 is 149.41433238983154 sec\n",
      "18800 [D real acc: 80.89%] [D fake acc: 80.34%] [G acc: 19.66%] \n",
      "18900 [D real acc: 80.93%] [D fake acc: 80.38%] [G acc: 19.62%] \n",
      "206/206 [==============================] - 4s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 14.87 ch1 5.13 ch2 12.70 ch3 18.31 ch4 18.67 ch5 19.52 Time for epoch 92 is 149.61376333236694 sec\n",
      "19000 [D real acc: 80.96%] [D fake acc: 80.42%] [G acc: 19.58%] \n",
      "19100 [D real acc: 81.00%] [D fake acc: 80.46%] [G acc: 19.54%] \n",
      "206/206 [==============================] - 4s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 21.28 ch1 7.87 ch2 21.80 ch3 13.08 ch4 19.99 ch5 43.65 Time for epoch 93 is 150.9684295654297 sec\n",
      "19200 [D real acc: 81.04%] [D fake acc: 80.50%] [G acc: 19.50%] \n",
      "19300 [D real acc: 81.07%] [D fake acc: 80.54%] [G acc: 19.46%] \n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 17ms/step\n",
      "ws mean 28.68 ch1 5.65 ch2 18.20 ch3 36.03 ch4 29.67 ch5 53.83 Time for epoch 94 is 149.58798265457153 sec\n",
      "19400 [D real acc: 81.11%] [D fake acc: 80.58%] [G acc: 19.42%] \n",
      "19500 [D real acc: 81.15%] [D fake acc: 80.62%] [G acc: 19.38%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 12.42 ch1 4.81 ch2 10.65 ch3 9.22 ch4 11.37 ch5 26.05 Time for epoch 95 is 149.31223320960999 sec\n",
      "19600 [D real acc: 81.20%] [D fake acc: 80.66%] [G acc: 19.34%] \n",
      "19700 [D real acc: 81.24%] [D fake acc: 80.71%] [G acc: 19.29%] \n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "206/206 [==============================] - 3s 16ms/step\n",
      "ws mean 29.04 ch1 8.36 ch2 27.28 ch3 37.55 ch4 22.73 ch5 49.25 Time for epoch 96 is 149.13912725448608 sec\n",
      "19800 [D real acc: 81.28%] [D fake acc: 80.75%] [G acc: 19.25%] \n"
     ]
    }
   ],
   "source": [
    "history = train(dataset_with_cond, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('notebook', font_scale = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# total_loss, reconstruction_loss, kl_loss\n",
    "history_losses = np.array([[float(loss) for loss in losses] for losses in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator_loss = history_losses[:,0]\n",
    "disc_loss = history_losses[:,1]\n",
    "discriminator_r_acc = history_losses[:,2]\n",
    "discriminator_f_acc = history_losses[:,3]\n",
    "generator_acc = history_losses[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a4_dims = (14, 5)\n",
    "def print_loss(loss_values, loss_str):\n",
    "    fig, ax = plt.subplots(figsize=a4_dims)\n",
    "    sns.lineplot(loss_values)\n",
    "    plt.xscale('log')\n",
    "    ax.set_title(f\"{loss_str} loss in each epoch\")\n",
    "    ax.set_xlabel(\"Epoch number\")\n",
    "    ax.set_ylabel(f\"{loss_str} loss value\")\n",
    "    # fig.save(f\"../images/{EXPERIMENT_NAME}/{loss_str}_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_loss(generator_loss, \"Generator\")\n",
    "print_loss(disc_loss, \"Discriminator\")\n",
    "print_loss(discriminator_r_acc, \"Discriminator r accuracy\")\n",
    "print_loss(discriminator_f_acc, \"Discriminator f accuracy\")\n",
    "print_loss(generator_acc, \"Generator accuracy\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "5.GAN.ipynb",
   "provenance": [
    {
     "file_id": "1NUVAcZshSKsDKHaeeNNI8ohcUJvqUKqO",
     "timestamp": 1623369938483
    },
    {
     "file_id": "1bthIWyh_69c0sa09VMrRj9mXKOPNu7sP",
     "timestamp": 1623365007022
    },
    {
     "file_id": "1L77q4mrDL6LbE2HURRWfH8vtKm8pUBzK",
     "timestamp": 1620124217177
    },
    {
     "file_id": "1640z4JoZlvsSRGao2KCW322_UwjcQaSp",
     "timestamp": 1620084259696
    },
    {
     "file_id": "1zwFy4NM6SiPYsXCIgVUOEhBXbJ6Rj1tV",
     "timestamp": 1620082260980
    },
    {
     "file_id": "1IbQMqLHIYF-vg6B4Abbobl4pkROWBwes",
     "timestamp": 1620082045291
    },
    {
     "file_id": "1fnbp6zHVBrdi1nCGkrokht8i-pAUfIyf",
     "timestamp": 1620056039573
    },
    {
     "file_id": "1WXvm9ORGBKSJCVApOV4gJIC0NC7OQFzX",
     "timestamp": 1619997366147
    },
    {
     "file_id": "1hYuZL48eIXUGFk2mPXxeUh610OkoOfmV",
     "timestamp": 1619991567406
    },
    {
     "file_id": "1SYhFD0Djg7etn9UB47zsjc948E5VxTFG",
     "timestamp": 1616868089323
    },
    {
     "file_id": "17CrRwBBNZfhpN3y5pk_dTt7PelHqfFSI",
     "timestamp": 1616535329540
    },
    {
     "file_id": "1eTiiJCqFmQvy8KLbd0FSI1WfweHUCz-w",
     "timestamp": 1616531331246
    },
    {
     "file_id": "1Tndd1egGbtLRTO0Hnz6QnI9XiV5nEZKq",
     "timestamp": 1616528772133
    },
    {
     "file_id": "1kTX59Ymn4DGGgjVVQTt7ocE_oy5Sanpx",
     "timestamp": 1616281970086
    },
    {
     "file_id": "1kPO5iwHQPVcMuTc_Tn5JcSIRJ4hNrWjH",
     "timestamp": 1616181812747
    },
    {
     "file_id": "1nkcov8gu5MJRqWxL1awc78jThGTXsTyZ",
     "timestamp": 1616013741366
    },
    {
     "file_id": "13iHBeXIFuSw6-EvChxlqiveFVFurjENx",
     "timestamp": 1615923191272
    },
    {
     "file_id": "1q1UXZxrBrZdwYFvuOtkaRAnqykzUqK1-",
     "timestamp": 1615843147388
    },
    {
     "file_id": "1NUjBm8LUmJ5_Cu3ithC2JLJ4aFAWFEm2",
     "timestamp": 1615585196418
    },
    {
     "file_id": "1Y8oTevpUwSdSV3oCdx_e37mkhaxPzrGf",
     "timestamp": 1615506244659
    }
   ]
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}