{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for running in collab, sagemaker etc.\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/studio-lab-user/Generative_Models_for_CERN_Fast_Simulations/utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 08:40:25.436293: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-28 08:40:25.436358: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-28 08:40:25.436367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "re6ywGTYfdzE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, decomposition, manifold, preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import time\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel as sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_EXPERIMENT_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CtnTQjy0Q05",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9tDJ602Bolmd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (48714, 56, 30) max: 678.0\n",
      "Loaded cond:  (48714, 12) max: 7000.0 min: -7000.0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('../../data/data_proton_photonsum_15_2133.pkl')\n",
    "print('Loaded: ',  data.shape, \"max:\", data.max())\n",
    "\n",
    "# Data containing particle conditional data from particle having responses with proton photon sum in interval [70, 2312] without taking into consideration photon sums of neutron responses.\n",
    "data_cond = pd.read_pickle('../../data/data_cond_photonsum_15_2133_15_3273.pkl')\n",
    "print('Loaded cond: ',  data_cond.shape, \"max:\",data_cond.values.max(), \"min:\",data_cond.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate min max proton sum\n",
    "photon_sum_proton_min, photon_sum_proton_max = data_cond.proton_photon_sum.min(), data_cond.proton_photon_sum.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Energy', 'Vx', 'Vy', 'Vz', 'Px', 'Py', 'Pz', 'mass', 'charge'], dtype='object'),\n",
       " 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond.drop(columns=['proton_photon_sum', 'neutron_photon_sum', 'Pdg'], inplace=True)\n",
    "data_cond.columns, len(data_cond.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>Pz</th>\n",
       "      <th>mass</th>\n",
       "      <th>charge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3192.38</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.182957</td>\n",
       "      <td>-3192.38</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3961.55</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.076487</td>\n",
       "      <td>0.179845</td>\n",
       "      <td>3961.55</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2770.10</td>\n",
       "      <td>1.861170e-17</td>\n",
       "      <td>2.517190e-17</td>\n",
       "      <td>-1.689330e-13</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>0.412760</td>\n",
       "      <td>-2770.10</td>\n",
       "      <td>497.611000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3195.12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>-0.149980</td>\n",
       "      <td>3195.11</td>\n",
       "      <td>938.272081</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1714.07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.457768</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>-1714.07</td>\n",
       "      <td>939.565413</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Energy            Vx            Vy            Vz        Px        Py  \\\n",
       "0  3192.38  0.000000e+00  0.000000e+00  0.000000e+00  0.022422 -0.182957   \n",
       "1  3961.55  0.000000e+00  0.000000e+00  0.000000e+00 -0.076487  0.179845   \n",
       "2  2770.10  1.861170e-17  2.517190e-17 -1.689330e-13  0.305187  0.412760   \n",
       "3  3195.12  0.000000e+00  0.000000e+00  0.000000e+00  0.560528 -0.149980   \n",
       "4  1714.07  0.000000e+00  0.000000e+00  0.000000e+00  0.457768  0.145639   \n",
       "\n",
       "        Pz        mass  charge  \n",
       "0 -3192.38  939.565413     0.0  \n",
       "1  3961.55  938.272081     1.0  \n",
       "2 -2770.10  497.611000     0.0  \n",
       "3  3195.11  938.272081     1.0  \n",
       "4 -1714.07  939.565413     0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment DIR:  experiments/gan_15_2133_28_03_2023_08_40\n"
     ]
    }
   ],
   "source": [
    "DATE_STR = datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "\n",
    "NAME = \"gan\"\n",
    "\n",
    "wandb_run_name = f\"{int(photon_sum_proton_min)}_{int(photon_sum_proton_max)}_{DATE_STR}\"\n",
    "\n",
    "EXPERIMENT_DIR_NAME = f\"experiments/{NAME}_{int(photon_sum_proton_min)}_{int(photon_sum_proton_max)}_{DATE_STR}\"\n",
    "\n",
    "print(\"Experiment DIR: \", EXPERIMENT_DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_scales(model_name, scaler_means, scaler_scales):\n",
    "    out_fnm = f\"{model_name}_scales.txt\"\n",
    "    res = \"#means\"\n",
    "    for mean_ in scaler_means:\n",
    "        res += \"\\n\" + str(mean_)\n",
    "    res += \"\\n\\n#scales\"\n",
    "    for scale_ in scaler_scales:\n",
    "        res += \"\\n\" + str(scale_)\n",
    "        \n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        filepath = f\"../../{EXPERIMENT_DIR_NAME}/scales/\"\n",
    "        create_dir(filepath)\n",
    "        with open(filepath+out_fnm, mode=\"w\") as f:\n",
    "            f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10522,
     "status": "ok",
     "timestamp": 1623609981138,
     "user": {
      "displayName": "Jan Dubiński",
      "photoUrl": "",
      "userId": "04866767089811362617"
     },
     "user_tz": -120
    },
    "id": "g5jISZN7WrvL",
    "outputId": "0cf63b56-f149-4ff0-d3a8-972fd3b7dca3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 6.5206213 min 0.0\n"
     ]
    }
   ],
   "source": [
    "data = np.log(data+1)\n",
    "data = np.float32(data)\n",
    "print(\"data max\", data.max(), \"min\", data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1623609981462,
     "user": {
      "displayName": "Jan Dubiński",
      "photoUrl": "",
      "userId": "04866767089811362617"
     },
     "user_tz": -120
    },
    "id": "45N-b-FGn4CO",
    "outputId": "9110f272-5c82-4a0b-dd31-2234414dfe5a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38971, 56, 30) (9743, 56, 30) (38971, 9) (9743, 9)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, = train_test_split(data, data_cond, test_size=0.2, shuffle=False, random_state=42)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond max 22.267750474560664 min -37.89385322476621\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "# scale cond datascaler = StandardScaler()\n",
    "y_train = scaler.fit_transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "print(\"cond max\", y_train.max(), \"min\", y_train.min())\n",
    "\n",
    "#save scales\n",
    "if SAVE_EXPERIMENT_DATA:\n",
    "    save_scales(\"Proton\", scaler.mean_, scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9FMxwgNpn-CU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size=128)\n",
    "dataset_cond = tf.data.Dataset.from_tensor_slices(y_train).batch(batch_size=128)\n",
    "fake_cond =  tf.data.Dataset.from_tensor_slices(y_train).shuffle(12800).batch(batch_size=128)\n",
    "dataset_with_cond = tf.data.Dataset.zip((dataset,dataset_cond, fake_cond)).shuffle(12800)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size=128)\n",
    "val_dataset_cond = tf.data.Dataset.from_tensor_slices(y_test).batch(batch_size=128)\n",
    "val_fake_cond =  tf.data.Dataset.from_tensor_slices(y_test).shuffle(12800).batch(batch_size=128)\n",
    "val_dataset_with_cond = tf.data.Dataset.zip((val_dataset,val_dataset_cond,val_fake_cond)).shuffle(12800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41Ri3GB8n7oI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iFhM4_mYfdzJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 19)           0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          5120        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256)         1024        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 256)          0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096)         1052672     ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 4096)        16384       ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096)         0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 4096)         0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 8, 4, 128)    0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 16, 8, 128)   0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 16, 8, 128)   147584      ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16, 8, 128)  512         ['conv2d[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 16, 8, 128)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 16, 8, 128)   0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 64, 32, 128)  0          ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 58, 32, 64)   57408       ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 58, 32, 64)  256         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 58, 32, 64)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 58, 32, 64)   0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 56, 30, 1)    577         ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,281,537\n",
      "Trainable params: 1,272,449\n",
      "Non-trainable params: 9,088\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 56, 30, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 54, 28, 32)   320         ['input_img[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 54, 28, 32)  128         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 54, 28, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 54, 28, 32)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 27, 14, 32)   0           ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 25, 12, 16)   4624        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 25, 12, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 25, 12, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 25, 12, 16)   0           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 16)  0           ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2304)         0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2313)         0           ['flatten[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          296192      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 128)          0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 128)          0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8256        ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64)          256         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 64)           0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            65          ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,417\n",
      "Trainable params: 309,937\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 10\n",
    "cond_dim = 9\n",
    "poz_dim = 6\n",
    "\n",
    "############################ generator ############################\n",
    "\n",
    "x = Input(shape=(latent_dim,))\n",
    "cond = Input(shape=(cond_dim,))\n",
    "inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "layer_1 = Dense(128*2)(inputs)\n",
    "layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "layer_2 = Dense(128*8*4)(layer_1_a)\n",
    "layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "reshaped = Reshape((8,4,128))(layer_2_a)\n",
    "reshaped_s = UpSampling2D(size=(2,2))(reshaped)\n",
    "\n",
    "conv1 = Conv2D(128, kernel_size=3, padding='same')(reshaped_s)\n",
    "conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "conv1_a_s = UpSampling2D(size=(4,4))(conv1_a)\n",
    "\n",
    "conv2 = Conv2D(64, kernel_size=(7, 1))(conv1_a_s)\n",
    "conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "outputs = Conv2D(1, kernel_size=3, activation='relu')(conv2_a)\n",
    "\n",
    "generator = Model([x, cond], outputs, name='generator')\n",
    "generator.summary()\n",
    "\n",
    "############################ discriminator ############################\n",
    "\n",
    "input_img = Input(shape=[56,30,1],name='input_img')\n",
    "conv1 = Conv2D(32, kernel_size=3)(input_img)\n",
    "conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "conv2 = Conv2D(16, kernel_size=3)(pool1)\n",
    "conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 1))(conv2_a)\n",
    "flat = Flatten()(pool2)\n",
    "cond = Input(shape=(cond_dim,))\n",
    "inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "layer_1 = Dense(128)(inputs2)\n",
    "layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "layer_2 = Dense(64)(layer_1_a)\n",
    "layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "outputs = Dense(1, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "discriminator = Model([input_img, cond], outputs, name='discriminator')\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1623609985345,
     "user": {
      "displayName": "Jan Dubiński",
      "photoUrl": "",
      "userId": "04866767089811362617"
     },
     "user_tz": -120
    },
    "id": "UVXZMLDUfdzJ",
    "outputId": "b6cd2a13-46f7-4c7b-d9b6-e6eb0dec7757",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# latent_dim = 10\n",
    "# cond_dim = 10\n",
    "# poz_dim = 6\n",
    "\n",
    "# ############################ generator ############################\n",
    "\n",
    "# x = Input(shape=(latent_dim,))\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "# inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "# layer_1 = Dense(128*2)(inputs)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "\n",
    "# layer_2 = Dense(128*28*15)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "\n",
    "# reshaped = Reshape((28, 15, 128))(layer_2_a)\n",
    "# # reshaped_s = UpSampling2D()(reshaped)\n",
    "\n",
    "# conv1 = Conv2D(128, kernel_size=1)(reshaped)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# conv1_a_s = UpSampling2D()(conv1_a)\n",
    "\n",
    "# conv2 = Conv2D(64, kernel_size=1)(conv1_a_s)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "\n",
    "# outputs = Conv2D(1, kernel_size=1,activation='relu')(conv2_a)\n",
    "\n",
    "# generator = Model([x, cond], outputs, name='generator')\n",
    "# generator.summary()\n",
    "\n",
    "# ############################ discriminator ############################\n",
    "\n",
    "# input_img = Input(shape=[56,30,1],name='input_img')\n",
    "# conv1 = Conv2D(32, kernel_size=3)(input_img)\n",
    "# conv1_bd = Dropout(0.2)(BatchNormalization()(conv1))\n",
    "# conv1_a = LeakyReLU(alpha=0.1)(conv1_bd)\n",
    "# pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
    "# conv2 = Conv2D(16, kernel_size=3)(pool1)\n",
    "# conv2_bd = Dropout(0.2)(BatchNormalization()(conv2))\n",
    "# conv2_a = LeakyReLU(alpha=0.1)(conv2_bd)\n",
    "# pool2 = MaxPooling2D(pool_size=(2, 2))(conv2_a)\n",
    "# flat = Flatten()(pool2)\n",
    "# cond = Input(shape=(cond_dim,))\n",
    "\n",
    "# inputs2 = Concatenate(axis=1)([flat, cond])\n",
    "# layer_1 = Dense(128)(inputs2)\n",
    "# layer_1_bd = Dropout(0.2)(BatchNormalization()(layer_1))\n",
    "# layer_1_a = LeakyReLU(alpha=0.1)(layer_1_bd)\n",
    "# layer_2 = Dense(64)(layer_1_a)\n",
    "# layer_2_bd = Dropout(0.2)(BatchNormalization()(layer_2))\n",
    "# layer_2_a = LeakyReLU(alpha=0.1)(layer_2_bd)\n",
    "# outputs = Dense(1, activation='sigmoid')(layer_2_a)\n",
    "\n",
    "# discriminator = Model([input_img, cond], outputs, name='discriminator')\n",
    "# discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "87NnkVJwaCOo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    d_acc_r.update_state(tf.ones_like(real_output), real_output)\n",
    "    d_acc_f.update_state(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss, fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZXwATQ9uaigO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HqTYRo-uki5k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "d_acc_r = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "d_acc_f = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "g_acc = keras.metrics.BinaryAccuracy(name=\"g_acc_g\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yjX97hnkkmlf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(step, fake_output):\n",
    "    g_acc.update_state(tf.ones_like(fake_output), fake_output)\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)# - div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KhuiPsi6koZY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "I4HsHLgwkurp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "noise_dim = 10\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "START_GENERATING_IMG_FROM_IDX = 20\n",
    "# Seed to reuse for generating samples for comparison during training\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed_cond = y_test[START_GENERATING_IMG_FROM_IDX:START_GENERATING_IMG_FROM_IDX+num_examples_to_generate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbedkowski-patrick\u001b[0m (\u001b[33mgenerative-models-cern\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/studio-lab-user/Generative_Models_for_CERN_Fast_Simulations/notebooks/gan_proton_15_2312/wandb/run-20230328_084031-ub29thym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/generative-models-cern/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/ub29thym' target=\"_blank\">15_2133_28_03_2023_08_40</a></strong> to <a href='https://wandb.ai/generative-models-cern/Generative%20Models%20for%20CERN%20Fast%20Simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/generative-models-cern/Generative%20Models%20for%20CERN%20Fast%20Simulations' target=\"_blank\">https://wandb.ai/generative-models-cern/Generative%20Models%20for%20CERN%20Fast%20Simulations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/generative-models-cern/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/ub29thym' target=\"_blank\">https://wandb.ai/generative-models-cern/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/ub29thym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/generative-models-cern/Generative%20Models%20for%20CERN%20Fast%20Simulations/runs/ub29thym?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7bc9f3d820>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Generative Models for CERN Fast Simulations\",\n",
    "    name=wandb_run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"Model\": NAME,\n",
    "    \"dataset\": \"proton_data\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"Date\": DATE_STR,\n",
    "    \"Proton_min\": photon_sum_proton_min,\n",
    "    \"Proton_max\": photon_sum_proton_max,\n",
    "    \"Experiment_dir_name\": EXPERIMENT_DIR_NAME\n",
    "    },\n",
    "    tags=[f\"proton_min_{photon_sum_proton_min}\", f\"proton_max_{photon_sum_proton_max}\", \"gan\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rMxBrHhsTDXO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils import sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "org=np.exp(x_test)-1\n",
    "ch_org = np.array(org).reshape(-1,56,30)\n",
    "ch_org = pd.DataFrame(sum_channels_parallel(ch_org)).values\n",
    "del org\n",
    "\n",
    "\n",
    "def calculate_ws_ch(n_calc):\n",
    "    ws= [0,0,0,0,0]\n",
    "    for j in range(n_calc):\n",
    "        z = np.random.normal(0,1,(x_test.shape[0],10))\n",
    "        z_c = y_test\n",
    "        results = generator.predict([z,z_c])\n",
    "        results = np.exp(results)-1\n",
    "        try:\n",
    "            ch_gen = np.array(results).reshape(-1,56,30)\n",
    "            ch_gen = pd.DataFrame(sum_channels_parallel(ch_gen)).values\n",
    "            for i in range(5):\n",
    "                ws[i] = ws[i] + wasserstein_distance(ch_org[:,i], ch_gen[:,i])\n",
    "            ws =np.array(ws)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            \n",
    "    ws = ws/n_calc\n",
    "    ws_mean = ws.sum()/5\n",
    "    print(\"ws mean\",f'{ws_mean:.2f}', end=\" \")\n",
    "    for n, score in enumerate(ws):\n",
    "        print(\"ch\"+str(n+1),f'{score:.2f}',end=\" \")\n",
    "    return ws_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GmlMSiqCku5_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch,step):\n",
    "    images, cond, noise_cond = batch\n",
    "    step=step\n",
    "    BATCH_SIZE = tf.shape(images)[0]\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator([noise,noise_cond], training=True)\n",
    "\n",
    "        real_output = discriminator([images,cond], training=True)\n",
    "        fake_output = discriminator([generated_images, noise_cond], training=True)\n",
    "\n",
    "        gen_loss = generator_loss(step, fake_output)\n",
    "        real_loss, fake_loss = discriminator_loss(real_output, fake_output)\n",
    "        disc_loss = real_loss + fake_loss\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, real_loss, fake_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "D-wATS0PkvJo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_EXPERIMENT_DATA:\n",
    "    filepath_mod = f\"../../{EXPERIMENT_DIR_NAME}/models/\"\n",
    "    create_dir(filepath_mod)\n",
    "\n",
    "history = []\n",
    "def train(dataset, epochs):\n",
    "    experiment_start = time.time()\n",
    "    tf_step = tf.Variable(0, dtype=float)\n",
    "    step=0\n",
    "\n",
    "    # generate first image\n",
    "    generate_and_save_images(generator,\n",
    "                             epochs,\n",
    "                             [seed, seed_cond])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        gen_loss_epoch = []\n",
    "        disc_real_loss_epoch = []\n",
    "        disc_fake_loss_epoch = []\n",
    "        for batch in dataset:\n",
    "            gen_loss, disc_real_loss, disc_fake_loss = train_step(batch,tf_step)\n",
    "            disc_loss = disc_real_loss + disc_fake_loss\n",
    "            \n",
    "            history.append([gen_loss,disc_loss,\n",
    "                100*d_acc_r.result().numpy(),\n",
    "                100*d_acc_f.result().numpy(),\n",
    "                100*g_acc.result().numpy(),\n",
    "                ])\n",
    "            tf_step.assign_add(1)\n",
    "            step = step+1\n",
    "            \n",
    "            gen_loss_epoch.append(gen_loss)\n",
    "            disc_real_loss_epoch.append(disc_real_loss)\n",
    "            disc_fake_loss_epoch.append(disc_fake_loss)\n",
    "            if step % 100 == 0:\n",
    "                print(\"%d [D real acc: %.2f%%] [D fake acc: %.2f%%] [G acc: %.2f%%] \"% (\n",
    "                    step,\n",
    "                    100*d_acc_r.result().numpy(),\n",
    "                    100*d_acc_f.result().numpy(),\n",
    "                    100*g_acc.result().numpy()))\n",
    "\n",
    "        plot = generate_and_save_images(generator,\n",
    "                                 epoch,\n",
    "                                 [seed, seed_cond])\n",
    "        \n",
    "        if SAVE_EXPERIMENT_DATA:\n",
    "            # Save the model every epoch\n",
    "            generator.compile()\n",
    "            discriminator.compile()\n",
    "            generator.save((os.path.join(filepath_mod, \"gen_\"+NAME + \"_\"+ str(epoch) +\".h5\")))\n",
    "            discriminator.save((os.path.join(filepath_mod, \"disc_\"+NAME + \"_\"+ str(epoch) +\".h5\")))\n",
    "            np.savez(os.path.join(filepath_mod, \"history_\"+NAME+\".npz\"),np.array(history))\n",
    "\n",
    "        ws_mean = calculate_ws_ch(min(epoch//5+1,5))\n",
    "        \n",
    "        wandb.log({\n",
    "            'ws_mean': ws_mean,\n",
    "            'gen_loss': np.mean(gen_loss_epoch),\n",
    "            'disc_real_loss': np.mean(disc_real_loss_epoch),\n",
    "            'disc_fake_loss': np.mean(disc_fake_loss_epoch),\n",
    "            'epoch': epoch,\n",
    "            'plot': wandb.Image(plot),\n",
    "            'experiment_time': time.time()-experiment_start\n",
    "        })\n",
    "\n",
    "        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CxeGwn7ek8Q-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_EXPERIMENT_DATA:\n",
    "    filepath_img = f\"../images/{EXPERIMENT_DIR_NAME}/\"\n",
    "    create_dir(filepath_img)\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    \n",
    "    SUPTITLE_TXT = f\"\\nModel: GAN proton data\" \\\n",
    "               f\"\\nPhotonsum interval: [{photon_sum_proton_min}, {photon_sum_proton_max}]\" \\\n",
    "               f\"\\nEPOCH: {epoch}\"\n",
    "    \n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)  # returns 16 responses\n",
    "\n",
    "    fig, axs = plt.subplots(2, 7, figsize=(15, 5))\n",
    "    fig.suptitle(SUPTITLE_TXT, x=0.1, horizontalalignment='left')\n",
    "\n",
    "    for i in range(0, 14):\n",
    "        if i < 7:\n",
    "            x = x_test[20 + i].reshape(56, 30)\n",
    "        else:\n",
    "            x = predictions[i - 7].numpy().reshape(56, 30)\n",
    "        #x[x<=0]=x.max()*-0.1\n",
    "        im = axs[i // 7, i % 7].imshow(x, cmap='gnuplot')\n",
    "        axs[i // 7, i % 7].axis('off')\n",
    "        fig.colorbar(im, ax=axs[i // 7, i % 7])\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.975])\n",
    "    if SAVE_EXPERIMENT_DATA:\n",
    "        plt.savefig(os.path.join(filepath_img, 'image_at_epoch_{:04d}.png'.format(epoch)))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmR61h2W0vxC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D real acc: 51.17%] [D fake acc: 59.76%] [G acc: 40.24%] \n",
      "200 [D real acc: 59.44%] [D fake acc: 56.53%] [G acc: 43.47%] \n",
      "300 [D real acc: 59.42%] [D fake acc: 54.01%] [G acc: 45.99%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 140.66 ch1 0.05 ch2 10.80 ch3 0.27 ch4 289.06 ch5 403.10 Time for epoch 1 is 38.54326605796814 sec\n",
      "400 [D real acc: 59.59%] [D fake acc: 54.01%] [G acc: 45.99%] \n",
      "500 [D real acc: 60.33%] [D fake acc: 54.41%] [G acc: 45.59%] \n",
      "600 [D real acc: 61.46%] [D fake acc: 55.60%] [G acc: 44.40%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 74.66 ch1 0.19 ch2 40.20 ch3 0.51 ch4 150.80 ch5 181.59 Time for epoch 2 is 26.993642568588257 sec\n",
      "700 [D real acc: 62.61%] [D fake acc: 57.46%] [G acc: 42.54%] \n",
      "800 [D real acc: 63.71%] [D fake acc: 58.86%] [G acc: 41.14%] \n",
      "900 [D real acc: 64.02%] [D fake acc: 59.41%] [G acc: 40.59%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 24.52 ch1 0.02 ch2 6.66 ch3 0.20 ch4 54.52 ch5 61.17 Time for epoch 3 is 27.370929956436157 sec\n",
      "1000 [D real acc: 64.02%] [D fake acc: 59.67%] [G acc: 40.33%] \n",
      "1100 [D real acc: 63.74%] [D fake acc: 59.99%] [G acc: 40.01%] \n",
      "1200 [D real acc: 63.56%] [D fake acc: 60.15%] [G acc: 39.85%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 22.10 ch1 0.03 ch2 11.72 ch3 0.18 ch4 44.79 ch5 53.76 Time for epoch 4 is 27.22740387916565 sec\n",
      "1300 [D real acc: 63.60%] [D fake acc: 60.47%] [G acc: 39.53%] \n",
      "1400 [D real acc: 63.74%] [D fake acc: 60.63%] [G acc: 39.37%] \n",
      "1500 [D real acc: 63.78%] [D fake acc: 60.76%] [G acc: 39.24%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 26.93 ch1 0.03 ch2 11.30 ch3 0.21 ch4 56.01 ch5 67.08 Time for epoch 5 is 27.326308250427246 sec\n",
      "1600 [D real acc: 63.94%] [D fake acc: 60.89%] [G acc: 39.11%] \n",
      "1700 [D real acc: 64.17%] [D fake acc: 61.00%] [G acc: 39.00%] \n",
      "1800 [D real acc: 64.28%] [D fake acc: 61.29%] [G acc: 38.71%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 22.91 ch1 0.03 ch2 10.37 ch3 0.22 ch4 46.82 ch5 57.10 Time for epoch 6 is 30.07285165786743 sec\n",
      "1900 [D real acc: 64.50%] [D fake acc: 61.54%] [G acc: 38.46%] \n",
      "2000 [D real acc: 64.69%] [D fake acc: 61.73%] [G acc: 38.27%] \n",
      "2100 [D real acc: 64.92%] [D fake acc: 62.06%] [G acc: 37.94%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 19.07 ch1 0.02 ch2 9.73 ch3 0.09 ch4 39.53 ch5 45.96 Time for epoch 7 is 29.865745067596436 sec\n",
      "2200 [D real acc: 65.22%] [D fake acc: 62.38%] [G acc: 37.62%] \n",
      "2300 [D real acc: 65.55%] [D fake acc: 62.68%] [G acc: 37.32%] \n",
      "2400 [D real acc: 65.91%] [D fake acc: 62.99%] [G acc: 37.01%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 27.86 ch1 0.03 ch2 11.06 ch3 0.16 ch4 62.54 ch5 65.50 Time for epoch 8 is 29.859535694122314 sec\n",
      "2500 [D real acc: 66.28%] [D fake acc: 63.33%] [G acc: 36.67%] \n",
      "2600 [D real acc: 66.66%] [D fake acc: 63.72%] [G acc: 36.28%] \n",
      "2700 [D real acc: 67.11%] [D fake acc: 64.17%] [G acc: 35.83%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 36.47 ch1 0.04 ch2 11.11 ch3 0.24 ch4 82.19 ch5 88.75 Time for epoch 9 is 30.32846212387085 sec\n",
      "2800 [D real acc: 67.59%] [D fake acc: 64.67%] [G acc: 35.33%] \n",
      "2900 [D real acc: 68.06%] [D fake acc: 65.11%] [G acc: 34.89%] \n",
      "3000 [D real acc: 68.56%] [D fake acc: 65.58%] [G acc: 34.42%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 61.32 ch1 0.03 ch2 9.68 ch3 0.23 ch4 145.31 ch5 151.36 Time for epoch 10 is 29.995425701141357 sec\n",
      "3100 [D real acc: 69.06%] [D fake acc: 66.03%] [G acc: 33.97%] \n",
      "3200 [D real acc: 69.61%] [D fake acc: 66.49%] [G acc: 33.51%] \n",
      "3300 [D real acc: 70.07%] [D fake acc: 67.02%] [G acc: 32.98%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 48.14 ch1 0.03 ch2 9.07 ch3 0.17 ch4 111.32 ch5 120.12 Time for epoch 11 is 32.44087052345276 sec\n",
      "3400 [D real acc: 70.60%] [D fake acc: 67.56%] [G acc: 32.44%] \n",
      "3500 [D real acc: 71.10%] [D fake acc: 68.08%] [G acc: 31.92%] \n",
      "3600 [D real acc: 71.64%] [D fake acc: 68.54%] [G acc: 31.46%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 161.49 ch1 0.03 ch2 9.40 ch3 0.58 ch4 397.06 ch5 400.36 Time for epoch 12 is 32.896063566207886 sec\n",
      "3700 [D real acc: 72.16%] [D fake acc: 69.02%] [G acc: 30.98%] \n",
      "3800 [D real acc: 72.63%] [D fake acc: 69.56%] [G acc: 30.44%] \n",
      "3900 [D real acc: 73.11%] [D fake acc: 70.07%] [G acc: 29.93%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 435.11 ch1 0.03 ch2 9.19 ch3 1.17 ch4 1070.91 ch5 1094.27 Time for epoch 13 is 32.31853675842285 sec\n",
      "4000 [D real acc: 73.58%] [D fake acc: 70.55%] [G acc: 29.45%] \n",
      "4100 [D real acc: 74.05%] [D fake acc: 71.05%] [G acc: 28.95%] \n",
      "4200 [D real acc: 74.52%] [D fake acc: 71.53%] [G acc: 28.47%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 83.05 ch1 0.03 ch2 8.25 ch3 0.22 ch4 200.60 ch5 206.18 Time for epoch 14 is 32.4805862903595 sec\n",
      "4300 [D real acc: 74.98%] [D fake acc: 72.00%] [G acc: 28.00%] \n",
      "4400 [D real acc: 75.41%] [D fake acc: 72.46%] [G acc: 27.54%] \n",
      "4500 [D real acc: 75.85%] [D fake acc: 72.93%] [G acc: 27.07%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 103.87 ch1 0.03 ch2 9.49 ch3 0.50 ch4 251.70 ch5 257.61 Time for epoch 15 is 32.43073129653931 sec\n",
      "4600 [D real acc: 76.26%] [D fake acc: 73.32%] [G acc: 26.68%] \n",
      "4700 [D real acc: 76.68%] [D fake acc: 73.58%] [G acc: 26.42%] \n",
      "4800 [D real acc: 77.08%] [D fake acc: 73.78%] [G acc: 26.22%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 521.14 ch1 0.04 ch2 10.99 ch3 3.65 ch4 1289.99 ch5 1301.04 Time for epoch 16 is 35.33412432670593 sec\n",
      "4900 [D real acc: 77.37%] [D fake acc: 74.09%] [G acc: 25.91%] \n",
      "5000 [D real acc: 77.73%] [D fake acc: 74.40%] [G acc: 25.60%] \n",
      "5100 [D real acc: 78.12%] [D fake acc: 74.77%] [G acc: 25.23%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 8762.69 ch1 0.72 ch2 179.12 ch3 65.03 ch4 21651.72 ch5 21916.84 Time for epoch 17 is 34.966432332992554 sec\n",
      "5200 [D real acc: 78.47%] [D fake acc: 75.11%] [G acc: 24.89%] \n",
      "5300 [D real acc: 78.79%] [D fake acc: 75.51%] [G acc: 24.49%] \n",
      "5400 [D real acc: 79.12%] [D fake acc: 75.90%] [G acc: 24.10%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 29.88 ch1 0.03 ch2 9.93 ch3 0.26 ch4 64.85 ch5 74.32 Time for epoch 18 is 34.973963022232056 sec\n",
      "5500 [D real acc: 79.47%] [D fake acc: 76.26%] [G acc: 23.74%] \n",
      "5600 [D real acc: 79.77%] [D fake acc: 76.61%] [G acc: 23.39%] \n",
      "5700 [D real acc: 80.09%] [D fake acc: 76.99%] [G acc: 23.01%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 51.90 ch1 0.04 ch2 13.24 ch3 0.41 ch4 116.23 ch5 129.58 Time for epoch 19 is 34.92915916442871 sec\n",
      "5800 [D real acc: 80.40%] [D fake acc: 77.36%] [G acc: 22.64%] \n",
      "5900 [D real acc: 80.69%] [D fake acc: 77.72%] [G acc: 22.28%] \n",
      "6000 [D real acc: 80.99%] [D fake acc: 78.08%] [G acc: 21.92%] \n",
      "6100 [D real acc: 81.28%] [D fake acc: 78.44%] [G acc: 21.56%] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_610/1514722627.py:15: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, axs = plt.subplots(2, 7, figsize=(15, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 68.86 ch1 0.04 ch2 12.97 ch3 0.40 ch4 160.81 ch5 170.09 Time for epoch 20 is 35.083263874053955 sec\n",
      "6200 [D real acc: 81.56%] [D fake acc: 78.78%] [G acc: 21.22%] \n",
      "6300 [D real acc: 81.84%] [D fake acc: 79.11%] [G acc: 20.89%] \n",
      "6400 [D real acc: 82.11%] [D fake acc: 79.40%] [G acc: 20.60%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 51.75 ch1 0.04 ch2 12.87 ch3 0.41 ch4 116.06 ch5 129.37 Time for epoch 21 is 37.906517028808594 sec\n",
      "6500 [D real acc: 82.37%] [D fake acc: 79.58%] [G acc: 20.42%] \n",
      "6600 [D real acc: 82.61%] [D fake acc: 79.80%] [G acc: 20.20%] \n",
      "6700 [D real acc: 82.86%] [D fake acc: 80.04%] [G acc: 19.96%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 50.14 ch1 0.04 ch2 12.81 ch3 0.40 ch4 112.24 ch5 125.19 Time for epoch 22 is 37.109697580337524 sec\n",
      "6800 [D real acc: 83.09%] [D fake acc: 80.27%] [G acc: 19.73%] \n",
      "6900 [D real acc: 83.32%] [D fake acc: 80.51%] [G acc: 19.49%] \n",
      "7000 [D real acc: 83.53%] [D fake acc: 80.75%] [G acc: 19.25%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 33.59 ch1 0.03 ch2 9.90 ch3 0.30 ch4 76.81 ch5 80.93 Time for epoch 23 is 37.169893980026245 sec\n",
      "7100 [D real acc: 83.76%] [D fake acc: 80.98%] [G acc: 19.02%] \n",
      "7200 [D real acc: 83.96%] [D fake acc: 81.20%] [G acc: 18.80%] \n",
      "7300 [D real acc: 84.16%] [D fake acc: 81.41%] [G acc: 18.59%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 42.52 ch1 0.03 ch2 8.30 ch3 0.33 ch4 96.38 ch5 107.56 Time for epoch 24 is 37.30118680000305 sec\n",
      "7400 [D real acc: 84.36%] [D fake acc: 81.58%] [G acc: 18.42%] \n",
      "7500 [D real acc: 84.56%] [D fake acc: 81.77%] [G acc: 18.23%] \n",
      "7600 [D real acc: 84.72%] [D fake acc: 81.96%] [G acc: 18.04%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 35.24 ch1 0.04 ch2 8.30 ch3 0.31 ch4 78.73 ch5 88.82 Time for epoch 25 is 37.16094493865967 sec\n",
      "7700 [D real acc: 84.90%] [D fake acc: 82.18%] [G acc: 17.82%] \n",
      "7800 [D real acc: 85.06%] [D fake acc: 82.41%] [G acc: 17.59%] \n",
      "7900 [D real acc: 85.23%] [D fake acc: 82.62%] [G acc: 17.38%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 109.46 ch1 0.04 ch2 18.99 ch3 0.33 ch4 263.26 ch5 264.67 Time for epoch 26 is 37.933419704437256 sec\n",
      "8000 [D real acc: 85.39%] [D fake acc: 82.82%] [G acc: 17.18%] \n",
      "8100 [D real acc: 85.53%] [D fake acc: 83.00%] [G acc: 17.00%] \n",
      "8200 [D real acc: 85.69%] [D fake acc: 83.15%] [G acc: 16.85%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 37.01 ch1 0.02 ch2 7.33 ch3 0.30 ch4 83.57 ch5 93.84 Time for epoch 27 is 37.35180616378784 sec\n",
      "8300 [D real acc: 85.85%] [D fake acc: 83.28%] [G acc: 16.72%] \n",
      "8400 [D real acc: 86.00%] [D fake acc: 83.41%] [G acc: 16.59%] \n",
      "8500 [D real acc: 86.16%] [D fake acc: 83.56%] [G acc: 16.44%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 405.44 ch1 0.08 ch2 22.85 ch3 2.70 ch4 988.66 ch5 1012.89 Time for epoch 28 is 37.285035133361816 sec\n",
      "8600 [D real acc: 86.30%] [D fake acc: 83.72%] [G acc: 16.28%] \n",
      "8700 [D real acc: 86.45%] [D fake acc: 83.87%] [G acc: 16.13%] \n",
      "8800 [D real acc: 86.59%] [D fake acc: 84.02%] [G acc: 15.98%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 787.28 ch1 0.23 ch2 88.87 ch3 3.25 ch4 1943.86 ch5 1900.19 Time for epoch 29 is 37.34563374519348 sec\n",
      "8900 [D real acc: 86.73%] [D fake acc: 84.16%] [G acc: 15.84%] \n",
      "9000 [D real acc: 86.87%] [D fake acc: 84.30%] [G acc: 15.70%] \n",
      "9100 [D real acc: 87.01%] [D fake acc: 84.43%] [G acc: 15.57%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 100269523.17 ch1 2.86 ch2 246691008.20 ch3 1.72 ch4 2973.65 ch5 254653629.45 Time for epoch 30 is 37.47949934005737 sec\n",
      "9200 [D real acc: 87.12%] [D fake acc: 84.57%] [G acc: 15.43%] \n",
      "9300 [D real acc: 87.24%] [D fake acc: 84.72%] [G acc: 15.28%] \n",
      "9400 [D real acc: 87.37%] [D fake acc: 84.87%] [G acc: 15.13%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 1665029.24 ch1 2.07 ch2 7760.25 ch3 8.95 ch4 3187004.54 ch5 5130370.38 Time for epoch 31 is 37.41076064109802 sec\n",
      "9500 [D real acc: 87.50%] [D fake acc: 85.00%] [G acc: 15.00%] \n",
      "9600 [D real acc: 87.63%] [D fake acc: 85.14%] [G acc: 14.86%] \n",
      "9700 [D real acc: 87.74%] [D fake acc: 85.27%] [G acc: 14.73%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 3999.24 ch1 3.25 ch2 1115.20 ch3 17.46 ch4 8833.86 ch5 10026.46 Time for epoch 32 is 38.271167278289795 sec\n",
      "9800 [D real acc: 87.86%] [D fake acc: 85.40%] [G acc: 14.60%] \n",
      "9900 [D real acc: 87.98%] [D fake acc: 85.53%] [G acc: 14.47%] \n",
      "10000 [D real acc: 88.09%] [D fake acc: 85.66%] [G acc: 14.34%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 3604317986.26 ch1 20.46 ch2 6304.58 ch3 78.51 ch4 2511294122.30 ch5 15510289405.44 Time for epoch 33 is 37.54555344581604 sec\n",
      "10100 [D real acc: 88.20%] [D fake acc: 85.79%] [G acc: 14.21%] \n",
      "10200 [D real acc: 88.31%] [D fake acc: 85.92%] [G acc: 14.08%] \n",
      "10300 [D real acc: 88.42%] [D fake acc: 86.05%] [G acc: 13.95%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 774083.58 ch1 1.84 ch2 648.55 ch3 23.54 ch4 1850421.38 ch5 2019322.59 Time for epoch 34 is 37.64680480957031 sec\n",
      "10400 [D real acc: 88.52%] [D fake acc: 86.17%] [G acc: 13.83%] \n",
      "10500 [D real acc: 88.62%] [D fake acc: 86.29%] [G acc: 13.71%] \n",
      "10600 [D real acc: 88.73%] [D fake acc: 86.41%] [G acc: 13.59%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 455.49 ch1 2.69 ch2 956.69 ch3 0.43 ch4 179.91 ch5 1137.72 Time for epoch 35 is 37.393147230148315 sec\n",
      "10700 [D real acc: 88.83%] [D fake acc: 86.52%] [G acc: 13.48%] \n",
      "10800 [D real acc: 88.93%] [D fake acc: 86.64%] [G acc: 13.36%] \n",
      "10900 [D real acc: 89.03%] [D fake acc: 86.76%] [G acc: 13.24%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 51.29 ch1 0.03 ch2 12.31 ch3 0.41 ch4 115.95 ch5 127.73 Time for epoch 36 is 37.46246552467346 sec\n",
      "11000 [D real acc: 89.11%] [D fake acc: 86.86%] [G acc: 13.14%] \n",
      "11100 [D real acc: 89.20%] [D fake acc: 86.98%] [G acc: 13.02%] \n",
      "11200 [D real acc: 89.29%] [D fake acc: 87.09%] [G acc: 12.91%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 51.58 ch1 0.04 ch2 11.73 ch3 0.41 ch4 117.08 ch5 128.64 Time for epoch 37 is 37.38857626914978 sec\n",
      "11300 [D real acc: 89.38%] [D fake acc: 87.20%] [G acc: 12.80%] \n",
      "11400 [D real acc: 89.47%] [D fake acc: 87.32%] [G acc: 12.68%] \n",
      "11500 [D real acc: 89.56%] [D fake acc: 87.42%] [G acc: 12.58%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 51.92 ch1 0.04 ch2 12.70 ch3 0.41 ch4 116.88 ch5 129.56 Time for epoch 38 is 38.266772985458374 sec\n",
      "11600 [D real acc: 89.65%] [D fake acc: 87.53%] [G acc: 12.47%] \n",
      "11700 [D real acc: 89.73%] [D fake acc: 87.64%] [G acc: 12.36%] \n",
      "11800 [D real acc: 89.82%] [D fake acc: 87.74%] [G acc: 12.26%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "ws mean 54.20 ch1 0.04 ch2 18.48 ch3 0.41 ch4 116.03 ch5 136.02 Time for epoch 39 is 37.389302015304565 sec\n",
      "11900 [D real acc: 89.90%] [D fake acc: 87.84%] [G acc: 12.16%] \n",
      "12000 [D real acc: 89.98%] [D fake acc: 87.94%] [G acc: 12.06%] \n",
      "12100 [D real acc: 90.06%] [D fake acc: 88.00%] [G acc: 12.00%] \n",
      "12200 [D real acc: 90.14%] [D fake acc: 88.07%] [G acc: 11.93%] \n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "history = train(dataset_with_cond, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "5.GAN.ipynb",
   "provenance": [
    {
     "file_id": "1NUVAcZshSKsDKHaeeNNI8ohcUJvqUKqO",
     "timestamp": 1623369938483
    },
    {
     "file_id": "1bthIWyh_69c0sa09VMrRj9mXKOPNu7sP",
     "timestamp": 1623365007022
    },
    {
     "file_id": "1L77q4mrDL6LbE2HURRWfH8vtKm8pUBzK",
     "timestamp": 1620124217177
    },
    {
     "file_id": "1640z4JoZlvsSRGao2KCW322_UwjcQaSp",
     "timestamp": 1620084259696
    },
    {
     "file_id": "1zwFy4NM6SiPYsXCIgVUOEhBXbJ6Rj1tV",
     "timestamp": 1620082260980
    },
    {
     "file_id": "1IbQMqLHIYF-vg6B4Abbobl4pkROWBwes",
     "timestamp": 1620082045291
    },
    {
     "file_id": "1fnbp6zHVBrdi1nCGkrokht8i-pAUfIyf",
     "timestamp": 1620056039573
    },
    {
     "file_id": "1WXvm9ORGBKSJCVApOV4gJIC0NC7OQFzX",
     "timestamp": 1619997366147
    },
    {
     "file_id": "1hYuZL48eIXUGFk2mPXxeUh610OkoOfmV",
     "timestamp": 1619991567406
    },
    {
     "file_id": "1SYhFD0Djg7etn9UB47zsjc948E5VxTFG",
     "timestamp": 1616868089323
    },
    {
     "file_id": "17CrRwBBNZfhpN3y5pk_dTt7PelHqfFSI",
     "timestamp": 1616535329540
    },
    {
     "file_id": "1eTiiJCqFmQvy8KLbd0FSI1WfweHUCz-w",
     "timestamp": 1616531331246
    },
    {
     "file_id": "1Tndd1egGbtLRTO0Hnz6QnI9XiV5nEZKq",
     "timestamp": 1616528772133
    },
    {
     "file_id": "1kTX59Ymn4DGGgjVVQTt7ocE_oy5Sanpx",
     "timestamp": 1616281970086
    },
    {
     "file_id": "1kPO5iwHQPVcMuTc_Tn5JcSIRJ4hNrWjH",
     "timestamp": 1616181812747
    },
    {
     "file_id": "1nkcov8gu5MJRqWxL1awc78jThGTXsTyZ",
     "timestamp": 1616013741366
    },
    {
     "file_id": "13iHBeXIFuSw6-EvChxlqiveFVFurjENx",
     "timestamp": 1615923191272
    },
    {
     "file_id": "1q1UXZxrBrZdwYFvuOtkaRAnqykzUqK1-",
     "timestamp": 1615843147388
    },
    {
     "file_id": "1NUjBm8LUmJ5_Cu3ithC2JLJ4aFAWFEm2",
     "timestamp": 1615585196418
    },
    {
     "file_id": "1Y8oTevpUwSdSV3oCdx_e37mkhaxPzrGf",
     "timestamp": 1615506244659
    }
   ]
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
