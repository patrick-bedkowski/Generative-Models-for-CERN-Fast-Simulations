{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Wrangling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_proton_a_images(n=10000):\n",
    "    return pd.read_pickle('../data/proton_images2a.pkl')[:n]\n",
    "\n",
    "def load_particle_data_a(n=10000):\n",
    "    return pd.read_pickle('../data/particles_cond_with_mass2a.pkl')[:n]\n",
    "\n",
    "def get_proton_images(n=10000):\n",
    "    data = load_proton_a_images(n)\n",
    "    #preprocess data\n",
    "    data = np.log(data+1)\n",
    "    return np.float32(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (10000, 56, 30)\n",
      "Loaded cond:  (10000, 11)\n"
     ]
    }
   ],
   "source": [
    "data = get_proton_images()\n",
    "print('Loaded: ',  data.shape)\n",
    "data_cond = load_particle_data_a()\n",
    "print('Loaded cond: ',  data_cond.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def filter_photon_sum(data_response, data_conditional, min_photon_sum=10):\n",
    "    \"\"\"\n",
    "    Returns particle data and responses of experiments with minimum sum of photons.\n",
    "    \"\"\"\n",
    "    indecies = data_conditional.index[data_conditional.loc[:, 'PhotonSum'] > min_photon_sum]\n",
    "    return data_response[indecies], data_conditional.iloc[indecies].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "((5261, 56, 30), (5261, 11))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, data_cond = filter_photon_sum(data, data_cond, min_photon_sum=20)\n",
    "data.shape, data_cond.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "# Credits: majerzemilia\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, decomposition, manifold, preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import time\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from utils.utils import sum_channels_parallel_ as sum_channels_parallel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24_02_2023_21_16 model_24_02_2023_21_16\n"
     ]
    }
   ],
   "source": [
    "DATE_STR = datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "NAME = \"model_\"+DATE_STR\n",
    "print(DATE_STR, NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def save_scales(model_name, scaler_means, scaler_scales):\n",
    "    out_fnm = f\"{model_name}_scales.txt\"\n",
    "    res = \"#means\"\n",
    "    for mean_ in scaler_means:\n",
    "        res += \"\\n\" + str(mean_)\n",
    "    res += \"\\n\\n#scales\"\n",
    "    for scale_ in scaler_scales:\n",
    "        res += \"\\n\" + str(scale_)\n",
    "    with open(f\"../models/{out_fnm}\", mode=\"w\") as f:\n",
    "        f.write(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4208, 56, 30) (1053, 56, 30) (4208, 11) (1053, 11)\n",
      "cond max 9.330304365660513 min -5.561496551979909\n"
     ]
    }
   ],
   "source": [
    "#train/test split\n",
    "x_train, x_test, y_train, y_test, = train_test_split(data, data_cond, test_size=0.2, shuffle=False)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "#scale cond data\n",
    "scaler = StandardScaler()\n",
    "y_train = scaler.fit_transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "print(\"cond max\", y_train.max(), \"min\", y_train.min())\n",
    "\n",
    "#save scales\n",
    "save_scales(\"Proton\", scaler.mean_, scaler.scale_)\n",
    "\n",
    "# make tf datasets\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size=128)\n",
    "dataset_cond = tf.data.Dataset.from_tensor_slices(y_train).batch(batch_size=128)\n",
    "dataset_with_cond = tf.data.Dataset.zip((dataset,dataset_cond)).shuffle(12800)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size=128)\n",
    "val_dataset_cond = tf.data.Dataset.from_tensor_slices(y_test).batch(batch_size=128)\n",
    "val_dataset_with_cond = tf.data.Dataset.zip((val_dataset,val_dataset_cond)).shuffle(12800)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 56, 30, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 28, 15, 32)   544         ['input_img[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 14, 8, 64)    32832       ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 7, 4, 128)    131200      ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 7, 4, 128)    0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " input_cond (InputLayer)        [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3584)         0           ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 3595)         0           ['input_cond[0][0]',             \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           71920       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 10)           210         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 10)           210         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 10)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 236,916\n",
      "Trainable params: 236,916\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 21)           0           ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3584)         78848       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 7, 4, 128)    0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 14, 8, 128)  0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 14, 8, 128)   262272      ['up_sampling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 14, 8, 128)  512         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 14, 8, 128)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSampling2D)  (None, 28, 16, 128)  0          ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 28, 16, 64)   131136      ['up_sampling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 28, 16, 64)  256         ['conv2d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 28, 16, 64)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSampling2D)  (None, 56, 32, 64)  0           ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 56, 32, 32)   32800       ['up_sampling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 56, 32, 32)  128         ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 56, 32, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 56, 30, 1)    97          ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 506,049\n",
      "Trainable params: 505,601\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "############################ Define Models ############################\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "latent_dim = 10\n",
    "cond_dim = 11\n",
    "poz_dim = 6\n",
    "\n",
    "\n",
    "############################ encoder ############################\n",
    "\n",
    "input_img = Input(shape=[56,30,1],name='input_img')\n",
    "input_cond = Input(shape=cond_dim,name='input_cond')\n",
    "x = Conv2D(32, kernel_size=4, strides=2, padding='same')(input_img)\n",
    "x = Conv2D(64, kernel_size=4, strides=2,padding='same')(x)\n",
    "x = Conv2D(128, kernel_size=4, strides=2,padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Flatten()(x)\n",
    "x = layers.concatenate([input_cond,x])\n",
    "x = layers.Dense(latent_dim*2, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model([input_img, input_cond], [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "############################ decoder ############################\n",
    "\n",
    "\n",
    "x = Input(shape=(latent_dim,))\n",
    "cond = Input(shape=(cond_dim,))\n",
    "inputs = Concatenate(axis=1)([x, cond])\n",
    "\n",
    "g = Dense(7*4*128)(inputs)\n",
    "g = Reshape((7,4,128))(g)\n",
    "\n",
    "g = UpSampling2D()(g)\n",
    "g = Conv2D(128, kernel_size=4, padding='same')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = LeakyReLU(alpha=0)(g)\n",
    "\n",
    "g = UpSampling2D()(g)\n",
    "g = Conv2D(64, kernel_size=4, padding='same')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = LeakyReLU(alpha=0)(g)\n",
    "\n",
    "g = UpSampling2D()(g)\n",
    "g = Conv2D(32, kernel_size=4, padding='same')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = LeakyReLU(alpha=0)(g)\n",
    "\n",
    "outputs = Conv2D(1, kernel_size=(1,3) ,activation='relu')(g)\n",
    "\n",
    "generator = Model([x, cond], outputs, name='generator')\n",
    "generator.summary()\n",
    "\n",
    "\n",
    "# define losses\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "d_acc_r = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "d_acc_f = keras.metrics.BinaryAccuracy(name=\"d_acc_r\", threshold=0.5)\n",
    "g_acc = keras.metrics.BinaryAccuracy(name=\"g_acc_g\", threshold=0.5)\n",
    "\n",
    "# define optimizer\n",
    "vae_optimizer = tf.keras.optimizers.RMSprop(1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#trainin params\n",
    "\n",
    "EPOCHS = 30\n",
    "noise_dim = 10\n",
    "num_examples_to_generate = 32\n",
    "\n",
    "# Seed to reuse for generating samples for comparison during training\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed_cond = y_test[:num_examples_to_generate]\n",
    "\n",
    "\n",
    "### function to calculate ws distance between orginal and generated channels\n",
    "org=np.exp(x_test)-1\n",
    "ch_org = org.reshape(-1,56,30)\n",
    "ch_org = pd.DataFrame(sum_channels_parallel(ch_org)).values\n",
    "del org"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_ws_ch(n_calc):\n",
    "    ws= [0,0,0,0,0]\n",
    "    for j in range(n_calc):\n",
    "        z = np.random.normal(0,1,(x_test.shape[0],10))\n",
    "        z_c = y_test\n",
    "        results = generator.predict([z,z_c])\n",
    "        results = np.exp(results)-1\n",
    "        try:\n",
    "            ch_gen = np.array(results).reshape(-1,56,30)\n",
    "            ch_gen = pd.DataFrame(sum_channels_parallel(ch_gen)).values\n",
    "            for i in range(5):\n",
    "                ws[i] = ws[i] + wasserstein_distance(ch_org[:,i], ch_gen[:,i])\n",
    "            ws =np.array(ws)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "    ws = ws/n_calc\n",
    "    print(\"ws mean\",f'{ws.sum()/5:.2f}', end=\" \")\n",
    "    for n,score in enumerate(ws):\n",
    "        print(\"ch\"+str(n+1),f'{score:.2f}',end=\" \")\n",
    "\n",
    "\n",
    "\n",
    "####################### training ##############################\n",
    "@tf.function\n",
    "def train_step(batch,step):\n",
    "\n",
    "    images, cond = batch\n",
    "    step=step\n",
    "    BATCH_SIZE = tf.shape(images)[0]\n",
    "\n",
    "    #train vae\n",
    "    with tf.GradientTape() as tape:\n",
    "        z_mean, z_log_var, z = encoder([images, cond])\n",
    "        reconstruction = generator([z, cond])\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.mean_squared_error(tf.reshape(images,(-1,56,30,1)), reconstruction), axis=(1, 2)\n",
    "            )\n",
    "        )\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        total_loss = 0.7 * kl_loss + reconstruction_loss\n",
    "    grads = tape.gradient(total_loss, generator.trainable_weights+encoder.trainable_weights)\n",
    "    vae_optimizer.apply_gradients(zip(grads, generator.trainable_weights+encoder.trainable_weights))\n",
    "\n",
    "    return total_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "\n",
    "history = []\n",
    "def train(dataset, epochs):\n",
    "    tf_step =tf.Variable(0, dtype=float)\n",
    "    step=0\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           [seed, seed_cond])\n",
    "    #print(calculate_ws_mae(3))\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in dataset:\n",
    "            total_loss, reconstruction_loss, kl_loss =train_step(batch,tf_step)\n",
    "            history.append([total_loss, reconstruction_loss, kl_loss])\n",
    "            tf_step.assign_add(1)\n",
    "            step = step+1\n",
    "\n",
    "            if step%100==0:\n",
    "                print(\"%d [Total loss: %.2f] [Recon_loss: %.2f] [KL loss: %.2f]\"% (\n",
    "                    step, total_loss, reconstruction_loss, kl_loss))\n",
    "\n",
    "            if step%1000==0:\n",
    "                generate_and_save_images(generator,\n",
    "                               epochs,\n",
    "                               [seed, seed_cond])\n",
    "\n",
    "        generate_and_save_images(generator,\n",
    "                                 epoch + 1,\n",
    "                                 [seed, seed_cond]\n",
    "                                 )\n",
    "\n",
    "        # # Save the model every epoch\n",
    "        # encoder.save_weights(\"../models/enc_\"+NAME + \"_\"+ str(epoch) +\".h5\")\n",
    "        # generator.save_weights(\"../models/gen_\"+NAME + \"_\"+ str(epoch) +\".h5\")\n",
    "        # np.savez(\"../models/history_\"+NAME+\".npz\",np.array(history))\n",
    "\n",
    "        calculate_ws_ch(min(epoch//5+1,5))\n",
    "\n",
    "        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 7, figsize=(15,4))\n",
    "    fig.suptitle(f\"Epoch: {epoch}\")\n",
    "    for i in range(0,14):\n",
    "        if i <7:\n",
    "            x = x_test[20+i].reshape(56,30)\n",
    "        else:\n",
    "            x = predictions[i-7].numpy().reshape(56,30)\n",
    "        #x[x<=0]=x.max()*-0.1\n",
    "        im = axs[i//7,i%7].imshow(x,interpolation='none', cmap='gnuplot')\n",
    "        axs[i//7,i%7].axis('off')\n",
    "        fig.colorbar(im, ax=axs[i//7,i%7])\n",
    "    plt.show()\n",
    "    plt.savefig('../images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "\n",
    "\n",
    "# ### Train model\n",
    "history=train(dataset_with_cond, EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf",
   "language": "python",
   "display_name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}