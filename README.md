# Generative models for ALICE fast particle simulations at CERN.

## Table of contents

1. [Setting Developing Environment](#setting-developing-environment)
2. [Producing data for tests](#producing-data-for-tests)
    1. [Original files](#original-files)
    2. [Data filtering](#data-filtering)
3. [Python scripts for training models](#python-scripts-for-training-models)
   1. [Python files](#python-files)
   2. [Configure WandB tool for training models](#configure-wandb-tool-for-training-models)
   3. [Run experiments](#run-experiments)
4. [Analysis of results](#analysis-of-results)
   1. [Files of trained models](#files-of-trained-models)
   2. [Results of evaluating models](#results-of-evaluating-models)

## Setting Developing Environment

In order to run all experiments and analysis jupyter notebooks, it is necessary to install the following tools: <br />Download version for your operating system.
1. [Python 3.9.16 download](https://www.python.org/downloads/release/python-3916/)
2. Install CUDA: <br />
    Note, for your system to actually use the GPU, it nust have a [Compute Capibility](https://developer.nvidia.com/cuda-gpus) >= to 3.0<br />
    Install CUDA 11.7 for your OS
   1. [CUDA Toolkit 11.7 Downloads](https://developer.nvidia.com/cuda-11-7-0-download-archive)<br />
   * Windows:<br /> double-click the executable and follow setup instructions<br />
   * Linux:<br /> follow the instructions [here](http://askubuntu.com/a/799185)<br />
   2. [Download cuDNN v8.9.6 (November 1st, 2023), for CUDA 11.x](https://developer.nvidia.com/rdp/cudnn-archive)
3. Install python pip modules from `requirements.txt` using command:
```pip install -r requirements.txt```

After the above setup it should be possible to run the scripts.


## Producing data for tests

In order to run all experiments, it is necessary to build datasets from original files.

Necessary data to run the experiments are the following:
- dataset with 9 conditional variables describing the: Mass, Energy, Charge, 3 vectors for momenta and 3 vectors for coordinates
- dataset with images originating from Proton ZDC device
- dataset with images originating from Neutron ZDC device

### Original files

Original files were generated by the [GEANT4](https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.13048).
Instructions on how to generate data are available in [here](https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideSimulation).<br />
The below files explain the order of steps that need to be performed to run the experiments. 

### Data filtering

The notebook <a href="notebooks/data_filtering.ipynb">data_filtering.ipynb</a> contains the initial preprocessing
and filtering needed for training. It allows you to:
- calculates the photon sum values for images from Proton and Neutron ZDC devices.
- filter the data according to the photon sum values using function `filter_photon_sum()`. <br />This allows you to create datasets to replicate the results in the thesis.
- preprocess the data for the joint model, referred to as padded dataset. <br /> This step adds padding to both images from Proton and Neutron ZDC and concatenates them create image with 2 channels.
- plot the distribution of photon values
- calculate quartile values of photon sum distribution

After the following steps you should have the following files:
- `data_cond_photonsum_proton_X_2312.pkl`
- `data_photonsum_proton_X_2312.pkl`
- `data_proton_neutron_photonsum_proton_18_1970_neutron_18_3249_padding.pkl`
- `data_cond_photonsum_p_18_n_18.pkl`

Where X denotes the minimal value for the photon sum value.

### Calculating diversity among the samples from SDI-GAN implementation

The notebook <a href="notebooks/calculating_diversity_for_data.ipynb">calculating_diversity_for_data.ipynb</a> contains the preprocessing of dataset to calculate diversity of samples explained in Section 8. of the thesis. <br />
You need to use files generated by above script. This is appropriate for both images coming from Proton ZDC device and Padded version of dataset. <br />
After completing the steps you should have the following files:
- `data_cond_stddev_photonsum_p_X.pkl`. Where X denotes the minimal value for the photon sum value in proton data
- `data_cond_stddev_photonsum_p_X_n_X.pkl`. Where X denotes the minimal value for the photon sum value in both proton and neutron data.

### Calculating data for auxiliary regressor
The notebook <a href="notebooks/auxilary regressor/calculate_max_coordinates.ipynb">calculate_max_coordinates.ipynb</a> contains calclation of min max coordinates in images for both Proton and padded dataset.

After completing the steps you should have the following files:
- `data_coord_proton_photonsum_proton_1_2312.pkl`. Where X denotes the minimal value for the photon sum value in proton data
- `data_coord_proton_neutron_photonsum_X.pkl`. Where X denotes the minimal value for the photon sum value in both proton and neutron data.

## Python scripts for training models

### Python files

The directory `notebooks/best_models_py/` contains python `.py` files of models that were described in the thesis. There are files:
- vae-proton-X.py: Variational Autoencoder for proton ZDC
- gan-proton-X.py: Generative Adversarial Network for proton ZDC
- sdi-gan-proton-X.py: SDI-GAN Generative Adversarial Network described in [paper](https://arxiv.org/pdf/2207.01561.pdf) for proton ZDC
- sin-gan-proton-X.py: SDI-GAN expanded by intensity regularization for proton ZDC explained in Section 8 of thesis
- sin-gan-proton-aux-reg-X.py: SDI-GAN expanded by intensity regularization with auxiliary regressor for proton ZDC
- sin-gan-joint-aux-reg-X.py: SDI-GAN expanded by intensity regularization with auxiliary regressor for joint model

Where X denotes the minimal value for the photon sum value in proton dataset or both proton and neutron data for the joint model.

It also contains `utils.py` containing necessary functions:
- `get_chanel_masks()`: returns 5 masks described in Figure 6.2 in thesis,
- `sum_channels_parallel()`: Given the input array describing images calculates the sum of pixels among channels,
- `get_max_value_image_coordinates()`: returns coordiantes of the maximal value in the image (used by auxiliary regressor)
- `calculate_ws_ch_proton_model()`: calculates WS metric for data coming from the proton channel

### Configure WandB tool for training models

Prior to executing training of models, I recommend configuring the [WandB](https://docs.wandb.ai/) tool for monitoring of training models. To do that, follow the steps:
1. Create WandB account
2. Generate API KEY for logging training [api_key](https://wandb.ai/authorize). Copy it.
3. Paste the generated API key at the beginning of each `python` file described in section [Python scripts for training models](#python-scripts-for-training-models).

After the following script of training is executed, you should be able to see the progress of results in the WandB online tool.

To get to know the tool better, refer to [quickstart page](https://docs.wandb.ai/quickstart).

### Run experiments

In order to run the training of models described in section [Python scripts for training models](#python-scripts-for-training-models), run the following command: <br/>

Windows:
```python
python vae-proton-1.py
```

Linux:
```python
python3 vae-proton-1.py
```

### Saving of the trained models

During the process of training the models, they are compiled and saved in [.h5](https://www.tensorflow.org/tutorials/keras/save_and_load) files during training.

## Analysis of results

In the following section it is described how to reproduce the results of analysis presented in thesis in Section 9. and 10.

### Files of trained models

Presented in Thesis trained models are available as `.h5` files in directory: `best models/`. The below sections use this files to produce plots, calculate WS metrics and number of empty responses.

### Results of evaluating models
Results of tests on trained models are in logs in directory `notebooks/calculating results logs/`. They are consistent with the results in thesis.